{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This is a catalog for the platform, where you can browse templates for its resources and find instructions and examples on how to use them.</p> <p>To know more about the platform and its features, please visit the official documentation.</p>"},{"location":"datasets/","title":"Datasets","text":"Search"},{"location":"functions/","title":"Functions","text":"Search"},{"location":"models/","title":"Models","text":"Search"},{"location":"functions/cancer-classfier-training-mlflow-example/","title":"Cancer classfier training mlflow example","text":"cancer-classfier-training-mlflow-example Name Cancer Classfier Training (MLFlow example) Description Train a cancer classifier ML model using MLFlow framework to structure and represent the model data. The trained model then can be deployed with the MLFlowServe framework or used for batch classifications loading it using the MLFlow framework. Version 1.0.0 Labels apache-2.0other0.14jobmodel-trainingsklearnhealthcare UsageNotebook <p> <p></p> <p></p>"},{"location":"functions/cancer-classfier-training-mlflow-example/#cancer-classfier-training-mlflow-example","title":"Cancer classfier Training (MLFlow example)","text":"<p>This function demonstrates a minimal example of a how to train a cancer classifier ML model using MLFlow framework to structure and represent the model data. The trained model then can be deployed with the MLFlowServe framework or used for batch classifications loading it using the MLFlow framework..</p>"},{"location":"functions/cancer-classfier-training-mlflow-example/#definition","title":"Definition","text":"<pre><code>from digitalhub_runtime_python import handler\n\nfrom digitalhub import from_mlflow_run\nimport mlflow\n\nfrom sklearn import datasets, svm\nfrom sklearn.model_selection import GridSearchCV\n\n@handler()\ndef train(project):\n    mlflow.sklearn.autolog(log_datasets=True)\n\n    iris = datasets.load_iris()\n    parameters = {\"kernel\": (\"linear\", \"rbf\"), \"C\": [1, 10]}\n    svc = svm.SVC()\n    clf = GridSearchCV(svc, parameters)\n\n    clf.fit(iris.data, iris.target)\n    run_id = mlflow.last_active_run().info.run_id\n\n    # utility to map mlflow run artifacts to model metadata\n    model_params = from_mlflow_run(run_id)\n\n    project.log_model(\n        name=\"model-mlflow\",\n        kind=\"mlflow\",\n        **model_params\n</code></pre> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>Note: Make sure to replace &lt;YOUR_PROJECT_NAME&gt; with the actual name of your project before running the code.</p> <p>Fetch the \"hello-world\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_mlfrow_train_model = proj.get_function(\"cancer-classfier-training-mlflow-example\") \n</pre> <p>Run the function as shown below. This will trigger the model training job on the platform and the model 'model-mlflow' get logged on to the platform.</p> In\u00a0[\u00a0]: <pre>function_mlfrow_train_model.run(action='job')\n</pre> <p>Now, one can fetch the trained model saved in project.</p> In\u00a0[\u00a0]: <pre>model = proj.get_model(\"model-mlflow\")\n</pre> <p>Note: Alternatively, using the Core Management UI, one can navigate to 'Model' menu to see the generated model.</p> <p>The 'serve' action deploys MLflow ML models as services on Kubernetes. A Task is created by calling run() on the Function; task parameters are passed through that call.</p> In\u00a0[\u00a0]: <pre>serve_func = proj.new_function(\n    name=\"serve-mlflow-model\",\n    kind=\"mlflowserve\",\n    model_name=model.name,\n    path=model.key,\n)\n</pre> In\u00a0[\u00a0]: <pre>serve_run = serve_func.run(\"serve\", wait=True)\n</pre> <p>Let's test our deployed MLflow model by making a prediction request with sample Iris data:</p> In\u00a0[\u00a0]: <pre>from sklearn import datasets\n\n# Load some test data from the Iris dataset\niris = datasets.load_iris()\ndata = iris.data[0:2].tolist()\njson_payload = {\n    \"inputs\": [{\"name\": \"input-0\", \"shape\": [-1, 4], \"datatype\": \"FP64\", \"data\": data}]\n}\n\n# Make prediction\nresult = serve_run.invoke(model_name=model.name, json=json_payload).json()\nprint(\"Prediction result:\")\nprint(result)\n</pre> <p> </p>"},{"location":"functions/cancer-classfier-training-mlflow-example/#usage","title":"Usage","text":"<p>The function demonstrates:</p> <p>Basic syntax of a python script used inside to the platform. It demonstrates - How to define a function that takes as input 'project' object,  a context in which you can run functions and manage models,data, and artifacts. - How to fetch input data and train model. - How to save the trained model in project.</p> <p>The function 'mlflow-train-model' is registered inside to the platform core durig the import and it can be fetched and executed</p> <pre><code>func = proj.get_function(name=\"cancer-classfier-training-mlflow-example\") \n</code></pre> <p>This code fetch the created function that uses Python runtime (versione 3.10) pointing to the created file and the handler method that should be called. In this case, the code and hanlder is already embedded during the funciton import from the .yaml file.</p> <p>Then, the function can be executed on the digital hub platform or (locally) as a single job.</p> <p>Notes: For detailed usage, check the usage notebook.</p>"},{"location":"functions/cancer-classfier-training-mlflow-example/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create the working context: data download project if not created already. Project is a placeholder for the code, data, and management of the data operations.</p>"},{"location":"functions/cancer-classfier-training-mlflow-example/#2.-Execution","title":"2. Execution\u00b6","text":""},{"location":"functions/cancer-classfier-training-mlflow-example/#Serve","title":"Serve\u00b6","text":""},{"location":"functions/cancer-classfier-training-mlflow-example/#Test-the-model","title":"Test the model\u00b6","text":""},{"location":"functions/download-sentinel-data/","title":"Download sentinel data","text":"download-sentinel-data Name Download Sentinel Data Description Function for Sentinel image download and preprocessing Version 0.14.6 Labels apache-2.0geopa0.14jobdata-preparation UsageNotebook"},{"location":"functions/download-sentinel-data/#download-sentinel-data","title":"Download Sentinel Data","text":"<p>This function is used to download Sentinel satellite data efficiently. This function serves as the base for performing elaborations on different kinds of geospatial data processing tasks. Each elaboration comes with different sets of data requirements depending on the specific geospatial processing task and satellite data type being utilized. </p>"},{"location":"functions/download-sentinel-data/#definition","title":"Definition","text":"<p>This function is implemented as a Docker container and executed at runtime with user-configurable parameters. The function takes as input a parameter object that controls the satellite data selection, temporal extent, and spatial footprint. Key parameter groups include:</p> <ul> <li> <p>satelliteParams</p> <ul> <li>satelliteType: Sentinel product family (e.g., \"Sentinel-1\", \"Sentinel-2\").</li> <li>processingLevel: desired processing stage (\"LEVEL1\", \"LEVEL2\", ...).</li> <li>sensorMode / polarization: sensor acquisition mode or polarization settings (mode names depend on satellite).</li> <li>productType: product format (e.g., \"SLC\", \"GRD\", \"L2A\").</li> <li>orbitDirection: pass \"ASCENDING\" or \"DESCENDING\".</li> <li>relativeOrbitNumber: numeric orbit track identifier (optional).</li> </ul> </li> <li> <p>temporal and spatial</p> <ul> <li>startDate / endDate: ISO date strings (YYYY-MM-DD) defining the search window.</li> <li>geometry: spatial footprint in WKT or GeoJSON (polygon or bbox).</li> <li>area_sampling: boolean toggle to enable area-level sampling.</li> </ul> </li> <li> <p>storage and runtime</p> <ul> <li>tmp_path_same_folder_dwl: boolean to control temporary download placement.</li> <li>artifact_name: name used for storing the downloaded artifact in the project context.</li> </ul> </li> </ul> <p>Notes and tips: - Date ranges should be chosen to limit result sets; large ranges may require increased compute and storage. - Use relativeOrbitNumber to filter by specific orbit tracks when needed. - For Sentinel-2 searches you can also supply cloudCoverage thresholds; for Sentinel-1 specify polarization/product specifics. - Ensure valid Copernicus credentials are available to the runtime via secrets before launching jobs.</p> <p>Example parameter shape (illustrative): {     \"satelliteParams\": { \"satelliteType\": \"Sentinel-1\", \"processingLevel\": \"LEVEL1\", \"productType\": \"SLC\", \"orbitDirection\": \"DESCENDING\" },     \"startDate\": \"2021-01-01\", \"endDate\": \"2021-01-15\",     \"geometry\": \"\",     \"artifact_name\": \"download_output\" }"},{"location":"functions/download-sentinel-data/#usage","title":"Usage","text":"<p>The function is of kind container runtime that allows you to deploy deployments, jobs and services on Kubernetes. It uses the base image of sentinel-tools developed in the context of project which is a wrapper for the Sentinel download and preprocessing routine for the integration with the AIxPA platform. For more details Click here.</p> <p><pre><code>string_dict_data = \"\"\"{\n     \"satelliteParams\":{\n        \"satelliteType\": \"Sentinel2\",\n        \"bandmath\": [\"NDWI\"]\n     },\n     \"startDate\": \"2023-12-12\",\n     \"endDate\": \"2019-12-30\",\n     \"geometry\": \"POLYGON((10.88558452267069 46.2069331490752, 11.02591468396198 46.2069331490752, 11.02591468396198 46.288250617785245, 10.88558452267069 46.288250617785245, 10.88558452267069 46.2069331490752))\",\n     \"cloudCover\": \"[0,5]\",\n     \"area_sampling\": \"True\",\n     \"artifact_name\": \"sentinel2_ndwi_area_sampling_2018\"\n }\"\"\"\n\n\nlist_args =  [\"main.py\",string_dict_data]\nfunction = proj.get_function(\"download-sentinel-data\",kind=\"container\",image=\"ghcr.io/tn-aixpa/sentinel-tools:latest\",command=\"python\")\n</code></pre> Notes: For detailed usage see the usage notebook.</p>"},{"location":"functions/download-sentinel-data/#environment","title":"Environment","text":"<p>The runtime environment of function consist of properties like - CDSETOOL_ESA_PASSWORD - CDSETOOL_ESA_USER</p> <p>Register to the open data space copernicus(if not already) and get your credentials.</p> <p>https://identity.dataspace.copernicus.eu/auth/realms/CDSE/login-actions/registration?client_id=cdse-public&amp;tab_id=FIiRPJeoiX4</p> <p>Log the credentials as project secret keys as shown below</p> <pre><code># THIS NEED TO BE EXECUTED JUST ONCE\nsecret0 = proj.new_secret(name=\"CDSETOOL_ESA_USER\", secret_value=\"esa_username\")\nsecret1 = proj.new_secret(name=\"CDSETOOL_ESA_PASSWORD\", secret_value=\"esa_password\")\n</code></pre> <p>To avoid capacity issues the environment variable \"TMPDIR\" for this function execution is set to same path of volume mount. As a general confromance to best practice approach, the container runtime is executed as non root user(fs_group='8877')</p> <pre><code>function.run(\n    action=\"job\",\n    secrets=[\"CDSETOOL_ESA_USER\",\"CDSETOOL_ESA_PASSWORD\"],\n    fs_group='8877',\n    args=[\"main.py\", string_dict_data],\n    envs=[{\"name\": \"TMPDIR\", \"value\": \"/app/files\"}],\n    ...\n    }])\n</code></pre>"},{"location":"functions/download-sentinel-data/#resources","title":"Resources","text":"<p>The resources for running this function varies as per the envisaged elaboration scenario requirements which depends on many factors such as type of elaboration, data period, index, geometry etc. The download-sentinel-data function depends on Sentinel Hub dataspace. It could happen that data download takes more time than usual due to various factors, including technical issues, data processing delays, and limitations in the data access infrastructure. Sentinel data has both temporal and spatial types, as it collects data over time (temporal) with specific spatial resolutions. The size of sentinal data payload in is normally large based on requirement of usecase scenario and requires a significant block of computing resources to executed which includes number of cpu, memory(Gi), and volume(Gi). The function performance improves with significant number of cpu and memory. In general, the recommended resources(cpu, memory) for running this function:</p> <pre><code>{\n    \"resources\": {\n        \"cpu\": \"6\",\n        \"mem\": \"32Gi\"\n    }\n}\n</code></pre> <p>Data volume requirements vary by scenario: - Single scene download: 100\u2013500 MB (Sentinel-1 GRD), 500\u20131000 MB (Sentinel-2 L2A) - Multi-temporal series: Scales linearly with date range and area size - Large geographic areas: May require 10+ GB for month-long searches - Band math / preprocessing: Adds 20\u201330% overhead to storage needs</p> <p>In order to run this function, a volume of type 'persistent_volume_claim' is specified to ensure significant data space. For example, the scenarios based on environmental degradation usecases like deorestation, vegetation loss are based on temporal analysis and requires downloading of big data over a period of time. On the other hand, the scenario based on natural disasters events requires downloading of different data payloads around a given event date, compute pre/post windows of data payloads for elaboration. Inside the usage notebook, one can find more fine grained resource configurations for different kinds of data analysis for e.g. one such example is the flood scenario for which the volume configuration for data payload (\u00b1 10 days) with respect to flood event date is shown below.</p> <pre><code>{\n    \"volumes\": [\n        {\n            \"volume_type\": \"persistent_volume_claim\",\n            \"name\": \"volume-flood\",\n            \"mount_path\": \"/app/files\",\n            \"spec\": {\n                \"size\": \"100Gi\"\n            }\n        }\n    ]\n}\n</code></pre> <p>For more detailed usage for different kind of scenario, check the usage notebook.</p> <p> <p></p> <p></p> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>This section describes required fields, recommended practices, and example payloads (stringified JSON) to prepare Sentinel data downloads. Use these payloads as the <code>string_dict_data</code> argument when calling the download function in this notebook.</p> <p>As indicated in README.MD file register and store Copernicus CDSETOOL credentials in the project context as shown below</p> In\u00a0[\u00a0]: <pre>proj.set_secret(\"CDSETOOL_ESA_USER\", \"&lt;YOUR_COPERNICUS_USERNAME&gt;\")\nproj.set_secret(\"CDSETOOL_ESA_PASSWORD\", \"&lt;YOUR_COPERNICUS_PASSWORD&gt;\")\n</pre> <p>In the following sections, one can find usage examples for different kinds of geo data.</p> <p>This section demonstrates the date fetch process in case of natural disaster tragedy like geological hazards, alluvion, flood etc, the data is fetched for a given event date compute pre/post windows for elaboration and analysis. The data is prepared for those dates (ISO format) in different payloads(one per sensor/window) with distinct 'artifact' names inside to the project context.</p> In\u00a0[\u00a0]: <pre># Example payload (stringified JSON)\n{\n \"satelliteParams\":{\n    \"satelliteType\": \"Sentinel2\",\n    \"processingLevel\": \"S2MSI2A\",\n    \"bandmath\": [\"NDWI\"]\n },\n \"startDate\": \"2020-10-02\",\n \"endDate\": \"2020-10-22\",\n \"geometry\": \"POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))\",\n \"cloudCover\": \"[0,20]\",\n \"area_sampling\": \"True\",\n \"tmp_path_same_folder_dwl\": \"True\",\n \"artifact_name\": \"sentinel2_post_event\",\n \"preprocess_data_only\": \"false\"\n }\n</pre> <p>Example workflow: For flood analysis four datasets are prepared (e.g. <code>sentinel2_pre_flood</code>, <code>sentinel2_post_flood</code>, <code>sentinel1_GRD_preflood</code>, <code>sentinel1_GRD_postflood</code>). The four datasets: Sentinel\u20112 imagery for the 20\u2011day pre\u2011 and post\u2011event windows, and Sentinel\u20111 SAR for the 7\u2011day pre\u2011 and post\u2011event windows. Accordingly, the section below demonstrates four separate runs of the \"download-sentinel-data\" function \u2014 one per sensor/time window. Example payload (stringified JSON)</p> <p>Fetch the \"download-sentinel-data\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_data = proj.get_function(\"download-sentinel-data\")\n</pre> In\u00a0[\u00a0]: <pre># Example payload (stringified JSON)\n\nstring_dict_data = \"\"\"{\n \"satelliteParams\":{\n    \"satelliteType\": \"Sentinel2\",\n    \"processingLevel\": \"S2MSI2A\",\n   \"bandmath\": [\"NDWI\"]\n },\n \"startDate\": \"2020-10-02\",\n \"endDate\": \"2020-10-22\",\n \"geometry\": \"POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))\",\n \"cloudCover\": \"[0,20]\",\n \"area_sampling\": \"True\",\n \"tmp_path_same_folder_dwl\": \"True\",\n \"artifact_name\": \"sentinel2_post_flood\",\n \"preprocess_data_only\": \"false\"\n }\"\"\"\nlist_args =  [\"main.py\",string_dict_data]\n</pre> <p>Run the function. As a result the post flood sentinel-2 data is logged as project artifact(\"sentinel2_post_flood\")</p> In\u00a0[\u00a0]: <pre>func_run = function_data.run(action=\"job\",\nsecrets=[\"CDSETOOL_ESA_USER\",\"CDSETOOL_ESA_PASSWORD\"],\nfs_group=\"8877\",\nargs=list_args,\nresources={\"cpu\": \"6\",\"mem\": \"32Gi\"},\nvolumes=[{\n    \"volume_type\": \"persistent_volume_claim\",\n    \"name\": \"volume-flood\",\n    \"mount_path\": \"/app/files\",\n    \"spec\": {\n        \"size\": \"100Gi\"\n        }\n    }]\n)\n</pre> In\u00a0[\u00a0]: <pre>string_dict_data = \"\"\"{\n     \"satelliteParams\":{\n        \"satelliteType\": \"Sentinel2\",\n        \"processingLevel\": \"S2MSI2A\",\n       \"bandmath\": [\"NDWI\"]\n     },\n     \"startDate\": \"2020-09-12\",\n     \"endDate\": \"2020-10-02\",\n     \"geometry\": \"POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))\",\n     \"cloudCover\": \"[0,20]\",\n     \"area_sampling\": \"True\",\n     \"tmp_path_same_folder_dwl\": \"True\",\n     \"artifact_name\": \"sentinel2_pre_flood\",\n     \"preprocess_data_only\": \"false\"\n     }\"\"\"\nlist_args =  [\"main.py\",string_dict_data]\n</pre> <p>Run the function again. As a result the pre flood sentinel-2 data is logged as project artifact(\"sentinel2_post_flood\")</p> In\u00a0[\u00a0]: <pre>run = function_data.run(action=\"job\",\n    secrets=[\"CDSETOOL_ESA_USER\",\"CDSETOOL_ESA_PASSWORD\"],\n    fs_group=\"8877\",\n    args=list_args,\n    resources={\"cpu\": \"6\",\"mem\": \"32Gi\"},\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-flood\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": {\n            \"size\": \"100Gi\"\n            }\n    }]\n    )\n</pre> <p>The parameters passed for sentinel-1 downloads includes the starts and ends dates corresponding to period of 7 days from flood event date. The ouput of this step will be logged inside to the platfrom project context as indicated by parameter \"artifact_name\" (\"sentinel1_GRD_postflood\").Several other paramters can be configures as per requirements for e.g. geometry, cloud cover percentage etc.</p> In\u00a0[\u00a0]: <pre>string_dict_data = \"\"\"{\n\"satelliteParams\": {\n  \"satelliteType\": \"Sentinel1\",\n  \"processingLevel\": \"LEVEL1\",\n  \"sensorMode\": \"IW\",\n  \"productType\": \"GRD\"\n},\n\"startDate\": \"2020-10-02\",\n\"endDate\": \"2020-10-09\",\n\"geometry\": \"POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))\",\n\"area_sampling\": \"True\",\n\"tmp_path_same_folder_dwl\":\"True\",\n\"artifact_name\": \"sentinel1_GRD_postflood\"\n}\"\"\"\nlist_args =  [\"main.py\",string_dict_data]\n</pre> <p>Run the function. As a result the post flood sentinel-2 data is logged as project artifact(\"sentinel2_post_flood\")</p> In\u00a0[\u00a0]: <pre>run = function_data.run(action=\"job\",\n    secrets=[\"CDSETOOL_ESA_USER\",\"CDSETOOL_ESA_PASSWORD\"],\n    fs_group=\"8877\",\n    args=list_args,\n    resources={\"cpu\": \"6\",\"mem\": \"32Gi\"},\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-flood\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": {\n            \"size\": \"100Gi\"\n        }}])\n</pre> In\u00a0[\u00a0]: <pre>string_dict_data = \"\"\"{\n  \"satelliteParams\": {\n          \"satelliteType\": \"Sentinel1\",\n          \"processingLevel\": \"LEVEL1\",\n          \"sensorMode\": \"IW\",\n          \"productType\": \"GRD\"\n      },\n      \"startDate\": \"2020-09-25\",\n      \"endDate\": \"2020-10-02\",\n      \"geometry\": \"POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))\",\n      \"area_sampling\": \"True\",\n      \"tmp_path_same_folder_dwl\":\"True\",\n      \"artifact_name\": \"sentinel1_GRD_preflood\"\n  }\"\"\"\nlist_args =  [\"main.py\",string_dict_data]\n</pre> In\u00a0[\u00a0]: <pre>run = function_data.run(action=\"job\",\n    secrets=[\"CDSETOOL_ESA_USER\",\"CDSETOOL_ESA_PASSWORD\"],\n    fs_group=\"8877\",\n    args=list_args,\n    resources={\"cpu\": \"6\",\"mem\": \"32Gi\"},\n    envs={},\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-flood\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": {\n            \"size\": \"100Gi\"\n        }}])\n</pre> <p>Another example of natural disaster even is the geological hazard or landslide montioring scenario. For landslide monitoring, the \"download-sentinel-data\" function is used to fetch Sentinel\u20111 SLC scenes that cover the specified AOI geometry and time window. Retrieved images are split by orbit direction \u2014 ascending acquisitions are stored in an ascending artifact and descending acquisitions in a descending artifact \u2014 allowing independent processing workflows (e.g., SLC stacking or InSAR) for each orbit geometry. The example cells below show submitting one job per orbit direction and saving the results as project artifacts.</p> In\u00a0[\u00a0]: <pre>s1_ascending = \"s1_ascending_landslide_2020-10-01_2020-01-14\"\nstartDate = \"2020-10-01\"\nendDate = \"2020-10-14\"\ngeometry = \"POLYGON ((10.595369 45.923394, 10.644894 45.923394, 10.644894 45.945838, 10.595369 45.945838, 10.595369 45.923394))\"\nstring_dict_data_asc = \"\"\"{\n    \"satelliteParams\":{\n        \"satelliteType\": \"Sentinel1\",\n        \"processingLevel\": \"LEVEL1\",\n        \"sensorMode\": \"IW\",\"productType\": \"SLC\",\n        \"orbitDirection\": \"ASCENDING\",\n        \"relativeOrbitNumber\": \"117\"\n        },\n    \"startDate\": \\\"\"\"\" + startDate + \"\"\"\\\",\n    \"endDate\": \\\"\"\"\" + endDate + \"\"\"\\\",\n    \"geometry\": \\\"\"\"\" + geometry  + \"\"\"\\\",\n    \"area_sampling\": \"True\",\n    \"tmp_path_same_folder_dwl\":\"True\",\n    \"artifact_name\": \\\"\"\"\" + s1_ascending + \"\"\"\\\"\n    }\n\"\"\"\nlist_args =  [\"main.py\",string_dict_data_asc]\n</pre> In\u00a0[\u00a0]: <pre>run = function_data.run(action=\"job\",\n    secrets=[\"CDSETOOL_ESA_USER\",\"CDSETOOL_ESA_PASSWORD\"],\n    fs_group=\"8877\",\n    args=list_args,\n    resources={\"cpu\": \"6\",\"mem\": \"32Gi\"},\n    envs=[{\"name\": \"TMPDIR\", \"value\": \"/app/files\"}],\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-land\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": {\n            \"size\": \"100Gi\"\n        }}]\n    )\n</pre> In\u00a0[\u00a0]: <pre>s1_descending = \"s1_descending_landslide_2020-10-01_2010-01-14\"\nstartDate = \"2010-01-01\"\nendDate = \"2010-01-14\"\ngeometry = \"POLYGON ((10.595369 45.923394, 10.644894 45.923394, 10.644894 45.945838, 10.595369 45.945838, 10.595369 45.923394))\"\nstring_dict_data_des = \"\"\"{\n    \"satelliteParams\":{\n        \"satelliteType\": \"Sentinel1\",\n        \"processingLevel\": \"LEVEL1\",\n        \"sensorMode\": \"IW\",\n        \"productType\": \"SLC\",\n        \"orbitDirection\": \"DESCENDING\",\n        \"relativeOrbitNumber\": \"168\"\n        },\n    \"startDate\": \\\"\"\"\" + startDate + \"\"\"\\\",\n    \"endDate\": \\\"\"\"\" + endDate + \"\"\"\\\",\n    \"geometry\": \\\"\"\"\" + geometry + \"\"\"\\\",\n    \"tmp_path_same_folder_dwl\":\"True\",\n    \"area_sampling\": \"True\",\"artifact_name\": \\\"\"\"\" + s1_descending + \"\"\"\\\"\n    }\"\"\"\nlist_args =  [\"main.py\",string_dict_data_des]\n</pre> In\u00a0[\u00a0]: <pre>run = function_data.run(action=\"job\",\n    secrets=[\"CDSETOOL_ESA_USER\",\"CDSETOOL_ESA_PASSWORD\"],\n    fs_group=\"8877\",\n    args=list_args,\n    resources={\"cpu\": \"6\",\"mem\": \"32Gi\"},\n    envs=[{\"name\": \"TMPDIR\", \"value\": \"/app/files\"}],\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-land\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": {\n            \"size\": \"100Gi\"\n        }}]\n    )\n</pre> In\u00a0[\u00a0]: <pre>#Example payload (stringified JSON)\n{\n    \"satelliteParams\": {\n        \"satelliteType\": \"Sentinel2\",\n        \"processingLevel\": \"S2MSI2A\",\n        \"bandmath\": [\"NDVI\",\"NBR\",\"BSI\"]\n    },\n    \"startDate\": \"2018-06-01\",\n    \"endDate\": \"2018-08-31\",\n    \"geometry\": \"POLYGON ((...))\",\n    \"cloudCover\": \"[0,20]\",\n    \"area_sampling\": \"True\",\n    \"tmp_path_same_folder_dwl\": \"True\",\n    \"artifact_name\": \"sentinel2_envdeg_baseline\",\n    \"preprocess_data_only\": \"false\"\n}\n</pre> <p>Example workflow: For deforestation analysis, temporal Sentinel\u20112 Level\u20112A imagery spanning one to two years is required to build monthly time\u2011series (e.g., NDVI, BSI), enable trend and seasonality modeling, and detect change events using methods like BFAST. The temporal dataset is prepared and logged as artifact (data_s2_deforestation) inside to the project context. Accordinly, the section below demontrates a run of the 'download-sentinel-data' function.</p> <p>Fetch the \"download-sentinel-data\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_data = proj.get_function(\"download-sentinel-data\")\n</pre> In\u00a0[\u00a0]: <pre>string_dict_data = \"\"\"{\n \"satelliteParams\":{\n     \"satelliteType\": \"Sentinel2\"\n },\n \"startDate\": \"2018-01-01\",\n \"endDate\": \"2018-12-31\",\n \"geometry\": \"POLYGON((10.968432350469937 46.093829019481056,10.968432350469937 46.09650743619973, 10.97504139531014 46.09650743619973,10.97504139531014 46.093829019481056, 10.968432350469937 46.093829019481056))\",\n \"area_sampling\": \"true\",\n \"cloudCover\": \"[0,5]\",\n \"artifact_name\": \"data_s2_deforestation\"\n }\"\"\"\n\nlist_args =  [\"main.py\",string_dict_data]\n</pre> In\u00a0[\u00a0]: <pre>run = function_data.run(action=\"job\",\n    secrets=[\"CDSETOOL_ESA_USER\",\"CDSETOOL_ESA_PASSWORD\"],\n    fs_group=\"8877\",\n    args=list_args,\n    resources={\"cpu\": \"6\",\"mem\": \"32Gi\"},\n    envs=[{\"name\": \"TMPDIR\", \"value\": \"/app/files\"}],\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-deforestation\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": {\n            \"size\": \"50Gi\"\n        }}]\n    )\n</pre> <p> </p>"},{"location":"functions/download-sentinel-data/#How-to-prepare-data-for-download","title":"How to prepare data for download\u00b6","text":"<p>To prepare the geological data, it is required to log the data in the project context</p>"},{"location":"functions/download-sentinel-data/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create the working context: data download project if not created already. Project is a placeholder for the code, data, and management of the data operations.</p>"},{"location":"functions/download-sentinel-data/#2.-Preparation-guidelines-and-examples","title":"2. Preparation guidelines and examples\u00b6","text":""},{"location":"functions/download-sentinel-data/#1--Natural-disaster-events-data","title":"1- Natural disaster events data\u00b6","text":""},{"location":"functions/download-sentinel-data/#Post-Flood-Sentinel2-Data-+20days","title":"Post Flood Sentinel2 Data +20days\u00b6","text":"<p>The parameters passed for sentinel downloads includes the starts and ends dates corresponding flood event. The ouput of this step will be logged inside to the platfrom project context as indicated by parameter \"artifact_name\" (\"sentinel2_post_flood\").Several other paramters can be configures as per requirements for e.g. geometry, cloud cover percentage etc</p>"},{"location":"functions/download-sentinel-data/#Pre-Flood-Sentinel2-Data--20-days","title":"Pre Flood Sentinel2 Data -20 days\u00b6","text":""},{"location":"functions/download-sentinel-data/#Post-Flood-Sentinel1-Data-+7days","title":"Post Flood Sentinel1 Data +7days\u00b6","text":""},{"location":"functions/download-sentinel-data/#Pre-Flood-Sentinel1-Data--7days","title":"Pre Flood Sentinel1 Data -7days\u00b6","text":"<p>Similary download the sentine-1 data pre flood event.</p>"},{"location":"functions/download-sentinel-data/#Landslide-Sentinel-1-Data-acquistion-(Ascending)","title":"Landslide Sentinel 1 Data acquistion (Ascending)\u00b6","text":""},{"location":"functions/download-sentinel-data/#Landslide-Sentinel-1-Data-acquistion-(Descending)","title":"Landslide Sentinel 1 Data acquistion (Descending)\u00b6","text":""},{"location":"functions/download-sentinel-data/#2.-Environmental-Degradation-Data.","title":"2. Environmental Degradation Data.\u00b6","text":"<p>This section demonstrates the data-fetch process for environmental degradation monitoring (e.g., deforestation, vegetation loss). It explains required inputs, provides a sample payload for Sentinel-2, and lists recommended steps and best practices to obtain consistent baseline and change-detection datasets.</p> <p>Purpose</p> <ul> <li>Acquire time-series optical imagery to compute vegetation and burn/change indices (NDVI, NBR, BSI) for baseline and monitoring windows.</li> <li>Produce artifacts per time window (e.g., baseline, disturbance, post-disturbance) to support analysis (change detection, classification, trend analysis).</li> </ul>"},{"location":"functions/echo-service/","title":"Echo service","text":"echo-service Name Echo Service Description A simple python-based API responding to a POST message with the \"text\" field. The function exposes an API using \"serve\" task. Version 1.0.0 Labels apache-2.0other0.14serviceutilitycustom UsageNotebook <p> <p></p> <p></p>"},{"location":"functions/echo-service/#echoservice","title":"echoservice","text":"<p>echoservice is a simple Python service that demonstrates how a simple Python function can be operationalizes as a service based on serverless container runtime feature. The service is exposed as an API responding to POST message with the text input and returns the same text back \u2014 an echo.</p>"},{"location":"functions/echo-service/#definition","title":"Definition","text":"<pre><code>import json\n\ndef init(context):\n    print(\"some initialization function\")\n    setattr(context, \"value\", \"some value\")\n\ndef serve(context, event):\n\n    if isinstance(event.body, bytes):\n        body = json.loads(event.body)\n    else:\n        body = event.body\n    context.logger.info(f\"Received event: {body}\")\n    text = body[\"text\"]\n\n    return {\"result\": f\"hello {text} from '{context.value}'\"}\n</code></pre> <p>The init(context) method is responsible for initializing the function\u2019s execution context.</p> <p>The serve(context, event) method contains the core functionality of the service, including the incoming request handling and execution logic.</p>"},{"location":"functions/echo-service/#usage","title":"Usage","text":"<p>The function demonstrates:</p> <ul> <li>How to define a simple serverless python function</li> <li>How to expose the underlying function as an API</li> <li>How to initialize runtime service context</li> <li>How to generate and return output</li> <li>How to call the function</li> </ul>"},{"location":"functions/echo-service/#start-the-service","title":"Start the service","text":"<pre><code>serve_run = function_echoservice.run(action='serve', wait=True)\n</code></pre>"},{"location":"functions/echo-service/#send-a-request","title":"Send a request","text":"<pre><code>json = {\n    \"text\":\"DigitalHub!\"\n}\nserve_run.invoke(json=json)\nresult.json()\n</code></pre> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>Note: Make sure to replace &lt;YOUR_PROJECT_NAME&gt; with the actual name of your project before running the code.</p> <p>Fetch the \"echoservice\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_echoservice = proj.get_function(\"echoservice\") \n</pre> <p>Then, execute the function (locally) as specified with (local_execution=True) flag. This will execute the function as a single job in the local context. Without the local_execution flag the job will get executed on the platform. For more details about how to create, run, and view jobs inside to the digital hub platform see the documentation </p> In\u00a0[\u00a0]: <pre>serve_run = function_echoservice.run(action='serve', wait=True)\n</pre> <p>As a result the 'echoservice' get deployed. Now fetch the URL of the service for API invocation.</p> <p>Now, we can run the same function as a job in the platform by removing the 'local_execution' flag.</p> <p>Use the invoke api of 'run' object to invoke the service API with the following input json.</p> In\u00a0[\u00a0]: <pre>json = {\n    \"text\":\"DigitalHub!\"\n}\n</pre> In\u00a0[\u00a0]: <pre>serve_run.invoke(json=json)\nresult.json()\n</pre> <p>{'result': \"hello DigitalHub! from 'some value'\"}</p> <p>Note: Alternatively, using the Core Management UI, one can navigate to 'Runs' menu , select the corresponding 'service run' instance and inspect the logs using 'Logs' tab. One call also call the API using 'Client' button inside 'service run' view</p> <p> </p>"},{"location":"functions/echo-service/#response","title":"Response:","text":"<pre><code>{'result': \"hello DigitalHub! from 'some value'\"}\n</code></pre> <p>Notes: For detailed usage, check the usage notebook.</p>"},{"location":"functions/echo-service/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create the working context: data download project if not created already. Project is a placeholder for the code, data, and management of the data operations.</p>"},{"location":"functions/echo-service/#2.-Execution","title":"2. Execution\u00b6","text":""},{"location":"functions/elaborate-flood-data/","title":"Elaborate flood data","text":"elaborate-flood-data Name Elaborate Flood Data Description Function to elaborate flood data using Sentinel-1 and Sentinel-2 imagery Version 0.14.6 Labels apache-2.0geopasecurity0.14jobfeature-extraction UsageNotebook <p> <p></p> <p></p>"},{"location":"functions/elaborate-flood-data/#elaborate-flood-data","title":"Elaborate Flood Data","text":"<p>This function performs flood analysis using Sentinel satellite data to assess flood extent and impact. It processes raw .SAFE or .zip Sentinel inputs, computes water indices, predicts water before and after a flood event, and outputs flood detection layer.</p> <p>The function provides complete workflow for - Ingesting Sentinel-1 (scene-based) and Sentinel-2 (tile-based) data using product-specific metadata. - Perform elaboration - Compute NDWI indices from Sentinel-2 imagery to detect water bodies before and after the flood event. - Calculate flood extent by analyzing pre- and post-event backscatter differences from Sentinel-1 data - Combine both results from Sentinel-1 and Sentinel-2 to have one flood prediction layer. - Post-process change maps to improve the results by masking permanent water bodies. - Log results as GeoTIFF raster files Raster and vector outputs.</p> <p>The function is implemented as a container that allows you to deploy deployments, jobs and services on Kubernetes. It uses the base image with gdal, snapista, and scikit-learn libaries installed and pre configured. It further runs the launch instructions specified by 'launch.sh' file. </p>"},{"location":"functions/elaborate-flood-data/#definition","title":"Definition","text":"<p>The function accepts a list of positional arguments that are passed directly to the Docker container. These parameters control Sentinel data selection, temporal configuration, output aritifact, and AOI geometry. These arguments are passed to the container\u2019s entrypoint script.</p> <p>These arguments define inputs, geospatial filters, auxiliary data, processing parameters, and scenario metadata.</p> Pos Value Description 1 <code>/shared/launch.sh</code> Entrypoint script executed in the container. 2 <code>sentinel1_GRD_preflood</code> Sentinel-1 GRD logged artifact name for the pre-flood period. 3 <code>sentinel1_GRD_postflood</code> Sentinel-1 GRD logged artifact name for the post-flood period. 4 <code>sentinel2_pre_flood</code> Sentinel-2 logged artifact name for the pre-flood period. 5 <code>sentinel2_post_flood</code> Sentinel-2 logged artifact name for the post-flood period. 6 <code>POLYGON ((...))</code> WKT geometry defining AOI for flood analysis. 7 <code>Slopes_TN</code> Logged artifact name of slope map resources. 8 <code>trentino_slope_map.tif</code> Name of Slope raster file inside 'Slopes_TN' artifact. 9 <code>Lakes_TN</code> Loggged Artifact name for lake datasets. 10 <code>idrspacq.shp</code> Name of specific Lake shapefile inside 'Lakes_TN' artifact 11 <code>Rivers_TN</code> Logged artifact name for river datasets. 12 <code>cif_pta2022_v.shp</code> Name of River network shapefile inside 'Rivers_TN' artifact. 13 <code>garda_oct_2020</code> output name 14 <code>2020-10-02</code> Event date of flood. 15 <code>EPSG:25832</code> Spatial reference system for output products. 16 <code>['VV','VH']</code> Sentinel-1 polarizations to process. 17 <code>700</code> Processing threshold (e.g., buffer, intensity, or scale parameter). 18 <code>7</code> Filter/window size parameter. 19 <code>15</code> Processing radius or kernel size. 20 <code>2</code> Flood-level classification/threshold parameter. 21 <code>val di fassa</code> Region name used for contextual labeling. <p>Example</p> <p>The following command launches the elaborate function as a containerized job, providing compute resources, storage volumes, and runtime arguments.</p> <pre><code>run_el = function_rs.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"6\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-flood\",\n        \"mount_path\": \"/app/data\",\n        \"spec\": { \"size\": \"100Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        'sentinel1_GRD_preflood',\n        'sentinel1_GRD_postflood',\n        'sentinel2_pre_flood',\n        'sentinel2_post_flood',\n        'POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))',\n        'Slopes_TN',\n        'trentino_slope_map.tif',\n        'Lakes_TN',\n        'idrspacq.shp',\n        'Rivers_TN',\n        'cif_pta2022_v.shp',\n        'garda_oct_2020',\n        '2020-10-02',\n        'EPSG:25832',\n        \"['VV','VH']\",\n        '700',\n        '7',\n        '15',\n        '2',\n        'val di fassa'\n        ]\n    )\n</code></pre>"},{"location":"functions/elaborate-flood-data/#usage","title":"Usage","text":"<p>The function expects an entry point launch script as shown below giving user the possibility to configure the runtime environment prior to elaboration. It further runs the launch instructions specified by 'launch.sh' file. </p> <pre><code>%#!/bin/bash\nls -la /shared\ncd ~\npwd\nsource .bashrc\nexport PATH=\"/home/nonroot/miniforge3/snap/bin:$PATH\"\nexport PROJ_LIB=/home/nonroot/miniforge3/share/proj\nexport GDAL_DATA=/home/nonroot/miniforge3/share/gdal\nexport GDAL_DRIVER_PATH=/home/nonroot/miniforge3/lib/gdalplugins\nexport PROJ_DATA=/home/nonroot/miniforge3/share/proj\ngdal-config --version\npython --version\necho \"GDAL DATA AFTER EXPORT:\"\necho $GDAL_DATA\necho \"PROJ_LIB AFTER EXPORT\"\necho $PROJ_LIB\necho \"Running flood mapping script with parameters:\"\necho \"{'s1PreFlood': '$1', 's1PostFlood':'$2', 's2PreFlood':'$3','s2PostFlood':'$4','geomWKT':'$5','slopeArtifact':'$6','slopeFileName':'$7','lakeShapeArtifactName':'$8','lakeShapeFileName':'$9','riverShapeArtifactName':'${10}','riverShapeFileName':'${11}','output':'${12}','eventDate':'${13}','targetCRS':'${14}','polarization':${15},'dem_threshold':${16},'slope_threshold':${17},'noise_min_pixels':${18},'river_buffer_meters':${19}}, 'aoi_name':'${20}'}\"\ncd /app\npython main.py \"{'s1PreFlood':'$1', 's1PostFlood':'$2', 's2PreFlood':'$3','s2PostFlood':'$4','geomWKT':'$5','slopeArtifact':'$6','slopeFileName':'$7','lakeShapeArtifactName':'$8','lakeShapeFileName':'$9','riverShapeArtifactName':'${10}','riverShapeFileName':'${11}','output':'${12}','eventDate':'${13}','targetCRS':'${14}','polarization':${15},'dem_threshold':${16},'slope_threshold':${17},'noise_min_pixels':${18},'river_buffer_meters':${19}, 'aoi_name':'${20}'}\"\nexit\n</code></pre> <p><pre><code>function_elaborate = proj.new_function(\"elaborate\",kind=\"container\", image=\"ghcr.io/tn-aixpa/rs-flood-mapping:0.14.6\", command=\"/bin/bash\", code_src=\"launch.sh\")\n</code></pre> Notes: For detailed usage see the usage notebook.</p>"},{"location":"functions/elaborate-flood-data/#environment","title":"Environment","text":"<p>The runtime environment of function consist of properties which are configured in the bash script created in previous section - PATH - PROJ_LIB - GDAL_DATA - GDAL_DRIVER_PATH - PROJ_DATA</p> <p><pre><code>function_elaborate.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"6\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-flood\",\n        \"mount_path\": \"/app/data\",\n        \"spec\": { \"size\": \"100Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        'sentinel1_GRD_preflood',\n        'sentinel1_GRD_postflood',\n        'sentinel2_pre_flood',\n        'sentinel2_post_flood',\n        'POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))',\n        'Slopes_TN',\n        'trentino_slope_map.tif',\n        'Lakes_TN',\n        'idrspacq.shp',\n        'Rivers_TN',\n        'cif_pta2022_v.shp',\n        'garda_oct_2020',\n        '2020-10-02',\n        'EPSG:25832',\n        \"['VV','VH']\",\n        '700',\n        '7',\n        '15',\n        '2',\n        'val di fassa'\n        ]\n    )\n</code></pre> To avoid capacity issues the environment variable \"TMPDIR\" for this function execution is set to same path of volume mount. As a general confromance to best practice approach, the container runtime is executed as non root user(fs_group='8877')</p> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>This section describes the required artifacts (sentinel data, shape and map files) needed to run the elaboration function for the requested area of interest.</p> <p>https://siat.provincia.tn.it/geonetwork/srv/ita/catalog.search#/metadata/p_TN:df06e63c-d0f3-46c9-8ec2-c25a22c50ef7</p> <p>Download the zip file from link above and extract the contents inside a folder 'Rivers_TN' and log it as project artifact</p> In\u00a0[\u00a0]: <pre>artifact_name='Rivers_TN'\nsrc_path='Rivers_TN'\nartifact_bosco = proj.log_artifact(name=artifact_name, kind=\"artifact\", source=src_path)\n</pre> <p>Similary, log the lakes shape file of the area of interest for e.g. the lake shape file for Trentino region can be downloadedfrom the SIAT Portal link below.</p> <p>https://siat.provincia.tn.it/geonetwork/srv/ita/catalog.search#/metadata/p_TN:0f1fdc33-5c71-4c6d-81e7-25eb2ab0e599</p> <p>Download the zip file from link above and extract the contents of subfolder 'idrspacq' inside a new folder 'Lakes_TN' and log it as project artifact</p> In\u00a0[\u00a0]: <pre>artifact_name='Lakes_TN'\nsrc_path='Lakes_TN'\nartifact_bosco = proj.log_artifact(name=artifact_name, kind=\"artifact\", source=src_path)\n</pre> <p>Log the slope shape file of your area of interest for e.g. for trentino region, slop shape file can be downloaded from the Huggingface repository.</p> <p>https://huggingface.co/datasets/lbergamasco/trentino-slope-map/blob/main/trentino_slope_map.tif</p> <p>Download and extract the contents inside a folder 'Slopes_TN' and log it as project artifact</p> In\u00a0[\u00a0]: <pre>artifact_name='Slopes_TN'\nsrc_path='Slopes_TN'\nartifact_bosco = proj.log_artifact(name=artifact_name, kind=\"artifact\", source=src_path)\n</pre> <p>The resulting datasets will be registered as the project artifact in the datalake under the given names ('Rivers_TN', 'Slopes_TN', 'Lakes_TN').</p> <p>Fetch the \"elaborate-flood-data\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_el = proj.get_function(\"elaborate-flood-data\") \n</pre> <p>As indicated, the elaboration of flood data depends on four distinct temporal Sentinel datasets collected around the flood event date:</p> <p>Sentinel-2: \u00b120 days</p> <p>Sentinel-1: \u00b17 days</p> <p>These temporal windows represent the default acquisition periods used by the elaborate function to detect pre- and post-flood conditions.</p> <p>Before running the elaborate function, you must log all four required Sentinel data artifacts, following the instructions provided in the Sentinel Data section above.</p> In\u00a0[\u00a0]: <pre>run_el = function_el.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"6\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-flood\",\n        \"mount_path\": \"/app/data\",\n        \"spec\": { \"size\": \"200Gi\" }\n        }],\n    args=[\n        '/shared/launch.sh',\n        'sentinel1_GRD_preflood',\n        'sentinel1_GRD_postflood',\n        'sentinel2_pre_flood',\n        'sentinel2_post_flood',\n        'POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))',\n        'Slopes_TN', \n        'trentino_slope_map.tif',\n        'Lakes_TN',\n        'idrspacq.shp',\n        'Rivers_TN',\n        'cif_pta2022_v.shp',\n        'garda_oct_2020',\n        '2020-10-02',\n        'EPSG:25832',\n        \"['VV','VH']\",\n        '700',\n        '7',\n        '15',\n        '2',\n        'val di fassa'\n        ]\n    )\n</pre> <p>As indicated in the function README, the pixel based analysis performed in the elaboration steps are computation heavy. The best possible performance matrix is more or less around the configuration indicated in the step above. The amount of sentinal data can vary. A safe limit volume of 200Gi is specified as persistent volume claim to ensure significant data space. The function takes around 40 mins to complete with 16 CPUs and 64GB Ram for a flood event indicated above. The output GeoTIFF raster file flood_detection_layer.tif is saved in the project context as an artifact zip file (garda_oct_2020) as configured output name.</p> <p> </p>"},{"location":"functions/elaborate-flood-data/#resources","title":"Resources","text":"<p>These settings define the resource requests and limits for the container runtime.</p> Resource Requests Description CPU <code>6</code> CPU cores allocated to the job. Memory <code>64Gi</code> Memory available to the container. <p>The job mounts a persistent storage volume used for reading/writing large datasets (e.g., Sentinel images).</p> Field Value Description <code>volume_type</code> <code>persistent_volume_claim</code> Indicates a persistent storage resource. <code>name</code> <code>volume-flood</code> Volume identifier. <code>mount_path</code> <code>/app/files</code> Directory inside the container where the volume is mounted. <code>size</code> <code>100Gi</code> Allocated storage capacity. <p>'elaboration' function consists of interpolation and post processing steps which are computationally heavy since it is pixel based analysis. The amount of sentinal data is huge that is why a default volume of 100Gi of type 'persistent_volume_claim' is specified to ensure significant data spacetake several hours to complete with 16 CPUs and 64GB Ram for processing data window around flood event date (\u00b120 days sentinel-2 data and \u00b1 7days Sentinel-1 data) which is the default period.</p>"},{"location":"functions/elaborate-flood-data/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create a working context (project) if not created already. Project is a placeholder for the code, function, data, and management of the data operations and workflows.</p>"},{"location":"functions/elaborate-flood-data/#2.-Prepare-data","title":"2. Prepare data\u00b6","text":""},{"location":"functions/elaborate-flood-data/#1--Sentinel-data","title":"1- Sentinel data\u00b6","text":"<p>Please note that this function 'elaborate-flood-data' depends on Sentinel data artifacts, which must be logged in to the project context(if not already) both for ascending and descending orbits using the catalog function 'download-sentinel-data'. For more detailed information, please refer to the catalog function download-sentinel-data</p>"},{"location":"functions/elaborate-flood-data/#2--Shape-and-Map-data.","title":"2- Shape and Map data.\u00b6","text":"<p>Save the necessary artifacts (shape, map files) for the required area of interest inside to the project context.  For e.g. river shape file for Trentino region can be downloaded from the open data portal link below.</p>"},{"location":"functions/elaborate-flood-data/#3.-Execution","title":"3. Execution\u00b6","text":""},{"location":"functions/elaborate-flood-data/#Function-Input-Parameters","title":"Function Input Parameters\u00b6","text":"<p>As described in the README.md, the elaboration function expects a list of arguments. The first argument is the bash script executed when the container starts, while the subsequent arguments contain both fixed and dynamic parameters.</p>"},{"location":"functions/elaborate-flood-data/#Fixed-Parameters","title":"Fixed Parameters\u00b6","text":"<p>The fixed parameter includes both the project artifacts names</p> <ul> <li><code>sentinel1_GRD_preflood</code></li> <li><code>sentinel1_GRD_postflood</code></li> <li><code>sentinel2_pre_flood</code></li> <li><code>sentinel2_post_flood</code></li> <li><code>Slopes_TN</code>,</li> <li><code>trentino_slope_map.tif</code>,</li> <li><code>Lakes_TN</code></li> <li><code>idrspacq.shp</code></li> <li><code>Rivers_TN</code></li> <li><code>cif_pta2022_v.shp</code></li> </ul> <p>as well as the the scenario configuration parameters like</p> <ul> <li><code>targetCRS</code>,</li> <li><code>polarization</code>,</li> <li><code>dem_threshold</code>,</li> <li><code>slope_threshold</code></li> <li><code>noise_min_pixels</code></li> <li><code>river_buffer_meters</code></li> </ul> <p>The set of dynamic parameters included</p> <ul> <li><code>outputName</code></li> <li><code>floodDate</code></li> <li><code>geometry</code></li> </ul> <p>These parameters are configured as per the area of interest for e.g. the launch for flood elaboration for flooding event of Oct 2, 2020 in Trentino region is shown below.</p>"},{"location":"functions/elaborate-forest-data/","title":"Elaborate forest data","text":"elaborate-forest-data Name Elaborate Deforestation Data Description Function to elaborate deforestation data using Sentinel-2 imagery Version 0.14.6 Labels apache-2.0geopasecurity0.14jobfeature-extraction UsageNotebook <p> <p></p> <p></p>"},{"location":"functions/elaborate-forest-data/#elaborate-deforestation-data","title":"Elaborate Deforestation Data","text":""},{"location":"functions/elaborate-forest-data/#overview","title":"Overview","text":"<p>This module processes and elaborates deforestation data obtained from Sentinel images for analysis and reporting.</p>"},{"location":"functions/elaborate-forest-data/#features","title":"Features","text":"<ul> <li>Data processing and validation</li> <li>Deforestation statistics generation</li> <li>Report generation</li> </ul>"},{"location":"functions/elaborate-forest-data/#functionality","title":"Functionality","text":"<p>This function performs elaboration for deforestation on Sentinel data, enhancing the analysis and reporting capabilities.</p>"},{"location":"functions/elaborate-forest-data/#contributing","title":"Contributing","text":"<p>Follow project guidelines for contributions.</p> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>This section describes the required artifacts (sentinel data, shape and map files) needed to run the elaboration function for the requested area of interest.</p> <p>https://siatservices.provincia.tn.it/idt/vector/p_TN_3d0874bc-7b9e-4c95-b885-0f7c610b08fa.zip</p> <p>Unzip the files in a folder named 'shapes_AOI' and then log it</p> In\u00a0[\u00a0]: <pre>artifact_name='Shapes_AOI'\nsrc_path='/Shapes_AOI'\nartifact_data = proj.log_artifact(name=artifact_name, kind=\"artifact\", source=src_path)\n</pre> <p>The resulting dataset will be registered as the project artifacts in the datalake under the name 'Shapes_AOI'.</p> <p>Fetch the \"elaborate-geological-data\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_el = proj.get_function(\"elaborate-forest-data\") \n</pre> <p>Run the function as shown below.</p> In\u00a0[\u00a0]: <pre>run_el = function_el.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"12\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-deforestation\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": { \"size\": \"250Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        'Shapes_AOI',\n        'data_s2_2018_19_tps',\n        \"[2018,2019]\",\n        'deforestation_2018_19_tps'\n    ]\n)\n</pre> <p> </p>"},{"location":"functions/elaborate-forest-data/#tags","title":"Tags","text":"<ul> <li><code>remote sensing</code></li> <li><code>deforestation</code></li> <li><code>sentinel-images</code></li> <li><code>thermal protection reusable (TPR)</code></li> <li><code>thermal protection systems (TPS)</code></li> <li><code>testing quality standards (TQS)</code></li> <li><code>data-processing</code></li> <li><code>analysis</code></li> <li><code>reporting</code></li> <li><code>elaboration</code></li> <li><code>functionality</code></li> <li><code>forest-loss</code></li> <li><code>change-detection</code></li> <li><code>land-cover</code></li> <li><code>time-series-analysis</code></li> <li><code>ndvi</code></li> <li><code>geospatial</code></li> <li><code>gis</code></li> <li><code>google-earth-engine</code></li> <li><code>gdal</code></li> <li><code>raster-processing</code></li> <li><code>classification</code></li> <li><code>accuracy-assessment</code></li> <li><code>validation</code></li> <li><code>monitoring</code></li> <li><code>conservation</code></li> <li><code>sustainability</code></li> <li><code>machine-learning</code></li> <li><code>python</code></li> <li><code>big-data</code></li> <li><code>open-data</code></li> </ul>"},{"location":"functions/elaborate-forest-data/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create a working context (project) if not created already. Project is a placeholder for the code, function, data, and management of the data operations and workflows.</p>"},{"location":"functions/elaborate-forest-data/#2.-Prepare-data","title":"2. Prepare data\u00b6","text":""},{"location":"functions/elaborate-forest-data/#1--Sentinel-data","title":"1- Sentinel data\u00b6","text":"<p>Please note that this function 'elaborate-geological-data' depends on Sentinel data artifacts, which must be logged in to the project context(if not already) both for ascending and descending orbits using the catalog function 'download-sentinel-data'. For more detailed information, please refer to the catalog function download-sentinel-data</p>"},{"location":"functions/elaborate-forest-data/#2--Shape-and-Map-data.","title":"2- Shape and Map data.\u00b6","text":"<p>Save the necessary artifacts (shape, map files) for the required area of interest inside to the project context.  For e.g. shape file for Trentino region can be downloaded from the open data portal link below.</p>"},{"location":"functions/elaborate-forest-data/#3.-Execution","title":"3. Execution\u00b6","text":""},{"location":"functions/elaborate-geological-data/","title":"Elaborate geological data","text":"elaborate-geological-data Name Elaborate Geological Data Description Function to elaborate landslide monitoring data using Sentinel-1 imagery Version 0.14.6 Labels apache-2.0geopasecurity0.14jobfeature-extraction UsageNotebook <p> <p></p> <p></p>"},{"location":"functions/elaborate-geological-data/#elaborate-geological-data","title":"Elaborate Geological Data","text":"<p>This function performs a complete preprocessing and analysis on geological Sentinel satellite data to extract information relevant to terrain composition, structural features, and surface variability. It detects and monitor ground deformation associated with landslides using Sentinel-1 Level-2A imagery.  The function computes, for both ascending and descending directions, the interferometry between couples of images acquired every six days and derives the total displacement, coherence map, and local incident angle. It derives the horizontal and vertical displacement components from these products and merges them to obtain their cumulative sum and the displacement between each couple of images. The coherence maps are averaged, and the results are used to filter out the areas with the lowest coherence.</p> <p>The function output GeoTiff Raster files containing: - Cumulative sum and temporal variation of the horizontal displacement ; - Cumulative sum and temporal variation of the vertical displacement; - Cumulative sum and temporal variation of the total displacement of ascending and descending Sentinel-1 images; - Cumulative sum of the horizontal displacement of the areas whaere the cumulative sum of the ascending and descending displacements has an opposite sign; - Cumulative sum of the vertical displacement of the areas where the cumulative sum of the ascending and descending displacements has an opposite sign; - Mean and temporal variation of the coherence maps; - Mean and temporal variation of the coherence maps of ascending and descending Sentinel-1 images; - Temporal variation of the C coefficient map representing the ratio of effective displacement computed in ascending and descending orbits;</p> <p>The function is implemented as a container that allows you to deploy deployments, jobs and services on Kubernetes. It uses the base image with gdal, snapista, and scikit-learn libaries installed and pre configured. It further runs the launch instructions specified by 'launch.sh' file. </p>"},{"location":"functions/elaborate-geological-data/#definition","title":"Definition","text":"<p>The function accepts a list of positional arguments that are passed directly to the Docker container. These parameters control Sentinel-1 data selection, temporal configuration, output aritifact, and AOI geometry. These arguments are passed to the container\u2019s entrypoint script.</p> Position Value Description 1 <code>/shared/launch.sh</code> Entry-point script executed inside the Docker container. Handles download and preprocessing workflow. 2 <code>s1_ascending_landslide</code> Name of artifact inside platform that contains Sentinel-1 ascending orbit cquisitions. 3 <code>s1_descending_landslide</code> Name of artifact inside platform that contains Sentinel-1 descending orbit acquisitions. 4 <code>2021-10-01</code> Start date of monitoring period window. 5 <code>2022-01-01</code> End date of monitoring period window. 6 <code>landslide_2021-10-01_2022-01-01</code> Name of output artifact. 7 <code>Shapes_AOI</code> Name of artifact inside platform containing regional shapefiles (e.g., Trentino region). 8 <code>ammprv_v.shp</code> Name of specific shapefile inside to the artifact 'Shapes_AOI' used for spatial clipping/filtering. 9 <code>Map</code> Name of artificat containing processing mode or output format label based on the workflow\u2019s internal logic. 10 <code>POLYGON ((10.595369 45.923394, 10.644894 45.923394, ...) )</code> WKT polygon defining the Area of Interest (AOI). <p>The function aims at downloading all the geological inputs specified in argument(s1_ascending_landslide, s1_descending_landslide, Shapes_AOI) from project context and perform the complex task of geological elaboration.</p> <p>Example</p> <p>The following command launches the elaborate function as a containerized job, providing compute resources, storage volumes, and runtime arguments.</p> <pre><code>run_el = function_rs.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"12\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-land\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": { \"size\": \"600Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        's1_ascending',\n        's1_descending',\n        '2021-03-01',\n        '2021-07-30',\n        'landslide_2020-11-01_2020-11-14',\n        'Shapes_AOI',\n        'ammprv_v.shp',\n        'Map',\n        'POLYGON ((10.595369 45.923394, 10.644894 45.923394, 10.644894 45.945838, \\\n                   10.595369 45.945838, 10.595369 45.923394))'\n    ]\n)\n</code></pre>"},{"location":"functions/elaborate-geological-data/#usage","title":"Usage","text":"<p>The function expect a entry point launch script as shown below giving user the possibility to configure the runtime environment prior to elaboration. It further runs the launch instructions specified by 'launch.sh' file. </p> <pre><code>%%writefile \"launch.sh\"\n#!/bin/bash\nls -la /shared\ncd ~\npwd\nsource .bashrc\nexport PATH=\"/home/nonroot/miniforge3/snap/bin:$PATH\"\nexport PROJ_LIB=/home/nonroot/miniforge3/share/proj\nexport GDAL_DATA=/home/nonroot/miniforge3/share/gdal\nexport GDAL_DRIVER_PATH=/home/nonroot/miniforge3/lib/gdalplugins\nexport PROJ_DATA=/home/nonroot/miniforge3/share/proj\ncd /app\necho \"{'s1_ascending': '$1', 's1_descending': '$2', 'startDate':'$3', 'endDate':'$4', 'outputArtifactName': '$5', 'shapeArtifactName': '$6', 'shapeFileName': '$7', 'mapArtifactName': '$8', 'geomWKT':'$9'}\"\n#export PATH=\"/home/nonroot/miniforge3/snap/.snap/auxdata/gdal/gdal-3-0-0/bin/:$PATH\"\necho \"GDAL DATA AFTER EXPORT:\"\necho $GDAL_DATA\necho \"PROJ_LIB AFTER EXPORT\"\necho $PROJ_LIB\npython main.py \"{'s1_ascending': '$1', 's1_descending': '$2', 'startDate':'$3', 'endDate':'$4', 'outputArtifactName': '$5', 'shapeArtifactName': '$6', 'shapeFileName': '$7', 'mapArtifactName': '$8', 'geomWKT':'$9'}\"\nexit\n</code></pre> <p><pre><code>function_elaborate = proj.new_function(\"elaborate\",kind=\"container\", image=\"ghcr.io/tn-aixpa/rs-landslide-monitoring:0.14.6\", command=\"/bin/bash\", code_src=\"launch.sh\")\n</code></pre> Notes: For detailed usage see the usage notebook.</p>"},{"location":"functions/elaborate-geological-data/#environment","title":"Environment","text":"<p>The runtime environment of function consist of properties which are configured in the bash script created in previous section - PATH - PROJ_LIB - GDAL_DATA - GDAL_DRIVER_PATH - PROJ_DATA</p> <p><pre><code>function_elaborate.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"12\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-land\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": { \"size\": \"600Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        's1_ascending',\n        's1_descending',\n        '2021-03-01',\n        '2021-07-30',\n        'landslide_2020-11-01_2020-11-14',\n        'Shapes_AOI',\n        'ammprv_v.shp',\n        'Map',\n        'POLYGON ((10.595369 45.923394, 10.644894 45.923394, 10.644894 45.945838, \\\n                   10.595369 45.945838, 10.595369 45.923394))'\n    ]\n)\n</code></pre> To avoid capacity issues the environment variable \"TMPDIR\" for this function execution is set to same path of volume mount. As a general confromance to best practice approach, the container runtime is executed as non root user(fs_group='8877')</p> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>This section describes the required artifacts (sentinel data, shape and map files) needed to run the elaboration function for the requested area of interest.</p> <p>https://siatservices.provincia.tn.it/idt/vector/p_TN_377793f1-1094-4e81-810e-403897418b23.zip.</p> <p>Copy the corresponding files for your area of interest inside 'Shapes_AOI' folder. for e.g. for Trention region unzip the file from link above in a folder named 'Shapes_AOI' and then log it in project context.</p> In\u00a0[\u00a0]: <pre>artifact_name='Shapes_AOI'\nsrc_path='/Shapes_AOI'\nartifact_data = proj.log_artifact(name=artifact_name, kind=\"artifact\", source=src_path)\n</pre> <p>Log the Map aritfact with three files (slope map, aspect map, and legend.qml). For e.g trentino_slope_map.tiff, trentino_aspect_map.tiff, and legend.qml can be downloaded from the Huggingface repository.</p> <p>https://huggingface.co/datasets/lbergamasco/trentino-slope-map/tree/main</p> <p>Copy the required three files for the area of interset inside a folder 'Map' and log it as project artifact</p> In\u00a0[\u00a0]: <pre>artifact_name='Map'\nsrc_path='Map'\nartifact_data = proj.log_artifact(name=artifact_name, kind=\"artifact\", source=src_path)\n</pre> <p>The resulting dataset will be registered as the project artifacts in the datalake under the name 'Shapes_AOI' and 'Map'.</p> <p>Fetch the \"elaborate-geological-data\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_el = proj.get_function(\"elaborate-geological-data\") \n</pre> <p>Run the function as shown below.</p> In\u00a0[\u00a0]: <pre>run_el = function_el.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"12\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-land\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": { \"size\": \"600Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        's1_ascending',\n        's1_descending',\n        '2021-03-01',\n        '2021-07-30',\n        'landslide_2020-11-01_2020-11-14',\n        'Shapes_AOI',\n        'ammprv_v.shp',\n        'Map',\n        'POLYGON ((10.595369 45.923394, 10.644894 45.923394, 10.644894 45.945838, \\\n                   10.595369 45.945838, 10.595369 45.923394))'\n    ]\n)\n</pre> <p> </p>"},{"location":"functions/elaborate-geological-data/#resources","title":"Resources","text":"<p>These settings define the resource requests and limits for the container runtime.</p> Resource Requests Description CPU <code>12</code> CPU cores allocated to the job. Memory <code>64Gi</code> Memory available to the container. <p>The job mounts a persistent storage volume used for reading/writing large datasets (e.g., Sentinel images).</p> Field Value Description <code>volume_type</code> <code>persistent_volume_claim</code> Indicates a persistent storage resource. <code>name</code> <code>volume-land</code> Volume identifier. <code>mount_path</code> <code>/app/files</code> Directory inside the container where the volume is mounted. <code>size</code> <code>600Gi</code> Allocated storage capacity. <p>'elaboration' consists of interferometry step which is a remote sensing technique that uses radar data to detect and monitor ground deformation associated with landslides and post processing steps which are computationally heavy since it is pixel based analysis. In some cases, the amount of sentinal data is huge that is why a default volume of 600Gi of type 'persistent_volume_claim' is specified in example to ensure significant data space. This configuration must be change according to scenario requirement. In the example given in documentation usage notebook, an elaboration on two weeks data is performed which takes ~5 hours to complete with 16 CPUs and 64GB Ram.</p>"},{"location":"functions/elaborate-geological-data/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create a working context (project) if not created already. Project is a placeholder for the code, function, data, and management of the data operations and workflows.</p>"},{"location":"functions/elaborate-geological-data/#2.-Prepare-data","title":"2. Prepare data\u00b6","text":""},{"location":"functions/elaborate-geological-data/#1--Sentinel-data","title":"1- Sentinel data\u00b6","text":"<p>Please note that this function 'elaborate-geological-data' depends on Sentinel data artifacts, which must be logged in to the project context(if not already) both for ascending and descending orbits using the catalog function 'download-sentinel-data'. For more detailed information, please refer to the catalog function download-sentinel-data</p>"},{"location":"functions/elaborate-geological-data/#2--Shape-and-Map-data.","title":"2- Shape and Map data.\u00b6","text":"<p>Save the necessary artifacts (shape, map files) for the required area of interest inside to the project context.  For e.g. shape file for Trentino region can be downloaded from the open data portal link below.</p>"},{"location":"functions/elaborate-geological-data/#3.-Execution","title":"3. Execution\u00b6","text":""},{"location":"functions/hello-world/","title":"Hello world","text":"hello-world Name Hello World Description Hello world with python: write a string to stdout Version 1.0.0 Labels apache-2.0other0.14jobutility UsageNotebook <p> <p></p> <p></p>"},{"location":"functions/hello-world/#hello-world","title":"Hello world","text":"<p>This function demonstrates a minimal example of a function whose sole purpose is to print the classic phrase \u201cHello, World!\u201d in python. This type of function is traditionally used as an introductory exercise, as it demonstrates the most fundamental structure of a program: outputting text.</p>"},{"location":"functions/hello-world/#definition","title":"Definition","text":"<pre><code>def main(param):\n    print(f\"Hello {param}\")\n</code></pre> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>Note: Make sure to replace &lt;YOUR_PROJECT_NAME&gt; with the actual name of your project before running the code.</p> <p>Fetch the \"hello-world\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_hello = proj.get_function(\"hello-world\") \n</pre> <p>Then, execute the function (locally) as specified with (local_execution=True) flag. This will execute the function as a single job in the local context. Without the local_execution flag the job will get executed on the platform. For more details about how to create, run, and view jobs inside to the digital hub platform see the documentation </p> In\u00a0[\u00a0]: <pre>function_hello.run(action='job', parameters={\"param\": \"World\"}, wait=True, local_execution=True)\n</pre> <p>As a result the 'hello world' message is logged.</p> <p>Now, we can run the same function as a job in the platform by removing the 'local_execution' flag.</p> In\u00a0[\u00a0]: <pre>function_run = function_hello.run(action='job', parameters={\"param\": \"World\"}, wait=True)\n</pre> <p>The logs of platform execution can be viewed running the cell below.</p> In\u00a0[\u00a0]: <pre>import base64\nlogs = base64.b64decode(function_run.logs()[1]['content']).decode()\nprint(logs)\n</pre> <p>Note: Alternatively, using the Core Management UI, one can navigate to 'Runs' menu , select the corresponding 'run' instance and inspect the logs using 'Logs' tab.</p> <p> </p>"},{"location":"functions/hello-world/#usage","title":"Usage","text":"<p>The function demonstrates:</p> <p>Basic syntax of a python script used inside to the platform. It demonstrates - How to define a function that takes a parameter. - How to output Hello {parameter} message to the screen</p> <p>The function 'hello-world' is registered inside to the platform core durig the import and it can be fetched and executed</p> <pre><code>func = proj.get_function(name=\"hello-world\",\n                            kind=\"python\",\n                            python_version=\"PYTHON3_10\") \n</code></pre> <p>This code fetch the created function that uses Python runtime (versione 3.10) pointing to the created file and the handler method that should be called. In this case, the code and hanlder is already embedded during the funciton import from the .yaml file.</p> <p>Then, the function can be executed on the digital hub platform or (locally) as a single job.</p> <p>Notes: For detailed usage, check the usage notebook.</p>"},{"location":"functions/hello-world/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create the working context: data download project if not created already. Project is a placeholder for the code, data, and management of the data operations.</p>"},{"location":"functions/hello-world/#2.-Execution","title":"2. Execution\u00b6","text":""}]}