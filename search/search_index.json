{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This is a catalog for the platform, where you can browse templates for its resources and find instructions and examples on how to use them.</p> <p>To know more about the platform and its features, please visit the official documentation.</p>"},{"location":"datasets/","title":"Datasets","text":"Search"},{"location":"functions/","title":"Functions","text":"Search"},{"location":"models/","title":"Models","text":"Search"},{"location":"functions/cancer-classifier-training-mlflow-example/","title":"Cancer classifier training mlflow example","text":"Cancer Classifier Training (MLFlow example) View directoryDefinitionReference \u25bchub://functions/cancer-classifier-training-mlflow-example:1.0.0Copy Name Cancer Classifier Training (MLFlow example) Description Train a cancer classifier ML model using MLFlow framework to structure and represent the model data. The trained model then can be deployed with the MLFlowServe framework or used for batch classifications loading it using the MLFlow framework. Version 1.0.0 Labels apache-2.0other0.14jobmodel-trainingsklearnhealthcare Usage"},{"location":"functions/cancer-classifier-training-mlflow-example/#cancer-classfier-training-mlflow-example","title":"Cancer classfier Training (MLFlow example)","text":"<p>This function demonstrates a minimal example of a how to train a cancer classifier ML model using MLFlow framework to structure and represent the model data. The trained model then can be deployed with the MLFlowServe framework or used for batch classifications loading it using the MLFlow framework..</p>"},{"location":"functions/cancer-classifier-training-mlflow-example/#definition","title":"Definition","text":"<pre><code>from digitalhub_runtime_python import handler\n\nfrom digitalhub import from_mlflow_run\nimport mlflow\n\nfrom sklearn import datasets, svm\nfrom sklearn.model_selection import GridSearchCV\n\n@handler()\ndef train(project):\n    mlflow.sklearn.autolog(log_datasets=True)\n\n    iris = datasets.load_iris()\n    parameters = {\"kernel\": (\"linear\", \"rbf\"), \"C\": [1, 10]}\n    svc = svm.SVC()\n    clf = GridSearchCV(svc, parameters)\n\n    clf.fit(iris.data, iris.target)\n    run_id = mlflow.last_active_run().info.run_id\n\n    # utility to map mlflow run artifacts to model metadata\n    model_params = from_mlflow_run(run_id)\n\n    project.log_model(\n        name=\"model-mlflow\",\n        kind=\"mlflow\",\n        **model_params\n</code></pre>"},{"location":"functions/cancer-classifier-training-mlflow-example/#usage","title":"Usage","text":"<p>The function demonstrates:</p> <p>Basic syntax of a python script used inside to the platform. It demonstrates</p> <ul> <li>How to define a function that takes as input 'project' object,  a context in which you can run functions and manage models,data, and artifacts.</li> <li>How to fetch input data and train model.</li> <li>How to save the trained model in project.</li> </ul> <p>The function 'mlflow-train-model' is registered inside to the platform core durig the import and it can be fetched and executed</p> <pre><code>func = proj.get_function(name=\"cancer-classfier-training-mlflow-example\") \n</code></pre> <p>This code fetch the created function that uses Python runtime (versione 3.10) pointing to the created file and the handler method that should be called. In this case, the code and hanlder is already embedded during the funciton import from the .yaml file.</p> <p>Then, the function can be executed on the digital hub platform or (locally) as a single job.</p> <p>Notes: For detailed usage, check the usage notebook.</p>"},{"location":"functions/download-sentinel-data/","title":"Download sentinel data","text":"Download Sentinel Data View directoryDefinitionReference \u25bchub://functions/download-sentinel-data:0.14.6Copy Name Download Sentinel Data Description Function for Sentinel image download and preprocessing Version 0.14.6 Labels apache-2.0geopa0.14jobdata-preparation Usage"},{"location":"functions/download-sentinel-data/#download-sentinel-data","title":"Download Sentinel Data","text":"<p>This function is used to download Sentinel satellite data efficiently. This function serves as the base for performing elaborations on different kinds of geospatial data processing tasks. Each elaboration comes with different sets of data requirements depending on the specific geospatial processing task and satellite data type being utilized. </p>"},{"location":"functions/download-sentinel-data/#definition","title":"Definition","text":"<p>This function is implemented as a Docker container and executed at runtime with user-configurable parameters. The function takes as input a parameter object that controls the satellite data selection, temporal extent, and spatial footprint. Key parameter groups include:</p> <ul> <li> <p>satelliteParams</p> <ul> <li>satelliteType: Sentinel product family (e.g., \"Sentinel-1\", \"Sentinel-2\").</li> <li>processingLevel: desired processing stage (\"LEVEL1\", \"LEVEL2\", ...).</li> <li>sensorMode / polarization: sensor acquisition mode or polarization settings (mode names depend on satellite).</li> <li>productType: product format (e.g., \"SLC\", \"GRD\", \"L2A\").</li> <li>orbitDirection: pass \"ASCENDING\" or \"DESCENDING\".</li> <li>relativeOrbitNumber: numeric orbit track identifier (optional).</li> </ul> </li> <li> <p>temporal and spatial</p> <ul> <li>startDate / endDate: ISO date strings (YYYY-MM-DD) defining the search window.</li> <li>geometry: spatial footprint in WKT or GeoJSON (polygon or bbox).</li> <li>area_sampling: boolean toggle to enable area-level sampling.</li> </ul> </li> <li> <p>storage and runtime</p> <ul> <li>tmp_path_same_folder_dwl: boolean to control temporary download placement.</li> <li>artifact_name: name used for storing the downloaded artifact in the project context.</li> </ul> </li> </ul> <p>Notes and tips: - Date ranges should be chosen to limit result sets; large ranges may require increased compute and storage. - Use relativeOrbitNumber to filter by specific orbit tracks when needed. - For Sentinel-2 searches you can also supply cloudCoverage thresholds; for Sentinel-1 specify polarization/product specifics. - Ensure valid Copernicus credentials are available to the runtime via secrets before launching jobs.</p> <p>Example parameter shape (illustrative): {     \"satelliteParams\": { \"satelliteType\": \"Sentinel-1\", \"processingLevel\": \"LEVEL1\", \"productType\": \"SLC\", \"orbitDirection\": \"DESCENDING\" },     \"startDate\": \"2021-01-01\", \"endDate\": \"2021-01-15\",     \"geometry\": \"\",     \"artifact_name\": \"download_output\" }"},{"location":"functions/download-sentinel-data/#usage","title":"Usage","text":"<p>The function is of kind container runtime that allows you to deploy deployments, jobs and services on Kubernetes. It uses the base image of sentinel-tools developed in the context of project which is a wrapper for the Sentinel download and preprocessing routine for the integration with the AIxPA platform. For more details Click here.</p> <p><pre><code>string_dict_data = \"\"\"{\n     \"satelliteParams\":{\n        \"satelliteType\": \"Sentinel2\",\n        \"bandmath\": [\"NDWI\"]\n     },\n     \"startDate\": \"2023-12-12\",\n     \"endDate\": \"2019-12-30\",\n     \"geometry\": \"POLYGON((10.88558452267069 46.2069331490752, 11.02591468396198 46.2069331490752, 11.02591468396198 46.288250617785245, 10.88558452267069 46.288250617785245, 10.88558452267069 46.2069331490752))\",\n     \"cloudCover\": \"[0,5]\",\n     \"area_sampling\": \"True\",\n     \"artifact_name\": \"sentinel2_ndwi_area_sampling_2018\"\n }\"\"\"\n\n\nlist_args =  [\"main.py\",string_dict_data]\nfunction = proj.get_function(\"download-sentinel-data\",kind=\"container\",image=\"ghcr.io/tn-aixpa/sentinel-tools:latest\",command=\"python\")\n</code></pre> Notes: For detailed usage see the usage notebook.</p>"},{"location":"functions/download-sentinel-data/#environment","title":"Environment","text":"<p>The runtime environment of function consist of properties like - CDSETOOL_ESA_PASSWORD - CDSETOOL_ESA_USER</p> <p>Register to the open data space copernicus(if not already) and get your credentials.</p> <p>https://identity.dataspace.copernicus.eu/auth/realms/CDSE/login-actions/registration?client_id=cdse-public&amp;tab_id=FIiRPJeoiX4</p> <p>Log the credentials as project secret keys as shown below</p> <pre><code># THIS NEED TO BE EXECUTED JUST ONCE\nsecret0 = proj.new_secret(name=\"CDSETOOL_ESA_USER\", secret_value=\"esa_username\")\nsecret1 = proj.new_secret(name=\"CDSETOOL_ESA_PASSWORD\", secret_value=\"esa_password\")\n</code></pre> <p>To avoid capacity issues the environment variable \"TMPDIR\" for this function execution is set to same path of volume mount. As a general confromance to best practice approach, the container runtime is executed as non root user(fs_group='8877')</p> <pre><code>function.run(\n    action=\"job\",\n    secrets=[\"CDSETOOL_ESA_USER\",\"CDSETOOL_ESA_PASSWORD\"],\n    fs_group='8877',\n    args=[\"main.py\", string_dict_data],\n    envs=[{\"name\": \"TMPDIR\", \"value\": \"/app/files\"}],\n    ...\n    }])\n</code></pre>"},{"location":"functions/download-sentinel-data/#resources","title":"Resources","text":"<p>The resources for running this function varies as per the envisaged elaboration scenario requirements which depends on many factors such as type of elaboration, data period, index, geometry etc. The download-sentinel-data function depends on Sentinel Hub dataspace. It could happen that data download takes more time than usual due to various factors, including technical issues, data processing delays, and limitations in the data access infrastructure. Sentinel data has both temporal and spatial types, as it collects data over time (temporal) with specific spatial resolutions. The size of sentinal data payload in is normally large based on requirement of usecase scenario and requires a significant block of computing resources to executed which includes number of cpu, memory(Gi), and volume(Gi). The function performance improves with significant number of cpu and memory. In general, the recommended resources(cpu, memory) for running this function:</p> <pre><code>{\n    \"resources\": {\n        \"cpu\": \"6\",\n        \"mem\": \"32Gi\"\n    }\n}\n</code></pre> <p>Data volume requirements vary by scenario: - Single scene download: 100\u2013500 MB (Sentinel-1 GRD), 500\u20131000 MB (Sentinel-2 L2A) - Multi-temporal series: Scales linearly with date range and area size - Large geographic areas: May require 10+ GB for month-long searches - Band math / preprocessing: Adds 20\u201330% overhead to storage needs</p> <p>In order to run this function, a volume of type 'persistent_volume_claim' is specified to ensure significant data space. For example, the scenarios based on environmental degradation usecases like deorestation, vegetation loss are based on temporal analysis and requires downloading of big data over a period of time. On the other hand, the scenario based on natural disasters events requires downloading of different data payloads around a given event date, compute pre/post windows of data payloads for elaboration. Inside the usage notebook, one can find more fine grained resource configurations for different kinds of data analysis for e.g. one such example is the flood scenario for which the volume configuration for data payload (\u00b1 10 days) with respect to flood event date is shown below.</p> <pre><code>{\n    \"volumes\": [\n        {\n            \"volume_type\": \"persistent_volume_claim\",\n            \"name\": \"volume-flood\",\n            \"mount_path\": \"/app/files\",\n            \"spec\": {\n                \"size\": \"100Gi\"\n            }\n        }\n    ]\n}\n</code></pre> <p>For more detailed usage for different kind of scenario, check the usage notebook.</p>"},{"location":"functions/echo-service/","title":"Echo service","text":"Echo Service View directoryDefinitionReference \u25bchub://functions/echo-service:1.0.0Copy Name Echo Service Description A simple python-based API responding to a POST message with the \"text\" field. The function exposes an API using \"serve\" task. Version 1.0.0 Labels apache-2.0other0.14serviceutilitycustom Usage"},{"location":"functions/echo-service/#echoservice","title":"echoservice","text":"<p>echoservice is a simple Python service that demonstrates how a simple Python function can be operationalizes as a service based on serverless container runtime feature. The service is exposed as an API responding to POST message with the text input and returns the same text back \u2014 an echo.</p>"},{"location":"functions/echo-service/#definition","title":"Definition","text":"<pre><code>import json\n\ndef init(context):\n    print(\"some initialization function\")\n    setattr(context, \"value\", \"some value\")\n\ndef serve(context, event):\n\n    if isinstance(event.body, bytes):\n        body = json.loads(event.body)\n    else:\n        body = event.body\n    context.logger.info(f\"Received event: {body}\")\n    text = body[\"text\"]\n\n    return {\"result\": f\"hello {text} from '{context.value}'\"}\n</code></pre> <p>The init(context) method is responsible for initializing the function\u2019s execution context.</p> <p>The serve(context, event) method contains the core functionality of the service, including the incoming request handling and execution logic.</p>"},{"location":"functions/echo-service/#usage","title":"Usage","text":"<p>The function demonstrates:</p> <ul> <li>How to define a simple serverless python function</li> <li>How to expose the underlying function as an API</li> <li>How to initialize runtime service context</li> <li>How to generate and return output</li> <li>How to call the function</li> </ul>"},{"location":"functions/echo-service/#start-the-service","title":"Start the service","text":"<pre><code>serve_run = function_echoservice.run(action='serve', wait=True)\n</code></pre>"},{"location":"functions/echo-service/#send-a-request","title":"Send a request","text":"<pre><code>json = {\n    \"text\":\"DigitalHub!\"\n}\nserve_run.invoke(json=json)\nresult.json()\n</code></pre>"},{"location":"functions/echo-service/#response","title":"Response:","text":"<pre><code>{'result': \"hello DigitalHub! from 'some value'\"}\n</code></pre> <p>Notes: For detailed usage, check the usage notebook.</p>"},{"location":"functions/elaborate-flood-data/","title":"Elaborate flood data","text":"Elaborate Flood Data View directoryDefinitionReference \u25bchub://functions/elaborate-flood-data:0.14.6Copy Name Elaborate Flood Data Description Function to elaborate flood data using Sentinel-1 and Sentinel-2 imagery Version 0.14.6 Labels apache-2.0geopasecurity0.14jobfeature-extraction Usage"},{"location":"functions/elaborate-flood-data/#elaborate-flood-data","title":"Elaborate Flood Data","text":"<p>This function performs flood analysis using Sentinel satellite data to assess flood extent and impact. It processes raw .SAFE or .zip Sentinel inputs, computes water indices, predicts water before and after a flood event, and outputs flood detection layer.</p> <p>The function provides complete workflow for - Ingesting Sentinel-1 (scene-based) and Sentinel-2 (tile-based) data using product-specific metadata. - Perform elaboration - Compute NDWI indices from Sentinel-2 imagery to detect water bodies before and after the flood event. - Calculate flood extent by analyzing pre- and post-event backscatter differences from Sentinel-1 data - Combine both results from Sentinel-1 and Sentinel-2 to have one flood prediction layer. - Post-process change maps to improve the results by masking permanent water bodies. - Log results as GeoTIFF raster files Raster and vector outputs.</p> <p>The function is implemented as a container that allows you to deploy deployments, jobs and services on Kubernetes. It uses the base image with gdal, snapista, and scikit-learn libaries installed and pre configured. It further runs the launch instructions specified by 'launch.sh' file. </p>"},{"location":"functions/elaborate-flood-data/#definition","title":"Definition","text":"<p>The function accepts a list of positional arguments that are passed directly to the Docker container. These parameters control Sentinel data selection, temporal configuration, output aritifact, and AOI geometry. These arguments are passed to the container\u2019s entrypoint script.</p> <p>These arguments define inputs, geospatial filters, auxiliary data, processing parameters, and scenario metadata.</p> Pos Value Description 1 <code>/shared/launch.sh</code> Entrypoint script executed in the container. 2 <code>sentinel1_GRD_preflood</code> Sentinel-1 GRD logged artifact name for the pre-flood period. 3 <code>sentinel1_GRD_postflood</code> Sentinel-1 GRD logged artifact name for the post-flood period. 4 <code>sentinel2_pre_flood</code> Sentinel-2 logged artifact name for the pre-flood period. 5 <code>sentinel2_post_flood</code> Sentinel-2 logged artifact name for the post-flood period. 6 <code>POLYGON ((...))</code> WKT geometry defining AOI for flood analysis. 7 <code>Slopes_TN</code> Logged artifact name of slope map resources. 8 <code>trentino_slope_map.tif</code> Name of Slope raster file inside 'Slopes_TN' artifact. 9 <code>Lakes_TN</code> Loggged Artifact name for lake datasets. 10 <code>idrspacq.shp</code> Name of specific Lake shapefile inside 'Lakes_TN' artifact 11 <code>Rivers_TN</code> Logged artifact name for river datasets. 12 <code>cif_pta2022_v.shp</code> Name of River network shapefile inside 'Rivers_TN' artifact. 13 <code>garda_oct_2020</code> output name 14 <code>2020-10-02</code> Event date of flood. 15 <code>EPSG:25832</code> Spatial reference system for output products. 16 <code>['VV','VH']</code> Sentinel-1 polarizations to process. 17 <code>700</code> Processing threshold (e.g., buffer, intensity, or scale parameter). 18 <code>7</code> Filter/window size parameter. 19 <code>15</code> Processing radius or kernel size. 20 <code>2</code> Flood-level classification/threshold parameter. 21 <code>val di fassa</code> Region name used for contextual labeling. <p>Example</p> <p>The following command launches the elaborate function as a containerized job, providing compute resources, storage volumes, and runtime arguments.</p> <pre><code>run_el = function_rs.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"6\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-flood\",\n        \"mount_path\": \"/app/data\",\n        \"spec\": { \"size\": \"100Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        'sentinel1_GRD_preflood',\n        'sentinel1_GRD_postflood',\n        'sentinel2_pre_flood',\n        'sentinel2_post_flood',\n        'POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))',\n        'Slopes_TN',\n        'trentino_slope_map.tif',\n        'Lakes_TN',\n        'idrspacq.shp',\n        'Rivers_TN',\n        'cif_pta2022_v.shp',\n        'garda_oct_2020',\n        '2020-10-02',\n        'EPSG:25832',\n        \"['VV','VH']\",\n        '700',\n        '7',\n        '15',\n        '2',\n        'val di fassa'\n        ]\n    )\n</code></pre>"},{"location":"functions/elaborate-flood-data/#usage","title":"Usage","text":"<p>The function expects an entry point launch script as shown below giving user the possibility to configure the runtime environment prior to elaboration. It further runs the launch instructions specified by 'launch.sh' file. </p> <pre><code>%#!/bin/bash\nls -la /shared\ncd ~\npwd\nsource .bashrc\nexport PATH=\"/home/nonroot/miniforge3/snap/bin:$PATH\"\nexport PROJ_LIB=/home/nonroot/miniforge3/share/proj\nexport GDAL_DATA=/home/nonroot/miniforge3/share/gdal\nexport GDAL_DRIVER_PATH=/home/nonroot/miniforge3/lib/gdalplugins\nexport PROJ_DATA=/home/nonroot/miniforge3/share/proj\ngdal-config --version\npython --version\necho \"GDAL DATA AFTER EXPORT:\"\necho $GDAL_DATA\necho \"PROJ_LIB AFTER EXPORT\"\necho $PROJ_LIB\necho \"Running flood mapping script with parameters:\"\necho \"{'s1PreFlood': '$1', 's1PostFlood':'$2', 's2PreFlood':'$3','s2PostFlood':'$4','geomWKT':'$5','slopeArtifact':'$6','slopeFileName':'$7','lakeShapeArtifactName':'$8','lakeShapeFileName':'$9','riverShapeArtifactName':'${10}','riverShapeFileName':'${11}','output':'${12}','eventDate':'${13}','targetCRS':'${14}','polarization':${15},'dem_threshold':${16},'slope_threshold':${17},'noise_min_pixels':${18},'river_buffer_meters':${19}}, 'aoi_name':'${20}'}\"\ncd /app\npython main.py \"{'s1PreFlood':'$1', 's1PostFlood':'$2', 's2PreFlood':'$3','s2PostFlood':'$4','geomWKT':'$5','slopeArtifact':'$6','slopeFileName':'$7','lakeShapeArtifactName':'$8','lakeShapeFileName':'$9','riverShapeArtifactName':'${10}','riverShapeFileName':'${11}','output':'${12}','eventDate':'${13}','targetCRS':'${14}','polarization':${15},'dem_threshold':${16},'slope_threshold':${17},'noise_min_pixels':${18},'river_buffer_meters':${19}, 'aoi_name':'${20}'}\"\nexit\n</code></pre> <p><pre><code>function_elaborate = proj.new_function(\"elaborate\",kind=\"container\", image=\"ghcr.io/tn-aixpa/rs-flood-mapping:0.14.6\", command=\"/bin/bash\", code_src=\"launch.sh\")\n</code></pre> Notes: For detailed usage see the usage notebook.</p>"},{"location":"functions/elaborate-flood-data/#environment","title":"Environment","text":"<p>The runtime environment of function consist of properties which are configured in the bash script created in previous section - PATH - PROJ_LIB - GDAL_DATA - GDAL_DRIVER_PATH - PROJ_DATA</p> <p><pre><code>function_elaborate.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"6\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-flood\",\n        \"mount_path\": \"/app/data\",\n        \"spec\": { \"size\": \"100Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        'sentinel1_GRD_preflood',\n        'sentinel1_GRD_postflood',\n        'sentinel2_pre_flood',\n        'sentinel2_post_flood',\n        'POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))',\n        'Slopes_TN',\n        'trentino_slope_map.tif',\n        'Lakes_TN',\n        'idrspacq.shp',\n        'Rivers_TN',\n        'cif_pta2022_v.shp',\n        'garda_oct_2020',\n        '2020-10-02',\n        'EPSG:25832',\n        \"['VV','VH']\",\n        '700',\n        '7',\n        '15',\n        '2',\n        'val di fassa'\n        ]\n    )\n</code></pre> To avoid capacity issues the environment variable \"TMPDIR\" for this function execution is set to same path of volume mount. As a general confromance to best practice approach, the container runtime is executed as non root user(fs_group='8877')</p>"},{"location":"functions/elaborate-flood-data/#resources","title":"Resources","text":"<p>These settings define the resource requests and limits for the container runtime.</p> Resource Requests Description CPU <code>6</code> CPU cores allocated to the job. Memory <code>64Gi</code> Memory available to the container. <p>The job mounts a persistent storage volume used for reading/writing large datasets (e.g., Sentinel images).</p> Field Value Description <code>volume_type</code> <code>persistent_volume_claim</code> Indicates a persistent storage resource. <code>name</code> <code>volume-flood</code> Volume identifier. <code>mount_path</code> <code>/app/files</code> Directory inside the container where the volume is mounted. <code>size</code> <code>100Gi</code> Allocated storage capacity. <p>'elaboration' function consists of interpolation and post processing steps which are computationally heavy since it is pixel based analysis. The amount of sentinal data is huge that is why a default volume of 100Gi of type 'persistent_volume_claim' is specified to ensure significant data spacetake several hours to complete with 16 CPUs and 64GB Ram for processing data window around flood event date (\u00b120 days sentinel-2 data and \u00b1 7days Sentinel-1 data) which is the default period.</p>"},{"location":"functions/elaborate-forest-data/","title":"Elaborate forest data","text":"Elaborate Deforestation Data View directoryDefinitionReference \u25bchub://functions/elaborate-forest-data:0.14.6Copy Name Elaborate Deforestation Data Description Function to elaborate deforestation data using Sentinel-2 imagery Version 0.14.6 Labels apache-2.0geopasecurity0.14jobfeature-extraction Usage"},{"location":"functions/elaborate-forest-data/#elaborate-deforestation-data","title":"Elaborate Deforestation Data","text":"<p>This function performs a complete preprocessing and analysis workflow for deforestation detection using Sentinel-2 Level-2A imagery. It operates on raw Sentinel-2 inputs provided in .SAFE or .zip format and focuses on the analysis of pre-downloaded spectral indices over a defined Area of Interest (AOI).</p> <p>The workflow extracts vegetation and soil indicators, specifically the Normalized Difference Vegetation Index (NDVI) and the Bare Soil Index (BSI). These indices are interpolated to generate a monthly time series, enabling temporal consistency across acquisitions. Change detection is then performed using BFAST (Breaks For Additive Season and Trend) to identify structural breaks associated with deforestation events. The function produces change detection maps and deforestation probability maps as outputs.</p> <p>Processing is carried out per Sentinel tile, allowing independent handling of overlapping tiles within the AOI (e.g. T32TQS, T32TPR, T32TPS, T32TQR). Each tile is processed separately to preserve spatial consistency and reduce edge effects. A Python-based clipping procedure is applied to convert the downloaded index data into AOI-specific input files. The clipped tiles are then used as inputs for the deforestation analysis pipeline.</p> <p>This tile-based and AOI-focused approach ensures scalable, reproducible, and spatially accurate deforestation monitoring across heterogeneous regions.</p> <p>The function provides complete workflow for - Ingesting Sentinel-2 tile-specific temporal metadata. - Perform elaboration - Compute NDVI and BSI indices from RED, NIR, and SWIR1 bands. - Apply cloud/shadow masks from precomputed binary mask files (MASK.npy). - Interpolate data to generate a complete 24-month time series (12 months/year). - Fuse features and reshape data into pixel-wise time series. - Run BFAST to detect change points across time. - Post-process change maps to remove isolated pixels and fill gaps. - Log results as GeoTIFF raster files.</p> <p>The function is implemented as a container that allows you to deploy deployments, jobs and services on Kubernetes. It uses the base image with gdal, snapista, and scikit-learn libaries installed and pre configured. It further runs the launch instructions specified by 'launch.sh' file. </p>"},{"location":"functions/elaborate-forest-data/#definition","title":"Definition","text":"<p>The function accepts a list of positional arguments that are passed directly to the Docker container. These parameters include pre-downloaded Sentinel-2 indices for a specified Area of Interest (AOI) and temporal range, producing deforestation change and probability outputs. These arguments are passed to the container\u2019s entrypoint script.</p> Position Argument Description 1 <code>/shared/launch.sh</code> Bash entrypoint script executed when the container starts. 2 <code>Shapes_AOI</code> Loggged Artifact name for AOI shapefile used for spatial clipping. 3 <code>data_s2_2018_19_tps</code> Logged artifact name for Sentinel-2 dataset (indices already downloaded). 4 <code>[2018,2019]</code> Temporal range (years) used to build the NDVI/BSI time series. 5 <code>deforestation_2018_19_tps</code> Output name / scenario identifier for generated results. <p>The function aims at downloading all the inputs specified in argument(data_s2_2018_19_tps, Shapes_AOI) from project context and perform the complex task of deforestation elaboration.</p> <p>Example</p> <p>The following command launches the elaborate function as a containerized job, providing compute resources, storage volumes, and runtime arguments.</p> <pre><code>run_el = function_rs.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"12\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-deforestation\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": { \"size\": \"250Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        'Shapes_AOI',\n        'data_s2_2018_19_tps',\n        \"[2018,2019]\",\n        'deforestation_2018_19_tps'\n    ]\n)\n</code></pre>"},{"location":"functions/elaborate-forest-data/#usage","title":"Usage","text":"<p>The function expect a entry point launch script as shown below giving user the possibility to configure the runtime environment prior to elaboration. It further runs the launch instructions specified by 'launch.sh' file. </p> <pre><code>#!/bin/bash\nls -la /shared\ncd ~\npwd\nsource .bashrc\necho \"GDAL version:\"\ngdal-config --version\npython --version\necho \"GDAL DATA:\"\necho $GDAL_DATA\necho \"PROJ_LIB\"\necho $PROJ_LIB\ncd /app\necho \"{'shapeArtifactName': '$1', 'dataArtifactName': '$2', 'years':$3, 'outputArtifactName': '$4'}\"\nexport PROJ_LIB=/home/nonroot/miniforge3/share/proj\nexport GDAL_DATA=/home/nonroot/miniforge3/share/gdal\n#export PATH=\"/home/nonroot/miniforge3/snap/.snap/auxdata/gdal/gdal-3-0-0/bin/:$PATH\"\necho \"GDAL DATA AFTER EXPORT:\"\necho $GDAL_DATA\necho \"PROJ_LIB AFTER EXPORT\"\necho $PROJ_LIB\npython main.py \"{'shapeArtifactName': '$1', 'dataArtifactName': '$2', 'years':$3, 'outputArtifactName': '$4'}\"\nexit\n</code></pre> <p><pre><code>function_elaborate = proj.new_function(\"elaborate\",kind=\"container\", image=\"ghcr.io/tn-aixpa/rs-deforestation:0.14.6\", command=\"/bin/bash\", code_src=\"launch.sh\")\n</code></pre> Notes: For detailed usage see the usage notebook.</p>"},{"location":"functions/elaborate-forest-data/#environment","title":"Environment","text":"<p>The runtime environment of function consist of properties which are configured in the bash script created in previous section - PATH - PROJ_LIB - GDAL_DATA - GDAL_DRIVER_PATH - PROJ_DATA</p> <p><pre><code>function_elaborate.un(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"12\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-deforestation\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": { \"size\": \"250Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        'Shapes_AOI',\n        'data_s2_2018_19_tps',\n        \"[2018,2019]\",\n        'deforestation_2018_19_tps'\n    ]\n)\n</code></pre> To avoid capacity issues the environment variable \"TMPDIR\" for this function execution is set to same path of volume mount. As a general confromance to best practice approach, the container runtime is executed as non root user(fs_group='8877')</p>"},{"location":"functions/elaborate-forest-data/#resources","title":"Resources","text":"<p>These settings define the resource requests and limits for the container runtime.</p> Resource Requests Description CPU <code>12</code> CPU cores allocated to the job. Memory <code>64Gi</code> Memory available to the container. <p>The job mounts a persistent storage volume used for reading/writing large datasets (e.g., Sentinel images).</p> Field Value Description <code>volume_type</code> <code>persistent_volume_claim</code> Indicates a persistent storage resource. <code>name</code> <code>volume-deforestation</code> Volume identifier. <code>mount_path</code> <code>/app/files</code> Directory inside the container where the volume is mounted. <code>size</code> <code>250Gi</code> Allocated storage capacity. <p>'elaboration' consists of interpolation and post processing steps which are computationally heavy since it is pixel based analysis. It is based on python joblib library for optimizations of numpy arrays. With the use of more images the interpolation will be shorter. The amount of sentinal data is huge that is why a volume of 250Gi of type 'persistent_volume_claim' is specified to ensure significant data space. On average the TPS tiles takes around 8-10 hours to complete with 16 CPUs and 64GB Ram for 2 years of data which is the default period.</p>"},{"location":"functions/elaborate-geological-data/","title":"Elaborate geological data","text":"Elaborate Geological Data View directoryDefinitionReference \u25bchub://functions/elaborate-geological-data:0.14.6Copy Name Elaborate Geological Data Description Function to elaborate landslide monitoring data using Sentinel-1 imagery Version 0.14.6 Labels apache-2.0geopasecurity0.14jobfeature-extraction Usage"},{"location":"functions/elaborate-geological-data/#elaborate-geological-data","title":"Elaborate Geological Data","text":"<p>This function performs a complete preprocessing and analysis on geological Sentinel satellite data to extract information relevant to terrain composition, structural features, and surface variability. It detects and monitor ground deformation associated with landslides using Sentinel-1 Level-2A imagery.  The function computes, for both ascending and descending directions, the interferometry between couples of images acquired every six days and derives the total displacement, coherence map, and local incident angle. It derives the horizontal and vertical displacement components from these products and merges them to obtain their cumulative sum and the displacement between each couple of images. The coherence maps are averaged, and the results are used to filter out the areas with the lowest coherence.</p> <p>The function output GeoTiff Raster files containing: - Cumulative sum and temporal variation of the horizontal displacement ; - Cumulative sum and temporal variation of the vertical displacement; - Cumulative sum and temporal variation of the total displacement of ascending and descending Sentinel-1 images; - Cumulative sum of the horizontal displacement of the areas whaere the cumulative sum of the ascending and descending displacements has an opposite sign; - Cumulative sum of the vertical displacement of the areas where the cumulative sum of the ascending and descending displacements has an opposite sign; - Mean and temporal variation of the coherence maps; - Mean and temporal variation of the coherence maps of ascending and descending Sentinel-1 images; - Temporal variation of the C coefficient map representing the ratio of effective displacement computed in ascending and descending orbits;</p> <p>The function is implemented as a container that allows you to deploy deployments, jobs and services on Kubernetes. It uses the base image with gdal, snapista, and scikit-learn libaries installed and pre configured. It further runs the launch instructions specified by 'launch.sh' file. </p>"},{"location":"functions/elaborate-geological-data/#definition","title":"Definition","text":"<p>The function accepts a list of positional arguments that are passed directly to the Docker container. These parameters control Sentinel-1 data selection, temporal configuration, output aritifact, and AOI geometry. These arguments are passed to the container\u2019s entrypoint script.</p> Position Value Description 1 <code>/shared/launch.sh</code> Entry-point script executed inside the Docker container. Handles download and preprocessing workflow. 2 <code>s1_ascending_landslide</code> Name of artifact inside platform that contains Sentinel-1 ascending orbit cquisitions. 3 <code>s1_descending_landslide</code> Name of artifact inside platform that contains Sentinel-1 descending orbit acquisitions. 4 <code>2021-10-01</code> Start date of monitoring period window. 5 <code>2022-01-01</code> End date of monitoring period window. 6 <code>landslide_2021-10-01_2022-01-01</code> Name of output artifact. 7 <code>Shapes_AOI</code> Name of artifact inside platform containing regional shapefiles (e.g., Trentino region). 8 <code>ammprv_v.shp</code> Name of specific shapefile inside to the artifact 'Shapes_AOI' used for spatial clipping/filtering. 9 <code>Map</code> Name of artificat containing processing mode or output format label based on the workflow\u2019s internal logic. 10 <code>POLYGON ((10.595369 45.923394, 10.644894 45.923394, ...) )</code> WKT polygon defining the Area of Interest (AOI). <p>The function aims at downloading all the geological inputs specified in argument(s1_ascending_landslide, s1_descending_landslide, Shapes_AOI) from project context and perform the complex task of geological elaboration.</p> <p>Example</p> <p>The following command launches the elaborate function as a containerized job, providing compute resources, storage volumes, and runtime arguments.</p> <pre><code>run_el = function_rs.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"12\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-land\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": { \"size\": \"600Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        's1_ascending',\n        's1_descending',\n        '2021-03-01',\n        '2021-07-30',\n        'landslide_2020-11-01_2020-11-14',\n        'Shapes_AOI',\n        'ammprv_v.shp',\n        'Map',\n        'POLYGON ((10.595369 45.923394, 10.644894 45.923394, 10.644894 45.945838, \\\n                   10.595369 45.945838, 10.595369 45.923394))'\n    ]\n)\n</code></pre>"},{"location":"functions/elaborate-geological-data/#usage","title":"Usage","text":"<p>The function expect a entry point launch script as shown below giving user the possibility to configure the runtime environment prior to elaboration. It further runs the launch instructions specified by 'launch.sh' file. </p> <pre><code>%%writefile \"launch.sh\"\n#!/bin/bash\nls -la /shared\ncd ~\npwd\nsource .bashrc\nexport PATH=\"/home/nonroot/miniforge3/snap/bin:$PATH\"\nexport PROJ_LIB=/home/nonroot/miniforge3/share/proj\nexport GDAL_DATA=/home/nonroot/miniforge3/share/gdal\nexport GDAL_DRIVER_PATH=/home/nonroot/miniforge3/lib/gdalplugins\nexport PROJ_DATA=/home/nonroot/miniforge3/share/proj\ncd /app\necho \"{'s1_ascending': '$1', 's1_descending': '$2', 'startDate':'$3', 'endDate':'$4', 'outputArtifactName': '$5', 'shapeArtifactName': '$6', 'shapeFileName': '$7', 'mapArtifactName': '$8', 'geomWKT':'$9'}\"\n#export PATH=\"/home/nonroot/miniforge3/snap/.snap/auxdata/gdal/gdal-3-0-0/bin/:$PATH\"\necho \"GDAL DATA AFTER EXPORT:\"\necho $GDAL_DATA\necho \"PROJ_LIB AFTER EXPORT\"\necho $PROJ_LIB\npython main.py \"{'s1_ascending': '$1', 's1_descending': '$2', 'startDate':'$3', 'endDate':'$4', 'outputArtifactName': '$5', 'shapeArtifactName': '$6', 'shapeFileName': '$7', 'mapArtifactName': '$8', 'geomWKT':'$9'}\"\nexit\n</code></pre> <p><pre><code>function_elaborate = proj.new_function(\"elaborate\",kind=\"container\", image=\"ghcr.io/tn-aixpa/rs-landslide-monitoring:0.14.6\", command=\"/bin/bash\", code_src=\"launch.sh\")\n</code></pre> Notes: For detailed usage see the usage notebook.</p>"},{"location":"functions/elaborate-geological-data/#environment","title":"Environment","text":"<p>The runtime environment of function consist of properties which are configured in the bash script created in previous section - PATH - PROJ_LIB - GDAL_DATA - GDAL_DRIVER_PATH - PROJ_DATA</p> <p><pre><code>function_elaborate.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"12\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-land\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": { \"size\": \"600Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        's1_ascending',\n        's1_descending',\n        '2021-03-01',\n        '2021-07-30',\n        'landslide_2020-11-01_2020-11-14',\n        'Shapes_AOI',\n        'ammprv_v.shp',\n        'Map',\n        'POLYGON ((10.595369 45.923394, 10.644894 45.923394, 10.644894 45.945838, \\\n                   10.595369 45.945838, 10.595369 45.923394))'\n    ]\n)\n</code></pre> To avoid capacity issues the environment variable \"TMPDIR\" for this function execution is set to same path of volume mount. As a general confromance to best practice approach, the container runtime is executed as non root user(fs_group='8877')</p>"},{"location":"functions/elaborate-geological-data/#resources","title":"Resources","text":"<p>These settings define the resource requests and limits for the container runtime.</p> Resource Requests Description CPU <code>12</code> CPU cores allocated to the job. Memory <code>64Gi</code> Memory available to the container. <p>The job mounts a persistent storage volume used for reading/writing large datasets (e.g., Sentinel images).</p> Field Value Description <code>volume_type</code> <code>persistent_volume_claim</code> Indicates a persistent storage resource. <code>name</code> <code>volume-land</code> Volume identifier. <code>mount_path</code> <code>/app/files</code> Directory inside the container where the volume is mounted. <code>size</code> <code>600Gi</code> Allocated storage capacity. <p>'elaboration' consists of interferometry step which is a remote sensing technique that uses radar data to detect and monitor ground deformation associated with landslides and post processing steps which are computationally heavy since it is pixel based analysis. In some cases, the amount of sentinal data is huge that is why a default volume of 600Gi of type 'persistent_volume_claim' is specified in example to ensure significant data space. This configuration must be change according to scenario requirement. In the example given in documentation usage notebook, an elaboration on two weeks data is performed which takes ~5 hours to complete with 16 CPUs and 64GB Ram.</p>"},{"location":"functions/hello-world/","title":"Hello world","text":"Hello World View directoryDefinitionReference \u25bchub://functions/hello-world:1.0.0Copy Name Hello World Description Hello world with python: write a string to stdout Version 1.0.0 Labels apache-2.0other0.14jobutility Usage"},{"location":"functions/hello-world/#hello-world","title":"Hello world","text":"<p>This function demonstrates a minimal example of a function whose sole purpose is to print the classic phrase \u201cHello, World!\u201d in python. This type of function is traditionally used as an introductory exercise, as it demonstrates the most fundamental structure of a program: outputting text.</p>"},{"location":"functions/hello-world/#definition","title":"Definition","text":"<pre><code>def main(param):\n    print(f\"Hello {param}\")\n</code></pre>"},{"location":"functions/hello-world/#usage","title":"Usage","text":"<p>The function demonstrates:</p> <p>Basic syntax of a python script used inside to the platform. It demonstrates - How to define a function that takes a parameter. - How to output Hello {parameter} message to the screen</p> <p>The function 'hello-world' is registered inside to the platform core durig the import and it can be fetched and executed</p> <pre><code>func = proj.get_function(name=\"hello-world\",\n                            kind=\"python\",\n                            python_version=\"PYTHON3_10\") \n</code></pre> <p>This code fetch the created function that uses Python runtime (versione 3.10) pointing to the created file and the handler method that should be called. In this case, the code and hanlder is already embedded during the funciton import from the .yaml file.</p> <p>Then, the function can be executed on the digital hub platform or (locally) as a single job.</p> <p>Notes: For detailed usage, check the usage notebook.</p>"}]}