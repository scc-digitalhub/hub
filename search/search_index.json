{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This is a catalog for the platform, where you can browse templates for its resources and find instructions and examples on how to use them.</p> <p>To know more about the platform and its features, please visit the official documentation.</p>"},{"location":"datasets/","title":"Datasets","text":"Search"},{"location":"functions/","title":"Functions","text":"Search"},{"location":"models/","title":"Models","text":"Search"},{"location":"functions/cancer-classifier-training-mlflow-example/","title":"Cancer classifier training mlflow example","text":"Example Cancer Classifier Training With MLFLow (Python Job) RepositoryDefinitionReference \u25bchub://functions/cancer-classifier-training-mlflow-example:1.0.0\u2750\u2713\u2750 Name Example Cancer Classifier Training With MLFLow (Python Job) Description Train a cancer classifier ML model using MLFlow framework to structure and represent the model data. The trained model then can be deployed with the MLFlowServe framework or used for batch classifications loading it using the MLFlow framework. Version 1.0.0 Labels apache-2.0other0.14jobmodel-trainingsklearnhealthcare UsageNotebook <p> <p></p> <p></p>"},{"location":"functions/cancer-classifier-training-mlflow-example/#cancer-classfier-training-mlflow-example","title":"Cancer classfier Training (MLFlow example)","text":"<p>This function demonstrates a minimal example of a how to train a cancer classifier ML model using MLFlow framework to structure and represent the model data. The trained model then can be deployed with the MLFlowServe framework or used for batch classifications loading it using the MLFlow framework..</p>"},{"location":"functions/cancer-classifier-training-mlflow-example/#definition","title":"Definition","text":"<pre><code>from digitalhub_runtime_python import handler\n\nfrom digitalhub import from_mlflow_run\nimport mlflow\n\nfrom sklearn import datasets, svm\nfrom sklearn.model_selection import GridSearchCV\n\n@handler()\ndef train(project):\n    mlflow.sklearn.autolog(log_datasets=True)\n\n    iris = datasets.load_iris()\n    parameters = {\"kernel\": (\"linear\", \"rbf\"), \"C\": [1, 10]}\n    svc = svm.SVC()\n    clf = GridSearchCV(svc, parameters)\n\n    clf.fit(iris.data, iris.target)\n    run_id = mlflow.last_active_run().info.run_id\n\n    # utility to map mlflow run artifacts to model metadata\n    model_params = from_mlflow_run(run_id)\n\n    project.log_model(\n        name=\"model-mlflow\",\n        kind=\"mlflow\",\n        **model_params\n</code></pre> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>Note: Make sure to replace &lt;YOUR_PROJECT_NAME&gt; with the actual name of your project before running the code.</p> <p>Fetch the \"hello-world\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_mlfrow_train_model = proj.get_function(\"cancer-classfier-training-mlflow-example\") \n</pre> <p>Run the function as shown below. This will trigger the model training job on the platform and the model 'model-mlflow' get logged on to the platform.</p> In\u00a0[\u00a0]: <pre>function_mlfrow_train_model.run(action='job')\n</pre> <p>Now, one can fetch the trained model saved in project.</p> In\u00a0[\u00a0]: <pre>model = proj.get_model(\"model-mlflow\")\n</pre> <p>Note: Alternatively, using the Core Management UI, one can navigate to 'Model' menu to see the generated model.</p> <p>The 'serve' action deploys MLflow ML models as services on Kubernetes. A Task is created by calling run() on the Function; task parameters are passed through that call.</p> In\u00a0[\u00a0]: <pre>serve_func = proj.new_function(\n    name=\"serve-mlflow-model\",\n    kind=\"mlflowserve\",\n    model_name=model.name,\n    path=model.key,\n)\n</pre> In\u00a0[\u00a0]: <pre>serve_run = serve_func.run(\"serve\", wait=True)\n</pre> <p>Let's test our deployed MLflow model by making a prediction request with sample Iris data:</p> In\u00a0[\u00a0]: <pre>from sklearn import datasets\n\n# Load some test data from the Iris dataset\niris = datasets.load_iris()\ndata = iris.data[0:2].tolist()\njson_payload = {\n    \"inputs\": [{\"name\": \"input-0\", \"shape\": [-1, 4], \"datatype\": \"FP64\", \"data\": data}]\n}\n\n# Make prediction\nresult = serve_run.invoke(model_name=model.name, json=json_payload).json()\nprint(\"Prediction result:\")\nprint(result)\n</pre> <p> </p>"},{"location":"functions/cancer-classifier-training-mlflow-example/#usage","title":"Usage","text":"<p>The function demonstrates:</p> <p>Basic syntax of a python script used inside to the platform. It demonstrates</p> <ul> <li>How to define a function that takes as input 'project' object,  a context in which you can run functions and manage models,data, and artifacts.</li> <li>How to fetch input data and train model.</li> <li>How to save the trained model in project.</li> </ul> <p>The function 'mlflow-train-model' is registered inside to the platform core durig the import and it can be fetched and executed</p> <pre><code>func = proj.get_function(name=\"cancer-classfier-training-mlflow-example\") \n</code></pre> <p>This code fetch the created function that uses Python runtime (versione 3.10) pointing to the created file and the handler method that should be called. In this case, the code and hanlder is already embedded during the funciton import from the .yaml file.</p> <p>Then, the function can be executed on the digital hub platform or (locally) as a single job.</p> <p>Notes: For detailed usage, check the usage notebook.</p>"},{"location":"functions/cancer-classifier-training-mlflow-example/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create the working context: data download project if not created already. Project is a placeholder for the code, data, and management of the data operations.</p>"},{"location":"functions/cancer-classifier-training-mlflow-example/#2.-Execution","title":"2. Execution\u00b6","text":""},{"location":"functions/cancer-classifier-training-mlflow-example/#Serve","title":"Serve\u00b6","text":""},{"location":"functions/cancer-classifier-training-mlflow-example/#Test-the-model","title":"Test the model\u00b6","text":""},{"location":"functions/document-classifier-serving-example/","title":"Document classifier serving example","text":"Document Classifier Serving With Bert Model (Python Serve) RepositoryDefinitionReference \u25bchub://functions/document-classifier-serving-example:1.0.0\u2750\u2713\u2750 Name Document Classifier Serving With Bert Model (Python Serve) Description Document Classifier Serving With Bert Model. The model is deployed as a service to assist with automated annotation and classification of official documents. It exposes an API using the \"serve\" task. Version 1.0.0 Labels apache-2.0other0.14serviceutilitycustompa UsageNotebook <p> <p></p> <p></p>"},{"location":"functions/document-classifier-serving-example/#document-classifier-service","title":"Document Classifier Service","text":"<p>This function demonstrates how a document classifier model can be operationalizes as a service based on serverless container runtime feature. The service is exposed as an API responding to POST message with the text input and returns the same text back \u2014 an echo.</p>"},{"location":"functions/document-classifier-serving-example/#definition","title":"Definition","text":"<pre><code>from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\nimport json\nimport pandas as pd\nimport numpy as np\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom torchmetrics.classification import MulticlassF1Score, Accuracy\n\nfrom transformers import AutoModelForSequenceClassification, AutoModel\nfrom transformers import PreTrainedModel\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\nclass BertForSentenceClassification(PreTrainedModel):\n\n    \"\"\"\n    BERT architecture is intended to be from \"dbmdz/bert-base-italian-xxl-cased\"\n    but other models can be tried.\n    \"\"\"\n\n\n    def __init__(self, config, model_name, num_labels, class_weights=None):\n        super().__init__(config)\n        self.num_labels = num_labels\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n        self.class_weights = class_weights\n        self.accuracy = Accuracy(num_classes=num_labels, task='multiclass')\n        self.f1 = MulticlassF1Score(num_classes=num_labels, average='micro') # changed weight\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n\n        loss = None\n        if labels is not None:\n\n            loss_fct = nn.CrossEntropyLoss(label_smoothing=0.1) #,weight=self.class_weights\n            loss = loss_fct(logits, labels)\n\n            f1_score = self.f1(logits.argmax(dim=1), labels)\n            accuracy_score = self.accuracy(logits.argmax(dim=1), labels)\n\n        return SequenceClassifierOutput(loss=loss, logits=logits)\n\ndef init(context):\n    model_name = \"document-classifier\"\n\n    model = context.project.get_model(model_name)\n    local_path_model = model.download(overwrite=True)\n\n    tokenizer = AutoTokenizer.from_pretrained(local_path_model)\n    config = AutoConfig.from_pretrained(local_path_model)\n\n    mm = BertForSentenceClassification.from_pretrained(\n        local_path_model,\n        config=config,\n        model_name=\"dbmdz/bert-base-italian-xxl-cased\",\n        num_labels=config.num_labels\n    ) \n\n    label_mapping = {\n        0: 0,\n        1: 2,\n        2: 4,\n        3: 7,\n        4: 8,\n        5: 10,  \n        6: 11,\n        7: 15,\n        8: 16,\n        9: 17,\n        10: 20,\n        11: 22,\n        12: 25,\n        13: 27,\n        14: 28,\n        15: 29,\n        16: 36,\n        17: 39,\n        18: 45,\n        19: 50,\n        20: 51,\n        21: 53,\n        22: 54,\n        23: 55,\n        24: 56,\n        25: 57,\n        26: 61,\n        27: 62,\n        28: 63,\n        29: 64,\n        30: 65,\n        31: 67,\n        32: 68,\n        33: 69,\n        34: 72,\n        35: 74,\n        36: 81,\n        37: 88,\n        38: 89,\n        39: 91,\n        40: 96,\n        41: 102,\n        42: 107,\n        43: 108,\n        44: 109,\n        45: 112,\n        46: 113,\n        47: 115,\n        48: 116,\n        49: 119,\n        50: 120,\n        51: 126,\n        52: 130,\n        53: 133,\n        54: 134,\n        55: 195\n    }\n\n    setattr(context, \"model\", mm)\n    setattr(context, \"tokenizer\", tokenizer)\n    setattr(context, \"label_mapping\", label_mapping)\n\ndef serve(context, event):\n\n    context.logger.info(f\"Received event: {event}\")\n\n    if isinstance(event.body, bytes):\n        body = json.loads(event.body)\n    else:\n        body = event.body\n\n    inference_input = body[\"inference_input\"]\n\n    pdf = pd.DataFrame(inference_input, index=[0])\n    k = int(pdf['k'])\n    inputs = context.tokenizer(str(pdf['text']), return_tensors=\"pt\", truncation=True, padding=True, return_token_type_ids=False)\n    context.logger.info(f\"k received: {k}\")\n\n    with torch.no_grad():\n        logits = context.model(**inputs).logits\n        sigmoid = torch.nn.Sigmoid()\n        probs = sigmoid(logits.squeeze().cpu())\n        indices_above_threshold = (probs &gt;= 0.6).nonzero(as_tuple=True)[0]\n        probs_above_threshold = probs[indices_above_threshold]\n        sorted_indices = probs_above_threshold.argsort(descending=True)\n        top_k_indices = indices_above_threshold[sorted_indices][:k]\n        result = [context.label_mapping[int(idx)] + 1 for idx in top_k_indices]\n        context.logger.info(f\"result: {result}\")\n\n    # Convert the result to a json string.\n    jsonstr = '{\"results\": ' + str(list(result)) + '}'\n\n    return json.loads(jsonstr)\n</code></pre> <p>The init(context) method is responsible for initializing the function\u2019s execution context.</p> <p>The serve(context, event) method contains the core functionality of the service, including the incoming request handling and execution logic.</p>"},{"location":"functions/document-classifier-serving-example/#usage","title":"Usage","text":"<p>The function demonstrates:</p> <ul> <li>How to define a simple serverless python functionfor document classification using BERT</li> <li>How to load a pre-trained model and tokenizer from the project</li> <li>How to implement custom model architecture with BertForSentenceClassification</li> <li>How to process inference requests with text input and return top-k predictions</li> <li>How to use label mapping for classification results</li> <li>How to expose the underlying function as an API</li> <li>How to initialize runtime service context</li> <li>How to generate and return output</li> <li>How to call the function</li> </ul>"},{"location":"functions/document-classifier-serving-example/#start-the-service","title":"Start the service","text":"<pre><code>function_doc_serve = proj.get_function(\"document-classifier-serve\")\nserve_run = function_doc_serve.run(\n    action=\"serve\",\n    resources={\"mem\": \"6Gi\"},\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"train-volume\",\n        \"mount_path\": \"/shared\",\n        \"spec\": {\n            \"size\": \"5Gi\"\n        }}],\n)\n</code></pre>"},{"location":"functions/document-classifier-serving-example/#send-a-request","title":"Send a request","text":"<pre><code>inputs = {\"text\": 'trasporto wifi ', \"k\": 1}\nserve_run.invoke(json={\"inference_input\": inputs}).text\n</code></pre> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>Note: Make sure to replace &lt;YOUR_PROJECT_NAME&gt; with the actual name of your project before running the code.</p> <p>Fetch the \"serve\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_doc_serve = proj.get_function(\"document-classifier-serve\")\n</pre> <p>Run the function with default configuration</p> In\u00a0[\u00a0]: <pre>serve_run = function_doc_serve.run(\n    action=\"serve\",\n    resources={\"mem\": \"6Gi\"},\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"train-volume\",\n        \"mount_path\": \"/shared\",\n        \"spec\": {\n            \"size\": \"5Gi\"\n        }}],\n)\n</pre> <p>Note: using the Core Management UI, one can navigate to 'Runs' menu , select the corresponding 'run' instance and inspect the logs using 'Logs' tab.</p> In\u00a0[\u00a0]: <pre>serve_run.refresh()\n</pre> In\u00a0[\u00a0]: <pre>inputs = {\"text\": 'trasporto wifi ', \"k\": 1}\nserve_run.invoke(json={\"inference_input\": inputs}).text\n</pre> <p> </p>"},{"location":"functions/document-classifier-serving-example/#response","title":"Response:","text":"<pre><code>'{\"results\": [46]}'\n</code></pre> <p>Notes: For detailed usage, check the usage notebook.</p>"},{"location":"functions/document-classifier-serving-example/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create the working context: data download project if not created already. Project is a placeholder for the code, data, and management of the data operations.</p>"},{"location":"functions/document-classifier-serving-example/#2--Generate-Model","title":"2- Generate Model\u00b6","text":"<p>Please note that this function 'document-classifier-serve' depends on Document classifier Model, which must be trained and logged in to the project context(if not already). For more detailed information, please refer to the catalog function document-classifier-train</p>"},{"location":"functions/document-classifier-serving-example/#3.-Execution","title":"3. Execution\u00b6","text":""},{"location":"functions/document-classifier-serving-example/#4.-Test-the-service","title":"4. Test the service\u00b6","text":""},{"location":"functions/document-classifier-train-example/","title":"Document classifier train example","text":"Document Classifier Training With Bert Model (Python Job) RepositoryDefinitionReference \u25bchub://functions/document-classifier-train-example:1.0.0\u2750\u2713\u2750 Name Document Classifier Training With Bert Model (Python Job) Description Document classifier relying on LLM implementations for different languages. This example uses Italian language BERT model (dbmdz/bert-base-italian-xxl-cased) for document classification. Built using HuggingFace transformers library. Version 1.0.0 Labels apache-2.0nlp0.14jobmodel-trainingpytorchpa UsageNotebook <p> <p></p> <p></p>"},{"location":"functions/document-classifier-train-example/#document-classifier-training-with-bert","title":"Document Classifier Training with Bert","text":"<p>This function provides a machine learning function for training a document classification model using BERT (Bidirectional Encoder Representations from Transformers). The classifier can categorize documents into multiple classes based on their text content.</p>"},{"location":"functions/document-classifier-train-example/#overview","title":"Overview","text":"<p>The function implements a multiclass sequence classifier fine-tuned on Italian text, designed to automatically assign taxonomy labels to documents based on their title, description, and objective.</p>"},{"location":"functions/document-classifier-train-example/#features","title":"Features","text":"<p>This function demonstrates a minimal example of a multiclass sequence classifier based on BERT base Italian, fine-tuned on a test dataset. The classifier is trained to suggest one or more labels from the taxonomy needed to categorize documents. When given a title, a description, and an objective, the classifier can predict the appropriate label from the taxonomy in use. The model can be further fine-tuned on new data.</p> <p>It demonstrate how to train classification model with BERT (Bidirectional Encoder Representations from Transformers). The classifier model is trained to suggest one or more labels within the input data. The training data looks like following table</p> text labels 0 text1.... 11 1 text2.... 12 <p>You can use the provided sample train dataset. It is also used in the embedded script as training and validation dataset. Ensure that your dataset includes a <code>text</code> column containing the document content and a <code>labels</code> column with the corresponding category labels. The dataset should be balanced, meaning each label should have a similar number of examples to avoid bias during training.</p>"},{"location":"functions/document-classifier-train-example/#definition","title":"Definition","text":"<p>The function is defined with the following key components:</p> <ul> <li>Handler: train (The entry point method that processes training requests)</li> <li>Embedded: The training logic is embedded directly in the function definition, including data preprocessing, model training, and evaluation steps</li> <li> <p>Parameters:   target_model_name: Name of the model to save</p> <ul> <li>num_train_epochs: Number of training epochs</li> <li>per_device_train_batch_size: Batch size for training</li> <li>per_device_eval_batch_size: Batch size for evaluation</li> <li>gradient_accumulation_steps: Number of steps to accumulate gradients</li> <li>weight_decay: Weight decay for regularization</li> <li>learning_rate: Learning rate for optimization</li> <li>lr_scheduler_type: Type of learning rate scheduler</li> </ul> </li> <li> <p>Outputs: </p> <ul> <li>Trained BERT-based classification model</li> <li>Training metrics and evaluation results</li> </ul> </li> </ul>"},{"location":"functions/document-classifier-train-example/#usage","title":"Usage","text":"<p>To use this function for training a document classifier:</p> <ol> <li>Prepare your dataset in CSV format with <code>text</code> and <code>labels</code> columns</li> <li>Configure the training parameters according to your requirements</li> <li>Call the function with your dataset and parameters</li> <li>The function will output a trained model and evaluation metrics</li> </ol> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>Note: Make sure to replace &lt;YOUR_PROJECT_NAME&gt; with the actual name of your project before running the code.</p> <p>Fetch the \"train\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_doc = proj.get_function(\"document-classifier-train\")\n</pre> <p>Build the function as shown below.</p> In\u00a0[\u00a0]: <pre>func_b = function_doc.run(action='build',wait=True)\n</pre> <p>Runt the built function as shown below.This will trigger the model training job on the platform and the model 'document-classifier' get logged on to the platform.</p> In\u00a0[\u00a0]: <pre>func_b.refresh()\n</pre> In\u00a0[\u00a0]: <pre># Run the function with default configuration\n</pre> In\u00a0[\u00a0]: <pre>train_run = func_b.run(action=\"job\",\n                     profile=\"1xa100\",\n                     parameters={\n                         \"target_model_name\": \"document-classifier\",\n                         \"num_train_epochs\": 1,\n                         \"per_device_train_batch_size\": 16,\n                         \"per_device_eval_batch_size\": 16,\n                         \"gradient_accumulation_steps\": 2,\n                         \"weight_decay\": 0.005,\n                         \"learning_rate\": 1e-5,\n                         \"lr_scheduler_type\": 'linear'\n                     },\n                     resources={\"mem\": \"6Gi\"},\n                     volumes=[{\n                         \"volume_type\": \"persistent_volume_claim\",\n                         \"name\": \"train-volume\",\n                         \"mount_path\": \"/local-data\",\n                         \"spec\": {\n                             \"size\": \"10Gi\"\n                         }}],\n                     envs=[\n                        {\"name\": \"HF_HOME\", \"value\": \"/local-data/huggingface\"},\n                        {\"name\": \"TRANSFORMERS_CACHE\", \"value\":  \"/local-data/huggingface\"}\n                     ],                     \n                     local_execution=False)\n</pre> In\u00a0[\u00a0]: <pre>## Configure and Run with Custom Parameters\n\nThe function can be executed with different parameters to customize the training process. Modify the parameters in the `run()` call above to adjust:\n\n- **target_model_name**: Name of the model to save\n- **num_train_epochs**: Number of training epochs\n- **per_device_train_batch_size**: Batch size for training\n- **per_device_eval_batch_size**: Batch size for evaluation\n- **gradient_accumulation_steps**: Number of steps to accumulate gradients\n- **weight_decay**: Weight decay for regularization\n- **learning_rate**: Learning rate for optimization\n- **lr_scheduler_type**: Type of learning rate scheduler\n\nYou can also adjust resource allocation (mem, GPU profile) and storage (volumes) based on your requirements.\n</pre> <p>Note: using the Core Management UI, one can navigate to 'Runs' menu , select the corresponding 'run' instance and inspect the logs using 'Logs' tab.</p> <p> </p>"},{"location":"functions/document-classifier-train-example/#example","title":"Example","text":"<p><pre><code>    # Import the function\n   function_doc = proj.get_function(\"document-classifier-training-llm-example\")\n</code></pre> <pre><code>   func_b = function_doc.run(action='build')   \n</code></pre> <pre><code>train_run = func_b.run(\n    action=\"job\",\n    profile=\"1xa100\",\n    parameters={\n        \"target_model_name\": \"document-classifier\",\n        \"num_train_epochs\": 1,\n        \"per_device_train_batch_size\": 16,\n        \"per_device_eval_batch_size\": 16,\n        \"gradient_accumulation_steps\": 2,\n        \"weight_decay\": 0.005,\n        \"learning_rate\": 1e-5,\n        \"lr_scheduler_type\": 'linear'\n    },\n    resources={\"mem\": \"6Gi\"},\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"train-volume\",\n        \"mount_path\": \"/local-data\",\n        \"spec\": {\n            \"size\": \"10Gi\"\n        }}],\n        envs=[\n            {\"name\": \"HF_HOME\", \"value\": \"/local-data/huggingface\"},\n            {\"name\": \"TRANSFORMERS_CACHE\", \"value\":  \"/local-data/huggingface\"}\n        ],\n        local_execution=False)\n</code></pre></p> <p>The trained model will be logged with the configured name 'document-classifier' and can be used for classifying new documents into the predefined categories.</p>"},{"location":"functions/document-classifier-train-example/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create the working context: data download project if not created already. Project is a placeholder for the code, data, and management of the data operations.</p>"},{"location":"functions/document-classifier-train-example/#2.-Execution","title":"2. Execution\u00b6","text":""},{"location":"functions/download-sentinel-data/","title":"Download sentinel data","text":"Download Sentinel Data (Container Job) RepositoryDefinitionReference \u25bchub://functions/download-sentinel-data:0.14.6\u2750\u2713\u2750 Name Download Sentinel Data (Container Job) Description Function for Sentinel image download and preprocessing Version 0.14.6 Labels apache-2.0geopa0.14jobdata-preparation UsageNotebook"},{"location":"functions/download-sentinel-data/#download-sentinel-data","title":"Download Sentinel Data","text":"<p>This function is used to download Sentinel satellite data efficiently. This function serves as the base for performing elaborations on different kinds of geospatial data processing tasks. Each elaboration comes with different sets of data requirements depending on the specific geospatial processing task and satellite data type being utilized. </p>"},{"location":"functions/download-sentinel-data/#definition","title":"Definition","text":"<p>This function is implemented as a Docker container and executed at runtime with user-configurable parameters. The function takes as input a parameter object that controls the satellite data selection, temporal extent, and spatial footprint. Key parameter groups include:</p> <ul> <li> <p>satelliteParams</p> <ul> <li>satelliteType: Sentinel product family (e.g., \"Sentinel-1\", \"Sentinel-2\").</li> <li>processingLevel: desired processing stage (\"LEVEL1\", \"LEVEL2\", ...).</li> <li>sensorMode / polarization: sensor acquisition mode or polarization settings (mode names depend on satellite).</li> <li>productType: product format (e.g., \"SLC\", \"GRD\", \"L2A\").</li> <li>orbitDirection: pass \"ASCENDING\" or \"DESCENDING\".</li> <li>relativeOrbitNumber: numeric orbit track identifier (optional).</li> </ul> </li> <li> <p>temporal and spatial</p> <ul> <li>startDate / endDate: ISO date strings (YYYY-MM-DD) defining the search window.</li> <li>geometry: spatial footprint in WKT or GeoJSON (polygon or bbox).</li> <li>area_sampling: boolean toggle to enable area-level sampling.</li> </ul> </li> <li> <p>storage and runtime</p> <ul> <li>tmp_path_same_folder_dwl: boolean to control temporary download placement.</li> <li>artifact_name: name used for storing the downloaded artifact in the project context.</li> </ul> </li> </ul> <p>Notes and tips: - Date ranges should be chosen to limit result sets; large ranges may require increased compute and storage. - Use relativeOrbitNumber to filter by specific orbit tracks when needed. - For Sentinel-2 searches you can also supply cloudCoverage thresholds; for Sentinel-1 specify polarization/product specifics. - Ensure valid Copernicus credentials are available to the runtime via secrets before launching jobs.</p> <p>Example parameter shape (illustrative): {     \"satelliteParams\": { \"satelliteType\": \"Sentinel-1\", \"processingLevel\": \"LEVEL1\", \"productType\": \"SLC\", \"orbitDirection\": \"DESCENDING\" },     \"startDate\": \"2021-01-01\", \"endDate\": \"2021-01-15\",     \"geometry\": \"\",     \"artifact_name\": \"download_output\" }"},{"location":"functions/download-sentinel-data/#usage","title":"Usage","text":"<p>The function is of kind container runtime that allows you to deploy deployments, jobs and services on Kubernetes. It uses the base image of sentinel-tools developed in the context of project which is a wrapper for the Sentinel download and preprocessing routine for the integration with the AIxPA platform. For more details Click here.</p> <p><pre><code>string_dict_data = \"\"\"{\n     \"satelliteParams\":{\n        \"satelliteType\": \"Sentinel2\",\n        \"bandmath\": [\"NDWI\"]\n     },\n     \"startDate\": \"2023-12-12\",\n     \"endDate\": \"2019-12-30\",\n     \"geometry\": \"POLYGON((10.88558452267069 46.2069331490752, 11.02591468396198 46.2069331490752, 11.02591468396198 46.288250617785245, 10.88558452267069 46.288250617785245, 10.88558452267069 46.2069331490752))\",\n     \"cloudCover\": \"[0,5]\",\n     \"area_sampling\": \"True\",\n     \"artifact_name\": \"sentinel2_ndwi_area_sampling_2018\"\n }\"\"\"\n\n\nlist_args =  [\"main.py\",string_dict_data]\nfunction = proj.get_function(\"download-sentinel-data\",kind=\"container\",image=\"ghcr.io/tn-aixpa/sentinel-tools:latest\",command=\"python\")\n</code></pre> Notes: For detailed usage see the usage notebook.</p>"},{"location":"functions/download-sentinel-data/#environment","title":"Environment","text":"<p>The runtime environment of function consist of properties like - CDSETOOL_ESA_PASSWORD - CDSETOOL_ESA_USER</p> <p>Register to the open data space copernicus(if not already) and get your credentials.</p> <p>https://identity.dataspace.copernicus.eu/auth/realms/CDSE/login-actions/registration?client_id=cdse-public&amp;tab_id=FIiRPJeoiX4</p> <p>Log the credentials as project secret keys as shown below</p> <pre><code># THIS NEED TO BE EXECUTED JUST ONCE\nsecret0 = proj.new_secret(name=\"CDSETOOL_ESA_USER\", secret_value=\"esa_username\")\nsecret1 = proj.new_secret(name=\"CDSETOOL_ESA_PASSWORD\", secret_value=\"esa_password\")\n</code></pre> <p>To avoid capacity issues the environment variable \"TMPDIR\" for this function execution is set to same path of volume mount. As a general confromance to best practice approach, the container runtime is executed as non root user(fs_group='8877')</p> <pre><code>function.run(\n    action=\"job\",\n    secrets=[\"CDSETOOL_ESA_USER\",\"CDSETOOL_ESA_PASSWORD\"],\n    fs_group='8877',\n    args=[\"main.py\", string_dict_data],\n    envs=[{\"name\": \"TMPDIR\", \"value\": \"/app/files\"}],\n    ...\n    }])\n</code></pre>"},{"location":"functions/download-sentinel-data/#resources","title":"Resources","text":"<p>The resources for running this function varies as per the envisaged elaboration scenario requirements which depends on many factors such as type of elaboration, data period, index, geometry etc. The download-sentinel-data function depends on Sentinel Hub dataspace. It could happen that data download takes more time than usual due to various factors, including technical issues, data processing delays, and limitations in the data access infrastructure. Sentinel data has both temporal and spatial types, as it collects data over time (temporal) with specific spatial resolutions. The size of sentinal data payload in is normally large based on requirement of usecase scenario and requires a significant block of computing resources to executed which includes number of cpu, memory(Gi), and volume(Gi). The function performance improves with significant number of cpu and memory. In general, the recommended resources(cpu, memory) for running this function:</p> <pre><code>{\n    \"resources\": {\n        \"cpu\": \"6\",\n        \"mem\": \"32Gi\"\n    }\n}\n</code></pre> <p>Data volume requirements vary by scenario: - Single scene download: 100\u2013500 MB (Sentinel-1 GRD), 500\u20131000 MB (Sentinel-2 L2A) - Multi-temporal series: Scales linearly with date range and area size - Large geographic areas: May require 10+ GB for month-long searches - Band math / preprocessing: Adds 20\u201330% overhead to storage needs</p> <p>In order to run this function, a volume of type 'persistent_volume_claim' is specified to ensure significant data space. For example, the scenarios based on environmental degradation usecases like deorestation, vegetation loss are based on temporal analysis and requires downloading of big data over a period of time. On the other hand, the scenario based on natural disasters events requires downloading of different data payloads around a given event date, compute pre/post windows of data payloads for elaboration. Inside the usage notebook, one can find more fine grained resource configurations for different kinds of data analysis for e.g. one such example is the flood scenario for which the volume configuration for data payload (\u00b1 10 days) with respect to flood event date is shown below.</p> <pre><code>{\n    \"volumes\": [\n        {\n            \"volume_type\": \"persistent_volume_claim\",\n            \"name\": \"volume-flood\",\n            \"mount_path\": \"/app/files\",\n            \"spec\": {\n                \"size\": \"100Gi\"\n            }\n        }\n    ]\n}\n</code></pre> <p>For more detailed usage for different kind of scenario, check the usage notebook.</p> <p> <p></p> <p></p> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>This section describes required fields, recommended practices, and example payloads (stringified JSON) to prepare Sentinel data downloads. Use these payloads as the <code>string_dict_data</code> argument when calling the download function in this notebook.</p> <p>As indicated in README.MD file register and store Copernicus CDSETOOL credentials in the project context as shown below</p> In\u00a0[\u00a0]: <pre>proj.set_secret(\"CDSETOOL_ESA_USER\", \"&lt;YOUR_COPERNICUS_USERNAME&gt;\")\nproj.set_secret(\"CDSETOOL_ESA_PASSWORD\", \"&lt;YOUR_COPERNICUS_PASSWORD&gt;\")\n</pre> <p>In the following sections, one can find usage examples for different kinds of geo data.</p> <p>This section demonstrates the date fetch process in case of natural disaster tragedy like geological hazards, alluvion, flood etc, the data is fetched for a given event date compute pre/post windows for elaboration and analysis. The data is prepared for those dates (ISO format) in different payloads(one per sensor/window) with distinct 'artifact' names inside to the project context.</p> In\u00a0[\u00a0]: <pre># Example payload (stringified JSON)\n{\n \"satelliteParams\":{\n    \"satelliteType\": \"Sentinel2\",\n    \"processingLevel\": \"S2MSI2A\",\n    \"bandmath\": [\"NDWI\"]\n },\n \"startDate\": \"2020-10-02\",\n \"endDate\": \"2020-10-22\",\n \"geometry\": \"POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))\",\n \"cloudCover\": \"[0,20]\",\n \"area_sampling\": \"True\",\n \"tmp_path_same_folder_dwl\": \"True\",\n \"artifact_name\": \"sentinel2_post_event\",\n \"preprocess_data_only\": \"false\"\n }\n</pre> <p>Example workflow: For flood analysis four datasets are prepared (e.g. <code>sentinel2_pre_flood</code>, <code>sentinel2_post_flood</code>, <code>sentinel1_GRD_preflood</code>, <code>sentinel1_GRD_postflood</code>). The four datasets: Sentinel\u20112 imagery for the 20\u2011day pre\u2011 and post\u2011event windows, and Sentinel\u20111 SAR for the 7\u2011day pre\u2011 and post\u2011event windows. Accordingly, the section below demonstrates four separate runs of the \"download-sentinel-data\" function \u2014 one per sensor/time window. Example payload (stringified JSON)</p> <p>Fetch the \"download-sentinel-data\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_data = proj.get_function(\"download-sentinel-data\")\n</pre> In\u00a0[\u00a0]: <pre># Example payload (stringified JSON)\n\nstring_dict_data = \"\"\"{\n \"satelliteParams\":{\n    \"satelliteType\": \"Sentinel2\",\n    \"processingLevel\": \"S2MSI2A\",\n   \"bandmath\": [\"NDWI\"]\n },\n \"startDate\": \"2020-10-02\",\n \"endDate\": \"2020-10-22\",\n \"geometry\": \"POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))\",\n \"cloudCover\": \"[0,20]\",\n \"area_sampling\": \"True\",\n \"tmp_path_same_folder_dwl\": \"True\",\n \"artifact_name\": \"sentinel2_post_flood\",\n \"preprocess_data_only\": \"false\"\n }\"\"\"\nlist_args =  [\"main.py\",string_dict_data]\n</pre> <p>Run the function. As a result the post flood sentinel-2 data is logged as project artifact(\"sentinel2_post_flood\")</p> In\u00a0[\u00a0]: <pre>func_run = function_data.run(action=\"job\",\nsecrets=[\"CDSETOOL_ESA_USER\",\"CDSETOOL_ESA_PASSWORD\"],\nfs_group=\"8877\",\nargs=list_args,\nresources={\"cpu\": \"6\",\"mem\": \"32Gi\"},\nvolumes=[{\n    \"volume_type\": \"persistent_volume_claim\",\n    \"name\": \"volume-flood\",\n    \"mount_path\": \"/app/files\",\n    \"spec\": {\n        \"size\": \"100Gi\"\n        }\n    }]\n)\n</pre> In\u00a0[\u00a0]: <pre>string_dict_data = \"\"\"{\n     \"satelliteParams\":{\n        \"satelliteType\": \"Sentinel2\",\n        \"processingLevel\": \"S2MSI2A\",\n       \"bandmath\": [\"NDWI\"]\n     },\n     \"startDate\": \"2020-09-12\",\n     \"endDate\": \"2020-10-02\",\n     \"geometry\": \"POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))\",\n     \"cloudCover\": \"[0,20]\",\n     \"area_sampling\": \"True\",\n     \"tmp_path_same_folder_dwl\": \"True\",\n     \"artifact_name\": \"sentinel2_pre_flood\",\n     \"preprocess_data_only\": \"false\"\n     }\"\"\"\nlist_args =  [\"main.py\",string_dict_data]\n</pre> <p>Run the function again. As a result the pre flood sentinel-2 data is logged as project artifact(\"sentinel2_post_flood\")</p> In\u00a0[\u00a0]: <pre>run = function_data.run(action=\"job\",\n    secrets=[\"CDSETOOL_ESA_USER\",\"CDSETOOL_ESA_PASSWORD\"],\n    fs_group=\"8877\",\n    args=list_args,\n    resources={\"cpu\": \"6\",\"mem\": \"32Gi\"},\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-flood\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": {\n            \"size\": \"100Gi\"\n            }\n    }]\n    )\n</pre> <p>The parameters passed for sentinel-1 downloads includes the starts and ends dates corresponding to period of 7 days from flood event date. The ouput of this step will be logged inside to the platfrom project context as indicated by parameter \"artifact_name\" (\"sentinel1_GRD_postflood\").Several other paramters can be configures as per requirements for e.g. geometry, cloud cover percentage etc.</p> In\u00a0[\u00a0]: <pre>string_dict_data = \"\"\"{\n\"satelliteParams\": {\n  \"satelliteType\": \"Sentinel1\",\n  \"processingLevel\": \"LEVEL1\",\n  \"sensorMode\": \"IW\",\n  \"productType\": \"GRD\"\n},\n\"startDate\": \"2020-10-02\",\n\"endDate\": \"2020-10-09\",\n\"geometry\": \"POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))\",\n\"area_sampling\": \"True\",\n\"tmp_path_same_folder_dwl\":\"True\",\n\"artifact_name\": \"sentinel1_GRD_postflood\"\n}\"\"\"\nlist_args =  [\"main.py\",string_dict_data]\n</pre> <p>Run the function. As a result the post flood sentinel-2 data is logged as project artifact(\"sentinel2_post_flood\")</p> In\u00a0[\u00a0]: <pre>run = function_data.run(action=\"job\",\n    secrets=[\"CDSETOOL_ESA_USER\",\"CDSETOOL_ESA_PASSWORD\"],\n    fs_group=\"8877\",\n    args=list_args,\n    resources={\"cpu\": \"6\",\"mem\": \"32Gi\"},\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-flood\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": {\n            \"size\": \"100Gi\"\n        }}])\n</pre> In\u00a0[\u00a0]: <pre>string_dict_data = \"\"\"{\n  \"satelliteParams\": {\n          \"satelliteType\": \"Sentinel1\",\n          \"processingLevel\": \"LEVEL1\",\n          \"sensorMode\": \"IW\",\n          \"productType\": \"GRD\"\n      },\n      \"startDate\": \"2020-09-25\",\n      \"endDate\": \"2020-10-02\",\n      \"geometry\": \"POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))\",\n      \"area_sampling\": \"True\",\n      \"tmp_path_same_folder_dwl\":\"True\",\n      \"artifact_name\": \"sentinel1_GRD_preflood\"\n  }\"\"\"\nlist_args =  [\"main.py\",string_dict_data]\n</pre> In\u00a0[\u00a0]: <pre>run = function_data.run(action=\"job\",\n    secrets=[\"CDSETOOL_ESA_USER\",\"CDSETOOL_ESA_PASSWORD\"],\n    fs_group=\"8877\",\n    args=list_args,\n    resources={\"cpu\": \"6\",\"mem\": \"32Gi\"},\n    envs={},\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-flood\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": {\n            \"size\": \"100Gi\"\n        }}])\n</pre> <p>Another example of natural disaster even is the geological hazard or landslide montioring scenario. For landslide monitoring, the \"download-sentinel-data\" function is used to fetch Sentinel\u20111 SLC scenes that cover the specified AOI geometry and time window. Retrieved images are split by orbit direction \u2014 ascending acquisitions are stored in an ascending artifact and descending acquisitions in a descending artifact \u2014 allowing independent processing workflows (e.g., SLC stacking or InSAR) for each orbit geometry. The example cells below show submitting one job per orbit direction and saving the results as project artifacts.</p> In\u00a0[\u00a0]: <pre>s1_ascending = \"s1_ascending_landslide_2020-10-01_2020-01-14\"\nstartDate = \"2020-10-01\"\nendDate = \"2020-10-14\"\ngeometry = \"POLYGON ((10.595369 45.923394, 10.644894 45.923394, 10.644894 45.945838, 10.595369 45.945838, 10.595369 45.923394))\"\nstring_dict_data_asc = \"\"\"{\n    \"satelliteParams\":{\n        \"satelliteType\": \"Sentinel1\",\n        \"processingLevel\": \"LEVEL1\",\n        \"sensorMode\": \"IW\",\"productType\": \"SLC\",\n        \"orbitDirection\": \"ASCENDING\",\n        \"relativeOrbitNumber\": \"117\"\n        },\n    \"startDate\": \\\"\"\"\" + startDate + \"\"\"\\\",\n    \"endDate\": \\\"\"\"\" + endDate + \"\"\"\\\",\n    \"geometry\": \\\"\"\"\" + geometry  + \"\"\"\\\",\n    \"area_sampling\": \"True\",\n    \"tmp_path_same_folder_dwl\":\"True\",\n    \"artifact_name\": \\\"\"\"\" + s1_ascending + \"\"\"\\\"\n    }\n\"\"\"\nlist_args =  [\"main.py\",string_dict_data_asc]\n</pre> In\u00a0[\u00a0]: <pre>run = function_data.run(action=\"job\",\n    secrets=[\"CDSETOOL_ESA_USER\",\"CDSETOOL_ESA_PASSWORD\"],\n    fs_group=\"8877\",\n    args=list_args,\n    resources={\"cpu\": \"6\",\"mem\": \"32Gi\"},\n    envs=[{\"name\": \"TMPDIR\", \"value\": \"/app/files\"}],\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-land\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": {\n            \"size\": \"100Gi\"\n        }}]\n    )\n</pre> In\u00a0[\u00a0]: <pre>s1_descending = \"s1_descending_landslide_2020-10-01_2010-01-14\"\nstartDate = \"2010-01-01\"\nendDate = \"2010-01-14\"\ngeometry = \"POLYGON ((10.595369 45.923394, 10.644894 45.923394, 10.644894 45.945838, 10.595369 45.945838, 10.595369 45.923394))\"\nstring_dict_data_des = \"\"\"{\n    \"satelliteParams\":{\n        \"satelliteType\": \"Sentinel1\",\n        \"processingLevel\": \"LEVEL1\",\n        \"sensorMode\": \"IW\",\n        \"productType\": \"SLC\",\n        \"orbitDirection\": \"DESCENDING\",\n        \"relativeOrbitNumber\": \"168\"\n        },\n    \"startDate\": \\\"\"\"\" + startDate + \"\"\"\\\",\n    \"endDate\": \\\"\"\"\" + endDate + \"\"\"\\\",\n    \"geometry\": \\\"\"\"\" + geometry + \"\"\"\\\",\n    \"tmp_path_same_folder_dwl\":\"True\",\n    \"area_sampling\": \"True\",\"artifact_name\": \\\"\"\"\" + s1_descending + \"\"\"\\\"\n    }\"\"\"\nlist_args =  [\"main.py\",string_dict_data_des]\n</pre> In\u00a0[\u00a0]: <pre>run = function_data.run(action=\"job\",\n    secrets=[\"CDSETOOL_ESA_USER\",\"CDSETOOL_ESA_PASSWORD\"],\n    fs_group=\"8877\",\n    args=list_args,\n    resources={\"cpu\": \"6\",\"mem\": \"32Gi\"},\n    envs=[{\"name\": \"TMPDIR\", \"value\": \"/app/files\"}],\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-land\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": {\n            \"size\": \"100Gi\"\n        }}]\n    )\n</pre> In\u00a0[\u00a0]: <pre>#Example payload (stringified JSON)\n{\n    \"satelliteParams\": {\n        \"satelliteType\": \"Sentinel2\",\n        \"processingLevel\": \"S2MSI2A\",\n        \"bandmath\": [\"NDVI\",\"NBR\",\"BSI\"]\n    },\n    \"startDate\": \"2018-06-01\",\n    \"endDate\": \"2018-08-31\",\n    \"geometry\": \"POLYGON ((...))\",\n    \"cloudCover\": \"[0,20]\",\n    \"area_sampling\": \"True\",\n    \"tmp_path_same_folder_dwl\": \"True\",\n    \"artifact_name\": \"sentinel2_envdeg_baseline\",\n    \"preprocess_data_only\": \"false\"\n}\n</pre> <p>Example workflow: For deforestation analysis, temporal Sentinel\u20112 Level\u20112A imagery spanning one to two years is required to build monthly time\u2011series (e.g., NDVI, BSI), enable trend and seasonality modeling, and detect change events using methods like BFAST. The temporal dataset is prepared and logged as artifact (data_s2_deforestation) inside to the project context. Accordinly, the section below demontrates a run of the 'download-sentinel-data' function.</p> <p>Fetch the \"download-sentinel-data\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_data = proj.get_function(\"download-sentinel-data\")\n</pre> In\u00a0[\u00a0]: <pre>string_dict_data = \"\"\"{\n \"satelliteParams\":{\n     \"satelliteType\": \"Sentinel2\"\n },\n \"startDate\": \"2018-01-01\",\n \"endDate\": \"2018-12-31\",\n \"geometry\": \"POLYGON((10.968432350469937 46.093829019481056,10.968432350469937 46.09650743619973, 10.97504139531014 46.09650743619973,10.97504139531014 46.093829019481056, 10.968432350469937 46.093829019481056))\",\n \"area_sampling\": \"true\",\n \"cloudCover\": \"[0,5]\",\n \"artifact_name\": \"data_s2_deforestation\"\n }\"\"\"\n\nlist_args =  [\"main.py\",string_dict_data]\n</pre> In\u00a0[\u00a0]: <pre>run = function_data.run(action=\"job\",\n    secrets=[\"CDSETOOL_ESA_USER\",\"CDSETOOL_ESA_PASSWORD\"],\n    fs_group=\"8877\",\n    args=list_args,\n    resources={\"cpu\": \"6\",\"mem\": \"32Gi\"},\n    envs=[{\"name\": \"TMPDIR\", \"value\": \"/app/files\"}],\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-deforestation\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": {\n            \"size\": \"50Gi\"\n        }}]\n    )\n</pre> <p> </p>"},{"location":"functions/download-sentinel-data/#How-to-prepare-data-for-download","title":"How to prepare data for download\u00b6","text":"<p>To prepare the geological data, it is required to log the data in the project context</p>"},{"location":"functions/download-sentinel-data/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create the working context: data download project if not created already. Project is a placeholder for the code, data, and management of the data operations.</p>"},{"location":"functions/download-sentinel-data/#2.-Preparation-guidelines-and-examples","title":"2. Preparation guidelines and examples\u00b6","text":""},{"location":"functions/download-sentinel-data/#1--Natural-disaster-events-data","title":"1- Natural disaster events data\u00b6","text":""},{"location":"functions/download-sentinel-data/#Post-Flood-Sentinel2-Data-+20days","title":"Post Flood Sentinel2 Data +20days\u00b6","text":"<p>The parameters passed for sentinel downloads includes the starts and ends dates corresponding flood event. The ouput of this step will be logged inside to the platfrom project context as indicated by parameter \"artifact_name\" (\"sentinel2_post_flood\").Several other paramters can be configures as per requirements for e.g. geometry, cloud cover percentage etc</p>"},{"location":"functions/download-sentinel-data/#Pre-Flood-Sentinel2-Data--20-days","title":"Pre Flood Sentinel2 Data -20 days\u00b6","text":""},{"location":"functions/download-sentinel-data/#Post-Flood-Sentinel1-Data-+7days","title":"Post Flood Sentinel1 Data +7days\u00b6","text":""},{"location":"functions/download-sentinel-data/#Pre-Flood-Sentinel1-Data--7days","title":"Pre Flood Sentinel1 Data -7days\u00b6","text":"<p>Similary download the sentine-1 data pre flood event.</p>"},{"location":"functions/download-sentinel-data/#Landslide-Sentinel-1-Data-acquistion-(Ascending)","title":"Landslide Sentinel 1 Data acquistion (Ascending)\u00b6","text":""},{"location":"functions/download-sentinel-data/#Landslide-Sentinel-1-Data-acquistion-(Descending)","title":"Landslide Sentinel 1 Data acquistion (Descending)\u00b6","text":""},{"location":"functions/download-sentinel-data/#2.-Environmental-Degradation-Data.","title":"2. Environmental Degradation Data.\u00b6","text":"<p>This section demonstrates the data-fetch process for environmental degradation monitoring (e.g., deforestation, vegetation loss). It explains required inputs, provides a sample payload for Sentinel-2, and lists recommended steps and best practices to obtain consistent baseline and change-detection datasets.</p> <p>Purpose</p> <ul> <li>Acquire time-series optical imagery to compute vegetation and burn/change indices (NDVI, NBR, BSI) for baseline and monitoring windows.</li> <li>Produce artifacts per time window (e.g., baseline, disturbance, post-disturbance) to support analysis (change detection, classification, trend analysis).</li> </ul>"},{"location":"functions/echo-service/","title":"Echo service","text":"Example Echo Service (Python Serve) RepositoryDefinitionReference \u25bchub://functions/echo-service:1.0.0\u2750\u2713\u2750 Name Example Echo Service (Python Serve) Description A simple python-based API responding to a POST message with the \"text\" field. The function exposes an API using \"serve\" task. Version 1.0.0 Labels apache-2.0other0.14serviceutilitycustom UsageNotebook <p> <p></p> <p></p>"},{"location":"functions/echo-service/#echoservice","title":"echoservice","text":"<p>echoservice is a simple Python service that demonstrates how a simple Python function can be operationalizes as a service based on serverless container runtime feature. The service is exposed as an API responding to POST message with the text input and returns the same text back \u2014 an echo.</p>"},{"location":"functions/echo-service/#definition","title":"Definition","text":"<pre><code>import json\n\ndef init(context):\n    print(\"some initialization function\")\n    setattr(context, \"value\", \"some value\")\n\ndef serve(context, event):\n\n    if isinstance(event.body, bytes):\n        body = json.loads(event.body)\n    else:\n        body = event.body\n    context.logger.info(f\"Received event: {body}\")\n    text = body[\"text\"]\n\n    return {\"result\": f\"hello {text} from '{context.value}'\"}\n</code></pre> <p>The init(context) method is responsible for initializing the function\u2019s execution context.</p> <p>The serve(context, event) method contains the core functionality of the service, including the incoming request handling and execution logic.</p>"},{"location":"functions/echo-service/#usage","title":"Usage","text":"<p>The function demonstrates:</p> <ul> <li>How to define a simple serverless python function</li> <li>How to expose the underlying function as an API</li> <li>How to initialize runtime service context</li> <li>How to generate and return output</li> <li>How to call the function</li> </ul>"},{"location":"functions/echo-service/#start-the-service","title":"Start the service","text":"<pre><code>serve_run = function_echoservice.run(action='serve', wait=True)\n</code></pre>"},{"location":"functions/echo-service/#send-a-request","title":"Send a request","text":"<pre><code>json = {\n    \"text\":\"DigitalHub!\"\n}\nserve_run.invoke(json=json)\nresult.json()\n</code></pre> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>Note: Make sure to replace &lt;YOUR_PROJECT_NAME&gt; with the actual name of your project before running the code.</p> <p>Fetch the \"echoservice\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_echoservice = proj.get_function(\"echoservice\") \n</pre> <p>Then, execute the function. For more details about how to create, run, and view jobs inside to the digital hub platform see the documentation </p> In\u00a0[\u00a0]: <pre>serve_run = function_echoservice.run(action='serve', wait=True)\n</pre> <p>As a result the 'echoservice' get deployed. Now fetch the URL of the service for API invocation.</p> <p>Now, we can run the same function as a job in the platform by removing the 'local_execution' flag.</p> <p>Use the invoke api of 'run' object to invoke the service API with the following input json.</p> In\u00a0[\u00a0]: <pre>json = {\n    \"text\":\"DigitalHub!\"\n}\n</pre> In\u00a0[\u00a0]: <pre>serve_run.invoke(json=json)\nresult.json()\n</pre> <p>{'result': \"hello DigitalHub! from 'some value'\"}</p> <p>Note: Alternatively, using the Core Management UI, one can navigate to 'Runs' menu , select the corresponding 'service run' instance and inspect the logs using 'Logs' tab. One call also call the API using 'Client' button inside 'service run' view</p> <p> </p>"},{"location":"functions/echo-service/#response","title":"Response:","text":"<pre><code>{'result': \"hello DigitalHub! from 'some value'\"}\n</code></pre> <p>Notes: For detailed usage, check the usage notebook.</p>"},{"location":"functions/echo-service/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create the working context: data download project if not created already. Project is a placeholder for the code, data, and management of the data operations.</p>"},{"location":"functions/echo-service/#2.-Execution","title":"2. Execution\u00b6","text":""},{"location":"functions/elaborate-flood-data/","title":"Elaborate flood data","text":"Flood Mapping Analysis (Container Job) RepositoryDefinitionReference \u25bchub://functions/elaborate-flood-data:0.14.6\u2750\u2713\u2750 Name Flood Mapping Analysis (Container Job) Description Function to elaborate  Sentinel-2 imagery data for flood mapping analysis Version 0.14.6 Labels apache-2.0geopasecurity0.14jobfeature-extraction UsageNotebook <p> <p></p> <p></p>"},{"location":"functions/elaborate-flood-data/#elaborate-flood-data","title":"Elaborate Flood Data","text":"<p>This function performs flood analysis using Sentinel satellite data to assess flood extent and impact. It processes raw .SAFE or .zip Sentinel inputs, computes water indices, predicts water before and after a flood event, and outputs flood detection layer.</p> <p>The function provides complete workflow for - Ingesting Sentinel-1 (scene-based) and Sentinel-2 (tile-based) data using product-specific metadata. - Perform elaboration - Compute NDWI indices from Sentinel-2 imagery to detect water bodies before and after the flood event. - Calculate flood extent by analyzing pre- and post-event backscatter differences from Sentinel-1 data - Combine both results from Sentinel-1 and Sentinel-2 to have one flood prediction layer. - Post-process change maps to improve the results by masking permanent water bodies. - Log results as GeoTIFF raster files Raster and vector outputs.</p> <p>The function is implemented as a container that allows you to deploy deployments, jobs and services on Kubernetes. It uses the base image with gdal, snapista, and scikit-learn libaries installed and pre configured. It further runs the launch instructions specified by 'launch.sh' file. </p>"},{"location":"functions/elaborate-flood-data/#definition","title":"Definition","text":"<p>The function accepts a list of positional arguments that are passed directly to the Docker container. These parameters control Sentinel data selection, temporal configuration, output aritifact, and AOI geometry. These arguments are passed to the container\u2019s entrypoint script.</p> <p>These arguments define inputs, geospatial filters, auxiliary data, processing parameters, and scenario metadata.</p> Pos Value Description 1 <code>/shared/launch.sh</code> Entrypoint script executed in the container. 2 <code>sentinel1_GRD_preflood</code> Sentinel-1 GRD logged artifact name for the pre-flood period. 3 <code>sentinel1_GRD_postflood</code> Sentinel-1 GRD logged artifact name for the post-flood period. 4 <code>sentinel2_pre_flood</code> Sentinel-2 logged artifact name for the pre-flood period. 5 <code>sentinel2_post_flood</code> Sentinel-2 logged artifact name for the post-flood period. 6 <code>POLYGON ((...))</code> WKT geometry defining AOI for flood analysis. 7 <code>Slopes_TN</code> Logged artifact name of slope map resources. 8 <code>trentino_slope_map.tif</code> Name of Slope raster file inside 'Slopes_TN' artifact. 9 <code>Lakes_TN</code> Loggged Artifact name for lake datasets. 10 <code>idrspacq.shp</code> Name of specific Lake shapefile inside 'Lakes_TN' artifact 11 <code>Rivers_TN</code> Logged artifact name for river datasets. 12 <code>cif_pta2022_v.shp</code> Name of River network shapefile inside 'Rivers_TN' artifact. 13 <code>garda_oct_2020</code> output name 14 <code>2020-10-02</code> Event date of flood. 15 <code>EPSG:25832</code> Spatial reference system for output products. 16 <code>['VV','VH']</code> Sentinel-1 polarizations to process. 17 <code>700</code> Processing threshold (e.g., buffer, intensity, or scale parameter). 18 <code>7</code> Filter/window size parameter. 19 <code>15</code> Processing radius or kernel size. 20 <code>2</code> Flood-level classification/threshold parameter. 21 <code>val di fassa</code> Region name used for contextual labeling. <p>Example</p> <p>The following command launches the elaborate function as a containerized job, providing compute resources, storage volumes, and runtime arguments.</p> <pre><code>run_el = function_rs.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"6\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-flood\",\n        \"mount_path\": \"/app/data\",\n        \"spec\": { \"size\": \"100Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        'sentinel1_GRD_preflood',\n        'sentinel1_GRD_postflood',\n        'sentinel2_pre_flood',\n        'sentinel2_post_flood',\n        'POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))',\n        'Slopes_TN',\n        'trentino_slope_map.tif',\n        'Lakes_TN',\n        'idrspacq.shp',\n        'Rivers_TN',\n        'cif_pta2022_v.shp',\n        'garda_oct_2020',\n        '2020-10-02',\n        'EPSG:25832',\n        \"['VV','VH']\",\n        '700',\n        '7',\n        '15',\n        '2',\n        'val di fassa'\n        ]\n    )\n</code></pre>"},{"location":"functions/elaborate-flood-data/#usage","title":"Usage","text":"<p>The function expects an entry point launch script as shown below giving user the possibility to configure the runtime environment prior to elaboration. It further runs the launch instructions specified by 'launch.sh' file. </p> <pre><code>%#!/bin/bash\nls -la /shared\ncd ~\npwd\nsource .bashrc\nexport PATH=\"/home/nonroot/miniforge3/snap/bin:$PATH\"\nexport PROJ_LIB=/home/nonroot/miniforge3/share/proj\nexport GDAL_DATA=/home/nonroot/miniforge3/share/gdal\nexport GDAL_DRIVER_PATH=/home/nonroot/miniforge3/lib/gdalplugins\nexport PROJ_DATA=/home/nonroot/miniforge3/share/proj\ngdal-config --version\npython --version\necho \"GDAL DATA AFTER EXPORT:\"\necho $GDAL_DATA\necho \"PROJ_LIB AFTER EXPORT\"\necho $PROJ_LIB\necho \"Running flood mapping script with parameters:\"\necho \"{'s1PreFlood': '$1', 's1PostFlood':'$2', 's2PreFlood':'$3','s2PostFlood':'$4','geomWKT':'$5','slopeArtifact':'$6','slopeFileName':'$7','lakeShapeArtifactName':'$8','lakeShapeFileName':'$9','riverShapeArtifactName':'${10}','riverShapeFileName':'${11}','output':'${12}','eventDate':'${13}','targetCRS':'${14}','polarization':${15},'dem_threshold':${16},'slope_threshold':${17},'noise_min_pixels':${18},'river_buffer_meters':${19}}, 'aoi_name':'${20}'}\"\ncd /app\npython main.py \"{'s1PreFlood':'$1', 's1PostFlood':'$2', 's2PreFlood':'$3','s2PostFlood':'$4','geomWKT':'$5','slopeArtifact':'$6','slopeFileName':'$7','lakeShapeArtifactName':'$8','lakeShapeFileName':'$9','riverShapeArtifactName':'${10}','riverShapeFileName':'${11}','output':'${12}','eventDate':'${13}','targetCRS':'${14}','polarization':${15},'dem_threshold':${16},'slope_threshold':${17},'noise_min_pixels':${18},'river_buffer_meters':${19}, 'aoi_name':'${20}'}\"\nexit\n</code></pre> <p><pre><code>function_elaborate = proj.new_function(\"elaborate\",kind=\"container\", image=\"ghcr.io/tn-aixpa/rs-flood-mapping:0.14.6\", command=\"/bin/bash\", code_src=\"launch.sh\")\n</code></pre> Notes: For detailed usage see the usage notebook.</p>"},{"location":"functions/elaborate-flood-data/#environment","title":"Environment","text":"<p>The runtime environment of function consist of properties which are configured in the bash script created in previous section - PATH - PROJ_LIB - GDAL_DATA - GDAL_DRIVER_PATH - PROJ_DATA</p> <p><pre><code>function_elaborate.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"6\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-flood\",\n        \"mount_path\": \"/app/data\",\n        \"spec\": { \"size\": \"100Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        'sentinel1_GRD_preflood',\n        'sentinel1_GRD_postflood',\n        'sentinel2_pre_flood',\n        'sentinel2_post_flood',\n        'POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))',\n        'Slopes_TN',\n        'trentino_slope_map.tif',\n        'Lakes_TN',\n        'idrspacq.shp',\n        'Rivers_TN',\n        'cif_pta2022_v.shp',\n        'garda_oct_2020',\n        '2020-10-02',\n        'EPSG:25832',\n        \"['VV','VH']\",\n        '700',\n        '7',\n        '15',\n        '2',\n        'val di fassa'\n        ]\n    )\n</code></pre> To avoid capacity issues the environment variable \"TMPDIR\" for this function execution is set to same path of volume mount. As a general confromance to best practice approach, the container runtime is executed as non root user(fs_group='8877')</p> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>This section describes the required artifacts (sentinel data, shape and map files) needed to run the elaboration function for the requested area of interest.</p> <p>https://siat.provincia.tn.it/geonetwork/srv/ita/catalog.search#/metadata/p_TN:df06e63c-d0f3-46c9-8ec2-c25a22c50ef7</p> <p>Download the zip file from link above and extract the contents inside a folder 'Rivers_TN' and log it as project artifact</p> In\u00a0[\u00a0]: <pre>artifact_name='Rivers_TN'\nsrc_path='Rivers_TN'\nartifact_bosco = proj.log_artifact(name=artifact_name, kind=\"artifact\", source=src_path)\n</pre> <p>Similary, log the lakes shape file of the area of interest for e.g. the lake shape file for Trentino region can be downloadedfrom the SIAT Portal link below.</p> <p>https://siat.provincia.tn.it/geonetwork/srv/ita/catalog.search#/metadata/p_TN:0f1fdc33-5c71-4c6d-81e7-25eb2ab0e599</p> <p>Download the zip file from link above and extract the contents of subfolder 'idrspacq' inside a new folder 'Lakes_TN' and log it as project artifact</p> In\u00a0[\u00a0]: <pre>artifact_name='Lakes_TN'\nsrc_path='Lakes_TN'\nartifact_bosco = proj.log_artifact(name=artifact_name, kind=\"artifact\", source=src_path)\n</pre> <p>Log the slope shape file of your area of interest for e.g. for trentino region, slop shape file can be downloaded from the Huggingface repository.</p> <p>https://huggingface.co/datasets/lbergamasco/trentino-slope-map/blob/main/trentino_slope_map.tif</p> <p>Download and extract the contents inside a folder 'Slopes_TN' and log it as project artifact</p> In\u00a0[\u00a0]: <pre>artifact_name='Slopes_TN'\nsrc_path='Slopes_TN'\nartifact_bosco = proj.log_artifact(name=artifact_name, kind=\"artifact\", source=src_path)\n</pre> <p>The resulting datasets will be registered as the project artifact in the datalake under the given names ('Rivers_TN', 'Slopes_TN', 'Lakes_TN').</p> <p>Fetch the \"elaborate-flood-data\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_el = proj.get_function(\"elaborate-flood-data\") \n</pre> <p>As indicated, the elaboration of flood data depends on four distinct temporal Sentinel datasets collected around the flood event date:</p> <p>Sentinel-2: \u00b120 days</p> <p>Sentinel-1: \u00b17 days</p> <p>These temporal windows represent the default acquisition periods used by the elaborate function to detect pre- and post-flood conditions.</p> <p>Before running the elaborate function, you must log all four required Sentinel data artifacts, following the instructions provided in the Sentinel Data section above.</p> In\u00a0[\u00a0]: <pre>run_el = function_el.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"6\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-flood\",\n        \"mount_path\": \"/app/data\",\n        \"spec\": { \"size\": \"200Gi\" }\n        }],\n    args=[\n        '/shared/launch.sh',\n        'sentinel1_GRD_preflood',\n        'sentinel1_GRD_postflood',\n        'sentinel2_pre_flood',\n        'sentinel2_post_flood',\n        'POLYGON ((10.644988646837982 45.85539621678084, 10.644988646837982 46.06780100571985, 10.991744628283294 46.06780100571985, 10.991744628283294 45.85539621678084, 10.644988646837982 45.85539621678084))',\n        'Slopes_TN', \n        'trentino_slope_map.tif',\n        'Lakes_TN',\n        'idrspacq.shp',\n        'Rivers_TN',\n        'cif_pta2022_v.shp',\n        'garda_oct_2020',\n        '2020-10-02',\n        'EPSG:25832',\n        \"['VV','VH']\",\n        '700',\n        '7',\n        '15',\n        '2',\n        'val di fassa'\n        ]\n    )\n</pre> <p>As indicated in the function README, the pixel based analysis performed in the elaboration steps are computation heavy. The best possible performance matrix is more or less around the configuration indicated in the step above. The amount of sentinal data can vary. A safe limit volume of 200Gi is specified as persistent volume claim to ensure significant data space. The function takes around 40 mins to complete with 16 CPUs and 64GB Ram for a flood event indicated above. The output GeoTIFF raster file flood_detection_layer.tif is saved in the project context as an artifact zip file (garda_oct_2020) as configured output name.</p> <p> </p>"},{"location":"functions/elaborate-flood-data/#resources","title":"Resources","text":"<p>These settings define the resource requests and limits for the container runtime.</p> Resource Requests Description CPU <code>6</code> CPU cores allocated to the job. Memory <code>64Gi</code> Memory available to the container. <p>The job mounts a persistent storage volume used for reading/writing large datasets (e.g., Sentinel images).</p> Field Value Description <code>volume_type</code> <code>persistent_volume_claim</code> Indicates a persistent storage resource. <code>name</code> <code>volume-flood</code> Volume identifier. <code>mount_path</code> <code>/app/files</code> Directory inside the container where the volume is mounted. <code>size</code> <code>100Gi</code> Allocated storage capacity. <p>'elaboration' function consists of interpolation and post processing steps which are computationally heavy since it is pixel based analysis. The amount of sentinal data is huge that is why a default volume of 100Gi of type 'persistent_volume_claim' is specified to ensure significant data spacetake several hours to complete with 16 CPUs and 64GB Ram for processing data window around flood event date (\u00b120 days sentinel-2 data and \u00b1 7days Sentinel-1 data) which is the default period.</p>"},{"location":"functions/elaborate-flood-data/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create a working context (project) if not created already. Project is a placeholder for the code, function, data, and management of the data operations and workflows.</p>"},{"location":"functions/elaborate-flood-data/#2.-Prepare-data","title":"2. Prepare data\u00b6","text":""},{"location":"functions/elaborate-flood-data/#1--Sentinel-data","title":"1- Sentinel data\u00b6","text":"<p>Please note that this function 'elaborate-flood-data' depends on Sentinel data artifacts, which must be logged in to the project context(if not already) both for ascending and descending orbits using the catalog function 'download-sentinel-data'. For more detailed information, please refer to the catalog function download-sentinel-data</p>"},{"location":"functions/elaborate-flood-data/#2--Shape-and-Map-data.","title":"2- Shape and Map data.\u00b6","text":"<p>Save the necessary artifacts (shape, map files) for the required area of interest inside to the project context.  For e.g. river shape file for Trentino region can be downloaded from the open data portal link below.</p>"},{"location":"functions/elaborate-flood-data/#3.-Execution","title":"3. Execution\u00b6","text":""},{"location":"functions/elaborate-flood-data/#Function-Input-Parameters","title":"Function Input Parameters\u00b6","text":"<p>As described in the README.md, the elaboration function expects a list of arguments. The first argument is the bash script executed when the container starts, while the subsequent arguments contain both fixed and dynamic parameters.</p>"},{"location":"functions/elaborate-flood-data/#Fixed-Parameters","title":"Fixed Parameters\u00b6","text":"<p>The fixed parameter includes both the project artifacts names</p> <ul> <li><code>sentinel1_GRD_preflood</code></li> <li><code>sentinel1_GRD_postflood</code></li> <li><code>sentinel2_pre_flood</code></li> <li><code>sentinel2_post_flood</code></li> <li><code>Slopes_TN</code>,</li> <li><code>trentino_slope_map.tif</code>,</li> <li><code>Lakes_TN</code></li> <li><code>idrspacq.shp</code></li> <li><code>Rivers_TN</code></li> <li><code>cif_pta2022_v.shp</code></li> </ul> <p>as well as the the scenario configuration parameters like</p> <ul> <li><code>targetCRS</code>,</li> <li><code>polarization</code>,</li> <li><code>dem_threshold</code>,</li> <li><code>slope_threshold</code></li> <li><code>noise_min_pixels</code></li> <li><code>river_buffer_meters</code></li> </ul> <p>The set of dynamic parameters included</p> <ul> <li><code>outputName</code></li> <li><code>floodDate</code></li> <li><code>geometry</code></li> </ul> <p>These parameters are configured as per the area of interest for e.g. the launch for flood elaboration for flooding event of Oct 2, 2020 in Trentino region is shown below.</p>"},{"location":"functions/elaborate-forest-data/","title":"Elaborate forest data","text":"Deforestation Analysis (Container Job) RepositoryDefinitionReference \u25bchub://functions/elaborate-forest-data:0.14.6\u2750\u2713\u2750 Name Deforestation Analysis (Container Job) Description Function to elaborate Sentinel-2 imagery data for deforestation analysis Version 0.14.6 Labels apache-2.0geopasecurity0.14jobfeature-extraction UsageNotebook <p> <p></p> <p></p>"},{"location":"functions/elaborate-forest-data/#elaborate-deforestation-data","title":"Elaborate Deforestation Data","text":"<p>This function performs a complete preprocessing and analysis workflow for deforestation detection using Sentinel-2 Level-2A imagery. It operates on raw Sentinel-2 inputs provided in .SAFE or .zip format and focuses on the analysis of pre-downloaded spectral indices over a defined Area of Interest (AOI).</p> <p>The workflow extracts vegetation and soil indicators, specifically the Normalized Difference Vegetation Index (NDVI) and the Bare Soil Index (BSI). These indices are interpolated to generate a monthly time series, enabling temporal consistency across acquisitions. Change detection is then performed using BFAST (Breaks For Additive Season and Trend) to identify structural breaks associated with deforestation events. The function produces change detection maps and deforestation probability maps as outputs.</p> <p>Processing is carried out per Sentinel tile, allowing independent handling of overlapping tiles within the AOI (e.g. T32TQS, T32TPR, T32TPS, T32TQR). Each tile is processed separately to preserve spatial consistency and reduce edge effects. A Python-based clipping procedure is applied to convert the downloaded index data into AOI-specific input files. The clipped tiles are then used as inputs for the deforestation analysis pipeline.</p> <p>This tile-based and AOI-focused approach ensures scalable, reproducible, and spatially accurate deforestation monitoring across heterogeneous regions.</p> <p>The function provides complete workflow for - Ingesting Sentinel-2 tile-specific temporal metadata. - Perform elaboration - Compute NDVI and BSI indices from RED, NIR, and SWIR1 bands. - Apply cloud/shadow masks from precomputed binary mask files (MASK.npy). - Interpolate data to generate a complete 24-month time series (12 months/year). - Fuse features and reshape data into pixel-wise time series. - Run BFAST to detect change points across time. - Post-process change maps to remove isolated pixels and fill gaps. - Log results as GeoTIFF raster files.</p> <p>The function is implemented as a container that allows you to deploy deployments, jobs and services on Kubernetes. It uses the base image with gdal, snapista, and scikit-learn libaries installed and pre configured. It further runs the launch instructions specified by 'launch.sh' file. </p>"},{"location":"functions/elaborate-forest-data/#definition","title":"Definition","text":"<p>The function accepts a list of positional arguments that are passed directly to the Docker container. These parameters include pre-downloaded Sentinel-2 indices for a specified Area of Interest (AOI) and temporal range, producing deforestation change and probability outputs. These arguments are passed to the container\u2019s entrypoint script.</p> Position Argument Description 1 <code>/shared/launch.sh</code> Bash entrypoint script executed when the container starts. 2 <code>Shapes_AOI</code> Loggged Artifact name for AOI shapefile used for spatial clipping. 3 <code>data_s2_2018_19_tps</code> Logged artifact name for Sentinel-2 dataset (indices already downloaded). 4 <code>[2018,2019]</code> Temporal range (years) used to build the NDVI/BSI time series. 5 <code>deforestation_2018_19_tps</code> Output name / scenario identifier for generated results. <p>The function aims at downloading all the inputs specified in argument(data_s2_2018_19_tps, Shapes_AOI) from project context and perform the complex task of deforestation elaboration.</p> <p>Example</p> <p>The following command launches the elaborate function as a containerized job, providing compute resources, storage volumes, and runtime arguments.</p> <pre><code>run_el = function_rs.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"12\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-deforestation\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": { \"size\": \"250Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        'Shapes_AOI',\n        'data_s2_2018_19_tps',\n        \"[2018,2019]\",\n        'deforestation_2018_19_tps'\n    ]\n)\n</code></pre>"},{"location":"functions/elaborate-forest-data/#usage","title":"Usage","text":"<p>The function expect a entry point launch script as shown below giving user the possibility to configure the runtime environment prior to elaboration. It further runs the launch instructions specified by 'launch.sh' file. </p> <pre><code>#!/bin/bash\nls -la /shared\ncd ~\npwd\nsource .bashrc\necho \"GDAL version:\"\ngdal-config --version\npython --version\necho \"GDAL DATA:\"\necho $GDAL_DATA\necho \"PROJ_LIB\"\necho $PROJ_LIB\ncd /app\necho \"{'shapeArtifactName': '$1', 'dataArtifactName': '$2', 'years':$3, 'outputArtifactName': '$4'}\"\nexport PROJ_LIB=/home/nonroot/miniforge3/share/proj\nexport GDAL_DATA=/home/nonroot/miniforge3/share/gdal\n#export PATH=\"/home/nonroot/miniforge3/snap/.snap/auxdata/gdal/gdal-3-0-0/bin/:$PATH\"\necho \"GDAL DATA AFTER EXPORT:\"\necho $GDAL_DATA\necho \"PROJ_LIB AFTER EXPORT\"\necho $PROJ_LIB\npython main.py \"{'shapeArtifactName': '$1', 'dataArtifactName': '$2', 'years':$3, 'outputArtifactName': '$4'}\"\nexit\n</code></pre> <p><pre><code>function_elaborate = proj.new_function(\"elaborate\",kind=\"container\", image=\"ghcr.io/tn-aixpa/rs-deforestation:0.14.6\", command=\"/bin/bash\", code_src=\"launch.sh\")\n</code></pre> Notes: For detailed usage see the usage notebook.</p>"},{"location":"functions/elaborate-forest-data/#environment","title":"Environment","text":"<p>The runtime environment of function consist of properties which are configured in the bash script created in previous section - PATH - PROJ_LIB - GDAL_DATA - GDAL_DRIVER_PATH - PROJ_DATA</p> <p><pre><code>function_elaborate.un(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"12\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-deforestation\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": { \"size\": \"250Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        'Shapes_AOI',\n        'data_s2_2018_19_tps',\n        \"[2018,2019]\",\n        'deforestation_2018_19_tps'\n    ]\n)\n</code></pre> To avoid capacity issues the environment variable \"TMPDIR\" for this function execution is set to same path of volume mount. As a general confromance to best practice approach, the container runtime is executed as non root user(fs_group='8877')</p> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>This section describes the required artifacts (sentinel data, shape and map files) needed to run the elaboration function for the requested area of interest.</p> <p>https://siatservices.provincia.tn.it/idt/vector/p_TN_3d0874bc-7b9e-4c95-b885-0f7c610b08fa.zip</p> <p>Unzip the files in a folder named 'shapes_AOI' and then log it</p> In\u00a0[\u00a0]: <pre>artifact_name='Shapes_AOI'\nsrc_path='/Shapes_AOI'\nartifact_data = proj.log_artifact(name=artifact_name, kind=\"artifact\", source=src_path)\n</pre> <p>The resulting dataset will be registered as the project artifacts in the datalake under the name 'Shapes_AOI'.</p> <p>Fetch the \"elaborate-geological-data\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_el = proj.get_function(\"elaborate-forest-data\") \n</pre> <p>Run the function as shown below.</p> In\u00a0[\u00a0]: <pre>run_el = function_el.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"12\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-deforestation\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": { \"size\": \"250Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        'Shapes_AOI',\n        'data_s2_2018_19_tps',\n        \"[2018,2019]\",\n        'deforestation_2018_19_tps'\n    ]\n)\n</pre> <p> </p>"},{"location":"functions/elaborate-forest-data/#resources","title":"Resources","text":"<p>These settings define the resource requests and limits for the container runtime.</p> Resource Requests Description CPU <code>12</code> CPU cores allocated to the job. Memory <code>64Gi</code> Memory available to the container. <p>The job mounts a persistent storage volume used for reading/writing large datasets (e.g., Sentinel images).</p> Field Value Description <code>volume_type</code> <code>persistent_volume_claim</code> Indicates a persistent storage resource. <code>name</code> <code>volume-deforestation</code> Volume identifier. <code>mount_path</code> <code>/app/files</code> Directory inside the container where the volume is mounted. <code>size</code> <code>250Gi</code> Allocated storage capacity. <p>'elaboration' consists of interpolation and post processing steps which are computationally heavy since it is pixel based analysis. It is based on python joblib library for optimizations of numpy arrays. With the use of more images the interpolation will be shorter. The amount of sentinal data is huge that is why a volume of 250Gi of type 'persistent_volume_claim' is specified to ensure significant data space. On average the TPS tiles takes around 8-10 hours to complete with 16 CPUs and 64GB Ram for 2 years of data which is the default period.</p>"},{"location":"functions/elaborate-forest-data/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create a working context (project) if not created already. Project is a placeholder for the code, function, data, and management of the data operations and workflows.</p>"},{"location":"functions/elaborate-forest-data/#2.-Prepare-data","title":"2. Prepare data\u00b6","text":""},{"location":"functions/elaborate-forest-data/#1--Sentinel-data","title":"1- Sentinel data\u00b6","text":"<p>Please note that this function 'elaborate-geological-data' depends on Sentinel data artifacts, which must be logged in to the project context(if not already) both for ascending and descending orbits using the catalog function 'download-sentinel-data'. For more detailed information, please refer to the catalog function download-sentinel-data</p>"},{"location":"functions/elaborate-forest-data/#2--Shape-and-Map-data.","title":"2- Shape and Map data.\u00b6","text":"<p>Save the necessary artifacts (shape, map files) for the required area of interest inside to the project context.  For e.g. shape file for Trentino region can be downloaded from the open data portal link below.</p>"},{"location":"functions/elaborate-forest-data/#3.-Execution","title":"3. Execution\u00b6","text":""},{"location":"functions/elaborate-geological-data/","title":"Elaborate geological data","text":"Landslide Monitoring (Container Job) RepositoryDefinitionReference \u25bchub://functions/elaborate-geological-data:0.14.6\u2750\u2713\u2750 Name Landslide Monitoring (Container Job) Description Function to elaborate Sentinel-1 imagery data for landslide monitoring Version 0.14.6 Labels apache-2.0geopasecurity0.14jobfeature-extraction UsageNotebook <p> <p></p> <p></p>"},{"location":"functions/elaborate-geological-data/#elaborate-geological-data","title":"Elaborate Geological Data","text":"<p>This function performs a complete preprocessing and analysis on geological Sentinel satellite data to extract information relevant to terrain composition, structural features, and surface variability. It detects and monitor ground deformation associated with landslides using Sentinel-1 Level-2A imagery.  The function computes, for both ascending and descending directions, the interferometry between couples of images acquired every six days and derives the total displacement, coherence map, and local incident angle. It derives the horizontal and vertical displacement components from these products and merges them to obtain their cumulative sum and the displacement between each couple of images. The coherence maps are averaged, and the results are used to filter out the areas with the lowest coherence.</p> <p>The function output GeoTiff Raster files containing: - Cumulative sum and temporal variation of the horizontal displacement ; - Cumulative sum and temporal variation of the vertical displacement; - Cumulative sum and temporal variation of the total displacement of ascending and descending Sentinel-1 images; - Cumulative sum of the horizontal displacement of the areas whaere the cumulative sum of the ascending and descending displacements has an opposite sign; - Cumulative sum of the vertical displacement of the areas where the cumulative sum of the ascending and descending displacements has an opposite sign; - Mean and temporal variation of the coherence maps; - Mean and temporal variation of the coherence maps of ascending and descending Sentinel-1 images; - Temporal variation of the C coefficient map representing the ratio of effective displacement computed in ascending and descending orbits;</p> <p>The function is implemented as a container that allows you to deploy deployments, jobs and services on Kubernetes. It uses the base image with gdal, snapista, and scikit-learn libaries installed and pre configured. It further runs the launch instructions specified by 'launch.sh' file. </p>"},{"location":"functions/elaborate-geological-data/#definition","title":"Definition","text":"<p>The function accepts a list of positional arguments that are passed directly to the Docker container. These parameters control Sentinel-1 data selection, temporal configuration, output aritifact, and AOI geometry. These arguments are passed to the container\u2019s entrypoint script.</p> Position Value Description 1 <code>/shared/launch.sh</code> Entry-point script executed inside the Docker container. Handles download and preprocessing workflow. 2 <code>s1_ascending_landslide</code> Name of artifact inside platform that contains Sentinel-1 ascending orbit cquisitions. 3 <code>s1_descending_landslide</code> Name of artifact inside platform that contains Sentinel-1 descending orbit acquisitions. 4 <code>2021-10-01</code> Start date of monitoring period window. 5 <code>2022-01-01</code> End date of monitoring period window. 6 <code>landslide_2021-10-01_2022-01-01</code> Name of output artifact. 7 <code>Shapes_AOI</code> Name of artifact inside platform containing regional shapefiles (e.g., Trentino region). 8 <code>ammprv_v.shp</code> Name of specific shapefile inside to the artifact 'Shapes_AOI' used for spatial clipping/filtering. 9 <code>Map</code> Name of artificat containing processing mode or output format label based on the workflow\u2019s internal logic. 10 <code>POLYGON ((10.595369 45.923394, 10.644894 45.923394, ...) )</code> WKT polygon defining the Area of Interest (AOI). <p>The function aims at downloading all the geological inputs specified in argument(s1_ascending_landslide, s1_descending_landslide, Shapes_AOI) from project context and perform the complex task of geological elaboration.</p> <p>Example</p> <p>The following command launches the elaborate function as a containerized job, providing compute resources, storage volumes, and runtime arguments.</p> <pre><code>run_el = function_rs.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"12\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-land\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": { \"size\": \"600Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        's1_ascending',\n        's1_descending',\n        '2021-03-01',\n        '2021-07-30',\n        'landslide_2020-11-01_2020-11-14',\n        'Shapes_AOI',\n        'ammprv_v.shp',\n        'Map',\n        'POLYGON ((10.595369 45.923394, 10.644894 45.923394, 10.644894 45.945838, \\\n                   10.595369 45.945838, 10.595369 45.923394))'\n    ]\n)\n</code></pre>"},{"location":"functions/elaborate-geological-data/#usage","title":"Usage","text":"<p>The function expect a entry point launch script as shown below giving user the possibility to configure the runtime environment prior to elaboration. It further runs the launch instructions specified by 'launch.sh' file. </p> <pre><code>%%writefile \"launch.sh\"\n#!/bin/bash\nls -la /shared\ncd ~\npwd\nsource .bashrc\nexport PATH=\"/home/nonroot/miniforge3/snap/bin:$PATH\"\nexport PROJ_LIB=/home/nonroot/miniforge3/share/proj\nexport GDAL_DATA=/home/nonroot/miniforge3/share/gdal\nexport GDAL_DRIVER_PATH=/home/nonroot/miniforge3/lib/gdalplugins\nexport PROJ_DATA=/home/nonroot/miniforge3/share/proj\ncd /app\necho \"{'s1_ascending': '$1', 's1_descending': '$2', 'startDate':'$3', 'endDate':'$4', 'outputArtifactName': '$5', 'shapeArtifactName': '$6', 'shapeFileName': '$7', 'mapArtifactName': '$8', 'geomWKT':'$9'}\"\n#export PATH=\"/home/nonroot/miniforge3/snap/.snap/auxdata/gdal/gdal-3-0-0/bin/:$PATH\"\necho \"GDAL DATA AFTER EXPORT:\"\necho $GDAL_DATA\necho \"PROJ_LIB AFTER EXPORT\"\necho $PROJ_LIB\npython main.py \"{'s1_ascending': '$1', 's1_descending': '$2', 'startDate':'$3', 'endDate':'$4', 'outputArtifactName': '$5', 'shapeArtifactName': '$6', 'shapeFileName': '$7', 'mapArtifactName': '$8', 'geomWKT':'$9'}\"\nexit\n</code></pre> <p><pre><code>function_elaborate = proj.new_function(\"elaborate\",kind=\"container\", image=\"ghcr.io/tn-aixpa/rs-landslide-monitoring:0.14.6\", command=\"/bin/bash\", code_src=\"launch.sh\")\n</code></pre> Notes: For detailed usage see the usage notebook.</p>"},{"location":"functions/elaborate-geological-data/#environment","title":"Environment","text":"<p>The runtime environment of function consist of properties which are configured in the bash script created in previous section - PATH - PROJ_LIB - GDAL_DATA - GDAL_DRIVER_PATH - PROJ_DATA</p> <p><pre><code>function_elaborate.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"12\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-land\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": { \"size\": \"600Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        's1_ascending',\n        's1_descending',\n        '2021-03-01',\n        '2021-07-30',\n        'landslide_2020-11-01_2020-11-14',\n        'Shapes_AOI',\n        'ammprv_v.shp',\n        'Map',\n        'POLYGON ((10.595369 45.923394, 10.644894 45.923394, 10.644894 45.945838, \\\n                   10.595369 45.945838, 10.595369 45.923394))'\n    ]\n)\n</code></pre> To avoid capacity issues the environment variable \"TMPDIR\" for this function execution is set to same path of volume mount. As a general confromance to best practice approach, the container runtime is executed as non root user(fs_group='8877')</p> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>This section describes the required artifacts (sentinel data, shape and map files) needed to run the elaboration function for the requested area of interest.</p> <p>https://siatservices.provincia.tn.it/idt/vector/p_TN_377793f1-1094-4e81-810e-403897418b23.zip.</p> <p>Copy the corresponding files for your area of interest inside 'Shapes_AOI' folder. for e.g. for Trention region unzip the file from link above in a folder named 'Shapes_AOI' and then log it in project context.</p> In\u00a0[\u00a0]: <pre>artifact_name='Shapes_AOI'\nsrc_path='/Shapes_AOI'\nartifact_data = proj.log_artifact(name=artifact_name, kind=\"artifact\", source=src_path)\n</pre> <p>Log the Map aritfact with three files (slope map, aspect map, and legend.qml). For e.g trentino_slope_map.tiff, trentino_aspect_map.tiff, and legend.qml can be downloaded from the Huggingface repository.</p> <p>https://huggingface.co/datasets/lbergamasco/trentino-slope-map/tree/main</p> <p>Copy the required three files for the area of interset inside a folder 'Map' and log it as project artifact</p> In\u00a0[\u00a0]: <pre>artifact_name='Map'\nsrc_path='Map'\nartifact_data = proj.log_artifact(name=artifact_name, kind=\"artifact\", source=src_path)\n</pre> <p>The resulting dataset will be registered as the project artifacts in the datalake under the name 'Shapes_AOI' and 'Map'.</p> <p>Fetch the \"elaborate-geological-data\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_el = proj.get_function(\"elaborate-geological-data\") \n</pre> <p>Run the function as shown below.</p> In\u00a0[\u00a0]: <pre>run_el = function_el.run(\n    action=\"job\",\n    fs_group='8877',\n    resources={\n         \"cpu\": \"12\",\n         \"mem\": \"64Gi\"\n    },\n    volumes=[{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"volume-land\",\n        \"mount_path\": \"/app/files\",\n        \"spec\": { \"size\": \"600Gi\" }\n    }],\n    args=[\n        '/shared/launch.sh',\n        's1_ascending',\n        's1_descending',\n        '2021-03-01',\n        '2021-07-30',\n        'landslide_2020-11-01_2020-11-14',\n        'Shapes_AOI',\n        'ammprv_v.shp',\n        'Map',\n        'POLYGON ((10.595369 45.923394, 10.644894 45.923394, 10.644894 45.945838, \\\n                   10.595369 45.945838, 10.595369 45.923394))'\n    ]\n)\n</pre> <p> </p>"},{"location":"functions/elaborate-geological-data/#resources","title":"Resources","text":"<p>These settings define the resource requests and limits for the container runtime.</p> Resource Requests Description CPU <code>12</code> CPU cores allocated to the job. Memory <code>64Gi</code> Memory available to the container. <p>The job mounts a persistent storage volume used for reading/writing large datasets (e.g., Sentinel images).</p> Field Value Description <code>volume_type</code> <code>persistent_volume_claim</code> Indicates a persistent storage resource. <code>name</code> <code>volume-land</code> Volume identifier. <code>mount_path</code> <code>/app/files</code> Directory inside the container where the volume is mounted. <code>size</code> <code>600Gi</code> Allocated storage capacity. <p>'elaboration' consists of interferometry step which is a remote sensing technique that uses radar data to detect and monitor ground deformation associated with landslides and post processing steps which are computationally heavy since it is pixel based analysis. In some cases, the amount of sentinal data is huge that is why a default volume of 600Gi of type 'persistent_volume_claim' is specified in example to ensure significant data space. This configuration must be change according to scenario requirement. In the example given in documentation usage notebook, an elaboration on two weeks data is performed which takes ~5 hours to complete with 16 CPUs and 64GB Ram.</p>"},{"location":"functions/elaborate-geological-data/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create a working context (project) if not created already. Project is a placeholder for the code, function, data, and management of the data operations and workflows.</p>"},{"location":"functions/elaborate-geological-data/#2.-Prepare-data","title":"2. Prepare data\u00b6","text":""},{"location":"functions/elaborate-geological-data/#1--Sentinel-data","title":"1- Sentinel data\u00b6","text":"<p>Please note that this function 'elaborate-geological-data' depends on Sentinel data artifacts, which must be logged in to the project context(if not already) both for ascending and descending orbits using the catalog function 'download-sentinel-data'. For more detailed information, please refer to the catalog function download-sentinel-data</p>"},{"location":"functions/elaborate-geological-data/#2--Shape-and-Map-data.","title":"2- Shape and Map data.\u00b6","text":"<p>Save the necessary artifacts (shape, map files) for the required area of interest inside to the project context.  For e.g. shape file for Trentino region can be downloaded from the open data portal link below.</p>"},{"location":"functions/elaborate-geological-data/#3.-Execution","title":"3. Execution\u00b6","text":""},{"location":"functions/example-dbt-transform/","title":"Example dbt transform","text":"Example DBT Transform (DBT Job) RepositoryDefinitionReference \u25bchub://functions/example-dbt-transform:1.0.0\u2750\u2713\u2750 Name Example DBT Transform (DBT Job) Description This function demonstrates how to create a DBT transformation job. It performs a simple SQL query on a sample dataitem named 'employees'. It further processes the data to extract all records and filters them based on a specific department ID. The resulting dataset is logged for verification. This example serves as a foundational template for building more complex DBT transformations within your data workflows. Version 1.0.0 Labels apache-2.0tabular0.14jobdata-preparation UsageNotebook <p> <p></p> <p></p>"},{"location":"functions/example-dbt-transform/#dbt-transform","title":"DBT-Transform","text":"<p>This function demonstrates how to create a DBT transformation job. It performs a simple SQL query on a sample dataitem named 'employees'. It further processes the data to extract all records and filters them based on a specific department ID. The resulting dataset is logged for verification. This example serves as a foundational template for building more complex DBT transformations within your data workflows.</p>"},{"location":"functions/example-dbt-transform/#features","title":"Features","text":"<ul> <li>Simple SQL-based data transformation using DBT</li> <li>Filters employee records by department ID</li> <li>Integrates with digitalhub dataitem management</li> <li>Supports parameterized inputs and outputs</li> <li>Suitable as a template for complex ETL workflows</li> </ul>"},{"location":"functions/example-dbt-transform/#definition","title":"Definition","text":"<pre><code>sql = \"\"\"\nWITH tab AS (\n    SELECT  *\n    FROM    {{ ref('employees') }}\n)\nSELECT  *\nFROM    tab\nWHERE   tab.\"DEPARTMENT_ID\" = '50'\n\"\"\"\n</code></pre> <p>We define the function:</p> <pre><code>function = project.new_function(\n    name=\"dbt-transform\",\n    kind=\"dbt\",\n    code=sql\n)\n</code></pre> <p>The parameters are:</p> <ul> <li>name is the identifier of the function.</li> <li>kind is the type of the function. Must be dbt.</li> <li>code contains the code that is the SQL we'll execute in the function.</li> </ul>"},{"location":"functions/example-dbt-transform/#fetching-the-function","title":"Fetching the Function","text":"<p>To use an existing DBT transform function, retrieve it from the project context:</p> <pre><code>function_dbt = proj.get_function(\"dbt-transform\")\n</code></pre> <p>This fetches the previously defined function by its name, allowing you to execute transformations without redefining the function code.</p> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>Note: Make sure to replace &lt;YOUR_PROJECT_NAME&gt; with the actual name of your project before running the code.</p> In\u00a0[\u00a0]: <pre>url = \"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\"\ndi = proj.new_dataitem(name=\"employees-data\", kind=\"table\", path=url)\n</pre> <p>Fetch the \"dbt-transform\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_dbt = proj.get_function(\"dbt-transform\") \n</pre> <p>We can now run the function and see the results. To do this we use the run method of the function. To the method, we pass:</p> <ul> <li>the task we want to run (in this case, transform)</li> <li>the inputs map the refereced table in the DBT query ({{ ref('employees') }}) to one of our dataitems key. The Runtime will fetch the data and use dem as reference for the query.</li> <li>the output map the output table name. The name of the output table will be department-50 and will be the sql query table name result and the output dataitem name.</li> </ul> In\u00a0[\u00a0]: <pre>run = function_dbt.run(\n    \"transform\",\n    inputs={\"employees\": di.key},\n    outputs={\"output_table\": \"department-50\"},\n    wait=True,\n)\n</pre> <p>Note: Wait for job to finish. Alternatively, using the Core Management UI, one can navigate to 'Runs' menu , select the corresponding 'run' instance and inspect the logs using 'Logs' tab.</p> <p>Let's examine the transformed data - employees from department 50:</p> In\u00a0[\u00a0]: <pre>run.output(\"department-50\").as_df().head()\n</pre> <p> </p>"},{"location":"functions/example-dbt-transform/#usage","title":"Usage","text":"<pre><code># Run the DBT transform function\nrun = function_dbt.run(\n    \"transform\",\n    inputs={\"employees\": di.key},\n    outputs={\"output_table\": \"department-50\"},\n    wait=True,\n)\n\n# Access the transformed data\ndf = run.output(\"department-50\").as_df()\n</code></pre> <p>Notes: For detailed usage, check the usage notebook.</p>"},{"location":"functions/example-dbt-transform/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create the working context: data download project if not created already. Project is a placeholder for the code, data, and management of the data operations.</p>"},{"location":"functions/example-dbt-transform/#2.-Data-Source-Setup","title":"2. Data Source Setup\u00b6","text":"<p>Please note that this function 'dbt-tranform' depends on dataitem which must be logged in to the project context(if not already) using the extract tranform load (ETL) procedure. For more detailed information, please refer to the catalog function example-etc</p> <p>We'll create a data item that points to employee data. This dataset contains employee information including department assignments.</p>"},{"location":"functions/example-dbt-transform/#3.-Execution","title":"3. Execution\u00b6","text":""},{"location":"functions/example-etl/","title":"Example etl","text":"Example ETL (Python Job) RepositoryDefinitionReference \u25bchub://functions/example-etl:1.0.0\u2750\u2713\u2750 Name Example ETL (Python Job) Description A simple extract transform load (ETL) procedure in Python that downloads a csv file and then returns the result as a dataframe that is stored in the project repository as table(data-item). Version 1.0.0 Labels apache-2.0tabular0.14jobdata-preparation UsageNotebook <p> <p></p> <p></p>"},{"location":"functions/example-etl/#extract-transform-load-function","title":"Extract Transform Load Function","text":"<p>This function demonstrates how to create an extract transform load procedure that downloads a CSV file from a remote URL, processes it using pandas, and stores the result as a dataitem in your project. It showcases a basic ETL pattern for ingesting external data sources into your digitalhub project environment.</p>"},{"location":"functions/example-etl/#features","title":"Features","text":"<ul> <li>Downloads CSV data from remote URLs</li> <li>Processes data using pandas DataFrame operations</li> <li>Automatically stores results as dataitems in your project</li> <li>Demonstrates basic ETL pattern for data ingestion</li> <li>Supports integration with digitalhub runtime environment</li> </ul>"},{"location":"functions/example-etl/#definition","title":"Definition","text":"<pre><code># function.py\nimport pandas as pd\nfrom digitalhub_runtime_python import handler\n\nURL=\"https://raw.githubusercontent.com/datasets/world-cities/refs/heads/main/data/world-cities.csv\"\n\n@handler(outputs=[\"world-cities\"])\ndef download_and_process():\n    df = pd.read_csv(URL)\n    df.reset_index(inplace=True)\n    return df\n</code></pre>"},{"location":"functions/example-etl/#usage","title":"Usage","text":"<pre><code># Create the function\nfunction = project.new_function(\n    name=\"example-etl\",\n    kind=\"python\",\n    source={\"source\": \"function.py\", \"handler\": \"download_and_process\"},\n)\n</code></pre>"},{"location":"functions/example-etl/#fetching-the-function","title":"Fetching the Function","text":"<p>To use an existing ETL transform function as in this example, retrieve it from the project context.</p> <pre><code>function = proj.get_function(\"example-etl\")\n</code></pre>"},{"location":"functions/example-etl/#run-the-function","title":"Run the function","text":"<p>run = function.run(\"job\", wait=True)</p> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>Note: Make sure to replace &lt;YOUR_PROJECT_NAME&gt; with the actual name of your project before running the code.</p> In\u00a0[\u00a0]: <pre>URL=\"https://raw.githubusercontent.com/datasets/world-cities/refs/heads/main/data/world-cities.csv\"\ndi = proj.new_dataitem(name=\"employees-data\", kind=\"table\", path=URL)\n</pre> <p>Fetch the \"example-etc\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_etl = proj.get_function(\"example-etl\") \n</pre> <p>We can now run the function and see the results. To do this we use the run method of the function.</p> In\u00a0[\u00a0]: <pre>run = function_etl.run(\"job\", wait=True)\n</pre> <p>Note: Wait for job to finish. Alternatively, using the Core Management UI, one can navigate to 'Runs' menu , select the corresponding 'run' instance and inspect the logs using 'Logs' tab.</p> <p>Let's examine the raw data we just downloaded:</p> In\u00a0[\u00a0]: <pre>dataset_di = proj.get_dataitem(\"world-cities\")\ndataset_di.as_df().head()\n</pre> <p> </p>"},{"location":"functions/example-etl/#access-the-data","title":"Access the data","text":"<p>dataset_di = proj.get_dataitem(\"world-cities\") dataset_di.as_df().head() ```</p> <p>Notes: For detailed usage, check the usage notebook.</p>"},{"location":"functions/example-etl/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create the working context: data download project if not created already. Project is a placeholder for the code, data, and management of the data operations.</p>"},{"location":"functions/example-etl/#Data-Source-Setup","title":"Data Source Setup\u00b6","text":"<p>We'll create a data item that points to world cities database. # The world cities database contains the following columns:</p> <ul> <li>name: Name of the city</li> <li>country: Country where the city is located</li> <li>subcountry: Subdivision (state/province/region) of the country</li> <li>geonameid: Unique identifier for the city in the GeoNames database</li> </ul>"},{"location":"functions/example-etl/#3.-Execution","title":"3. Execution\u00b6","text":""},{"location":"functions/hello-world/","title":"Hello world","text":"Example Hello World (Python Job) RepositoryDefinitionReference \u25bchub://functions/hello-world:1.0.0\u2750\u2713\u2750 Name Example Hello World (Python Job) Description Hello world with python: write a string to stdout Version 1.0.0 Labels apache-2.0other0.14jobutility UsageNotebook <p> <p></p> <p></p>"},{"location":"functions/hello-world/#hello-world","title":"Hello world","text":"<p>This function demonstrates a minimal example of a function whose sole purpose is to print the classic phrase \u201cHello, World!\u201d in python. This type of function is traditionally used as an introductory exercise, as it demonstrates the most fundamental structure of a program: outputting text.</p>"},{"location":"functions/hello-world/#definition","title":"Definition","text":"<pre><code>def main(param):\n    print(f\"Hello {param}\")\n</code></pre> In\u00a0[\u00a0]: <pre>import digitalhub as dh\nPROJECT_NAME = \"&lt;YOUR_PROJECT_NAME&gt;\"\nproj = dh.get_or_create_project(PROJECT_NAME)\n</pre> <p>Note: Make sure to replace &lt;YOUR_PROJECT_NAME&gt; with the actual name of your project before running the code.</p> <p>Fetch the \"hello-world\" operation in the project.</p> In\u00a0[\u00a0]: <pre>function_hello = proj.get_function(\"hello-world\") \n</pre> <p>Then, execute the function (locally) as specified with (local_execution=True) flag. This will execute the function as a single job in the local context. Without the local_execution flag the job will get executed on the platform. For more details about how to create, run, and view jobs inside to the digital hub platform see the documentation </p> In\u00a0[\u00a0]: <pre>function_hello.run(action='job', parameters={\"param\": \"World\"}, wait=True, local_execution=True)\n</pre> <p>As a result the 'hello world' message is logged.</p> <p>Now, we can run the same function as a job in the platform by removing the 'local_execution' flag.</p> In\u00a0[\u00a0]: <pre>function_run = function_hello.run(action='job', parameters={\"param\": \"World\"}, wait=True)\n</pre> <p>The logs of platform execution can be viewed running the cell below.</p> In\u00a0[\u00a0]: <pre>import base64\nlogs = base64.b64decode(function_run.logs()[1]['content']).decode()\nprint(logs)\n</pre> <p>Note: Alternatively, using the Core Management UI, one can navigate to 'Runs' menu , select the corresponding 'run' instance and inspect the logs using 'Logs' tab.</p> <p> </p>"},{"location":"functions/hello-world/#usage","title":"Usage","text":"<p>The function demonstrates:</p> <p>Basic syntax of a python script used inside to the platform. It demonstrates - How to define a function that takes a parameter. - How to output Hello {parameter} message to the screen</p> <p>The function 'hello-world' is registered inside to the platform core durig the import and it can be fetched and executed</p> <pre><code>func = proj.get_function(name=\"hello-world\",\n                            kind=\"python\",\n                            python_version=\"PYTHON3_10\") \n</code></pre> <p>This code fetch the created function that uses Python runtime (versione 3.10) pointing to the created file and the handler method that should be called. In this case, the code and hanlder is already embedded during the funciton import from the .yaml file.</p> <p>Then, the function can be executed on the digital hub platform or (locally) as a single job.</p> <p>Notes: For detailed usage, check the usage notebook.</p>"},{"location":"functions/hello-world/#1.-Initialize-the-project","title":"1. Initialize the project\u00b6","text":"<p>Create the working context: data download project if not created already. Project is a placeholder for the code, data, and management of the data operations.</p>"},{"location":"functions/hello-world/#2.-Execution","title":"2. Execution\u00b6","text":""}]}